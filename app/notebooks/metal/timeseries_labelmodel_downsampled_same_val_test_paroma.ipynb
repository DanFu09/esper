{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, pickle, csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "sys.path.append('/lfs/1/danfu/metal')\n",
    "sys.path.append('/lfs/1/danfu/sequential_ws')\n",
    "from metal.metrics import metric_score\n",
    "from torch.nn.functional import normalize\n",
    "from DP.label_model import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_train_100_windows_downsampled.npz'\n",
    "L_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_val_windows_downsampled_same_val_test.npz'\n",
    "Y_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_val_windows_downsampled_same_val_test.npy'\n",
    "L_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_test_windows_downsampled_same_val_test.npz'\n",
    "Y_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_test_windows_downsampled_same_val_test.npy'\n",
    "\n",
    "stride = 1\n",
    "L_train_raw = sp.sparse.load_npz(L_train_path).todense()[::stride]\n",
    "L_dev_raw = sp.sparse.load_npz(L_dev_path).todense()\n",
    "Y_dev_raw = np.load(Y_dev_path)\n",
    "L_test_raw = sp.sparse.load_npz(L_test_path).todense()\n",
    "Y_test_raw = np.load(Y_test_path)\n",
    "\n",
    "T = 5\n",
    "\n",
    "L_train = torch.FloatTensor(L_train_raw[:L_train_raw.shape[0] - (L_train_raw.shape[0] % T)]).to(device)\n",
    "L_dev = torch.FloatTensor(L_dev_raw[:L_dev_raw.shape[0] - (L_dev_raw.shape[0] % T)]).to(device)\n",
    "Y_dev = torch.FloatTensor(Y_dev_raw[:Y_dev_raw.shape[0] - (Y_dev_raw.shape[0] % T)]).to(device)\n",
    "L_test = torch.FloatTensor(L_test_raw[:L_test_raw.shape[0] - (L_test_raw.shape[0] % T)]).to(device)\n",
    "Y_test = torch.FloatTensor(Y_test_raw[:Y_test_raw.shape[0] - (Y_test_raw.shape[0] % T)]).to(device)\n",
    "m_per_task = L_train.size(1)\n",
    "n_frames_train = L_train.size(0)\n",
    "n_patients_train = n_frames_train//T\n",
    "n_frames_dev = L_dev.size(0)\n",
    "n_patients_dev = n_frames_dev//T\n",
    "n_frames_test = L_test.size(0)\n",
    "n_patients_test = n_frames_test//T\n",
    "\n",
    "# MRI_data_naive = {'Li_train': (L_train.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'Li_dev': (L_dev.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'R_dev': (Y_dev.unsqueeze(1) == torch.FloatTensor([-1,1]).to(device).unsqueeze(0)).argmax(1),\n",
    "#                   'm':m_per_task, 'T':1,\n",
    "#                  }\n",
    "\n",
    "# don't need to transform the raw data\n",
    "MRI_data_naive = {'Li_train': L_train.long().to(device),\n",
    "                  'Li_dev': L_dev.long().to(device),\n",
    "                  'R_dev': Y_dev.long().to(device),\n",
    "                  'Li_test': L_test.long().to(device),\n",
    "                  'R_test': Y_test.long().to(device),\n",
    "                  'm':m_per_task, 'T':1,\n",
    "                 }\n",
    "MRI_data_naive['class_balance'] = normalize((MRI_data_naive['R_dev'].unsqueeze(1)==torch.arange(2, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                            dim=0, p=1)\n",
    "MRI_data_temporal = {'Li_train': MRI_data_naive['Li_train'].view(n_patients_train, (m_per_task*T)),\n",
    "                     'Li_dev': MRI_data_naive['Li_dev'].view(n_patients_dev, (m_per_task*T)),\n",
    "                     'R_dev': MRI_data_naive['R_dev']*(2**T-1),\n",
    "                     'Li_test': MRI_data_naive['Li_test'].view(n_patients_test, (m_per_task*T)),\n",
    "                     'R_test': MRI_data_naive['R_test']*(2**T-1),\n",
    "                     'm': m_per_task * T, 'T': T,\n",
    "                    } \n",
    "MRI_data_temporal['class_balance'] = normalize((MRI_data_temporal['R_dev'].unsqueeze(1)==torch.arange(2**T, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                                dim=0, p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Label Voter Metrics:\n",
      "Accuracy: 0.951\n",
      "Precision: 0.828\n",
      "Recall: 0.936\n",
      "F1: 0.879\n",
      "        y=1    y=2   \n",
      " l=1    322    67    \n",
      " l=2    22    1404   \n"
     ]
    }
   ],
   "source": [
    "from metal.label_model.baselines import MajorityLabelVoter\n",
    "\n",
    "# majority vote of LFs\n",
    "mv = MajorityLabelVoter()\n",
    "print('Majority Label Voter Metrics:')\n",
    "mv_score = mv.score((MRI_data_naive['Li_dev'],MRI_data_naive['R_dev']), metric=['accuracy','precision', 'recall', 'f1'])\n",
    "\n",
    "mv_predictions = mv.predict(MRI_data_naive['Li_train']).astype(float)\n",
    "mv_predictions[mv_predictions==2.] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=50.763729095458984\n",
      "iteration=300 loss=8.765926361083984\n",
      "iteration=600 loss=3.584846258163452\n",
      "iteration=900 loss=1.8508527278900146\n",
      "iteration=1200 loss=0.9663214683532715\n",
      "iteration=1500 loss=0.6240480542182922\n",
      "iteration=1800 loss=0.49020180106163025\n",
      "iteration=2100 loss=0.41870707273483276\n",
      "iteration=2400 loss=0.3748452961444855\n",
      "iteration=2700 loss=0.344853013753891\n",
      "iteration=2999 loss=0.3229215145111084\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           # class_balance=MRI_data_naive['class_balance'], \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=3000, lr=4.087885261759692e-05,\n",
    "         momentum=0.9, clamp=True, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.142\n",
      "F1: 0.413\n",
      "Recall: 0.747\n",
      "Precision: 0.286\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2,  ..., 2, 2, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MRI_data_naive['R_dev'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_flipped.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping Parameters\n",
      "Accuracy: 0.048\n",
      "F1: 0.138\n",
      "Recall: 0.253\n",
      "Precision: 0.095\n"
     ]
    }
   ],
   "source": [
    "# Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MRI_data_temporal['class_balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRI_data_temporal['class_balance'] = normalize((MRI_data_temporal['R_dev'].unsqueeze(1)==torch.arange(2**T, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                                dim=0, p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_target = Y_dev.long()\n",
    "T = 5\n",
    "\n",
    "feasible_y = np.array([[-1, -1, -1, -1, -1],\n",
    "        [-1,  1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1],\n",
    "        [ 1,  1, -1, -1, -1],\n",
    "        [-1, -1,  1, -1, -1],\n",
    "        [-1,  1,  1, -1, -1],\n",
    "        [ 1, -1,  1, -1, -1],\n",
    "        [ 1,  1,  1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1],\n",
    "        [-1,  1, -1,  1, -1],\n",
    "        [ 1, -1, -1,  1, -1],\n",
    "        [ 1,  1, -1,  1, -1],\n",
    "        [-1, -1,  1,  1, -1],\n",
    "        [-1,  1,  1,  1, -1],\n",
    "        [ 1, -1,  1,  1, -1],\n",
    "        [ 1,  1,  1,  1, -1],\n",
    "        [-1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1,  1],\n",
    "        [ 1,  1, -1, -1,  1],\n",
    "        [-1, -1,  1, -1,  1],\n",
    "        [-1,  1,  1, -1,  1],\n",
    "        [ 1, -1,  1, -1,  1],\n",
    "        [ 1,  1,  1, -1,  1],\n",
    "        [-1, -1, -1,  1,  1],\n",
    "        [-1,  1, -1,  1,  1],\n",
    "        [ 1, -1, -1,  1,  1],\n",
    "        [ 1,  1, -1,  1,  1],\n",
    "        [-1, -1,  1,  1,  1],\n",
    "        [-1,  1,  1,  1,  1],\n",
    "        [ 1, -1,  1,  1,  1],\n",
    "        [ 1,  1,  1,  1,  1]])\n",
    "\n",
    "feasible_y[feasible_y==-1] = 0\n",
    "feasible_y = feasible_y.tolist()\n",
    "possibilities = list(map(lambda l : ''.join(map(str,l)), feasible_y))\n",
    "\n",
    "class_balance = np.empty(2 ** T)\n",
    "#compute class balance from dev set and use laplace smoothing\n",
    "\n",
    "valid_target_copy = np.copy(valid_target)\n",
    "valid_target_copy[valid_target_copy == 2] = 0\n",
    "\n",
    "assert len(valid_target_copy) % T == 0\n",
    "num_windows = len(valid_target_copy) / T\n",
    "\n",
    "freq = {}\n",
    "for i in range(0, len(valid_target_copy), T):\n",
    "    s = ''.join(map(str,valid_target_copy[i:i+T]))\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "for i in range(len(class_balance)):\n",
    "    if possibilities[i] in freq and freq[possibilities[i]] > 5:\n",
    "        class_balance[i] = (freq[possibilities[i]] + 1) / (num_windows + len(possibilities))\n",
    "    else:\n",
    "        class_balance[i] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2,  ..., 2, 2, 2], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_balance[class_balance == 0.] = 0.01\n",
    "class_balance = class_balance/np.sum(class_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 1e-05\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Accuracy: 0.183\n",
      "F1: 0.469\n",
      "Recall: 0.965\n",
      "Precision: 0.309\n",
      "Accuracy: 0.021\n",
      "F1: 0.123\n",
      "Recall: 0.110\n",
      "Precision: 0.138\n",
      "\n",
      "Accuracy: 0.152\n",
      "F1: 0.270\n",
      "Recall: 0.802\n",
      "Precision: 0.162\n",
      "Accuracy: 0.080\n",
      "F1: 0.531\n",
      "Recall: 0.422\n",
      "Precision: 0.718\n",
      "\n",
      "Accuracy: 0.174\n",
      "F1: 0.381\n",
      "Recall: 0.919\n",
      "Precision: 0.240\n",
      "Accuracy: 0.047\n",
      "F1: 0.337\n",
      "Recall: 0.247\n",
      "Precision: 0.528\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.486\n",
      "Recall: 0.927\n",
      "Precision: 0.330\n",
      "Accuracy: 0.021\n",
      "F1: 0.059\n",
      "Recall: 0.110\n",
      "Precision: 0.040\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.364\n",
      "Recall: 0.849\n",
      "Precision: 0.232\n",
      "Accuracy: 0.002\n",
      "F1: 0.022\n",
      "Recall: 0.012\n",
      "Precision: 0.308\n",
      "\n",
      "25 1e-06\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Accuracy: 0.183\n",
      "F1: 0.469\n",
      "Recall: 0.965\n",
      "Precision: 0.309\n",
      "Accuracy: 0.021\n",
      "F1: 0.123\n",
      "Recall: 0.110\n",
      "Precision: 0.138\n",
      "\n",
      "Accuracy: 0.152\n",
      "F1: 0.270\n",
      "Recall: 0.802\n",
      "Precision: 0.162\n",
      "Accuracy: 0.080\n",
      "F1: 0.531\n",
      "Recall: 0.422\n",
      "Precision: 0.718\n",
      "\n",
      "Accuracy: 0.174\n",
      "F1: 0.381\n",
      "Recall: 0.919\n",
      "Precision: 0.240\n",
      "Accuracy: 0.047\n",
      "F1: 0.337\n",
      "Recall: 0.247\n",
      "Precision: 0.528\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.486\n",
      "Recall: 0.927\n",
      "Precision: 0.330\n",
      "Accuracy: 0.021\n",
      "F1: 0.059\n",
      "Recall: 0.110\n",
      "Precision: 0.040\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.364\n",
      "Recall: 0.849\n",
      "Precision: 0.232\n",
      "Accuracy: 0.002\n",
      "F1: 0.022\n",
      "Recall: 0.012\n",
      "Precision: 0.308\n",
      "\n",
      "25 1e-07\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Accuracy: 0.183\n",
      "F1: 0.469\n",
      "Recall: 0.965\n",
      "Precision: 0.309\n",
      "Accuracy: 0.021\n",
      "F1: 0.123\n",
      "Recall: 0.110\n",
      "Precision: 0.138\n",
      "\n",
      "Accuracy: 0.152\n",
      "F1: 0.270\n",
      "Recall: 0.802\n",
      "Precision: 0.162\n",
      "Accuracy: 0.080\n",
      "F1: 0.531\n",
      "Recall: 0.422\n",
      "Precision: 0.718\n",
      "\n",
      "Accuracy: 0.174\n",
      "F1: 0.381\n",
      "Recall: 0.919\n",
      "Precision: 0.240\n",
      "Accuracy: 0.047\n",
      "F1: 0.337\n",
      "Recall: 0.247\n",
      "Precision: 0.528\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.486\n",
      "Recall: 0.927\n",
      "Precision: 0.330\n",
      "Accuracy: 0.021\n",
      "F1: 0.059\n",
      "Recall: 0.110\n",
      "Precision: 0.040\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.364\n",
      "Recall: 0.849\n",
      "Precision: 0.232\n",
      "Accuracy: 0.002\n",
      "F1: 0.022\n",
      "Recall: 0.012\n",
      "Precision: 0.308\n",
      "\n",
      "[25, 1, DPLabelModel(), 0.15206611570247933, 0.2699266503667481, 0.8023255813953488, 0.16225749559082892, 0.07988980716253444, 0.5311355311355311, 0.42151162790697677, 0.7178217821782178]\n",
      "CPU times: user 3min 9s, sys: 15.6 s, total: 3min 24s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best = None\n",
    "for iterations in [25]:\n",
    "    for learning_rate in [1e-5, 1e-6, 1e-7]:\n",
    "        print(iterations, learning_rate)\n",
    "        max_seed = 5\n",
    "        temporal_models = [None,]*max_seed\n",
    "        for seed in range(max_seed):\n",
    "            print(seed)\n",
    "            markov_model = DPLabelModel(m=m_per_task*T, \n",
    "                                        T=T,\n",
    "                                        edges=[(i,i+m_per_task) for i in range((T-1)*m_per_task)],\n",
    "                                        coverage_sets=[[t,] for t in range(T) for _ in range(m_per_task)],\n",
    "                                        mu_sharing=[[t*m_per_task+i for t in range(T)] for i in range(m_per_task)],\n",
    "                                        phi_sharing=[[(t*m_per_task+i, (t+1)*m_per_task+i)\n",
    "                                                      for t in range(T-1)] for i in range(m_per_task)],\n",
    "                                        device=device,\n",
    "                                        class_balance=torch.tensor(class_balance).float().to(device),\n",
    "                                        seed=seed)\n",
    "            optimize(markov_model, L_hat=MRI_data_temporal['Li_train'], num_iter=iterations,\n",
    "                     lr=1e-5, momentum=0.8, clamp=True, \n",
    "                     verbose=False, seed=seed)\n",
    "            temporal_models[seed] = markov_model\n",
    "\n",
    "        for seed, model in enumerate(temporal_models):\n",
    "            Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            scores = [iterations, seed, model]\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "                \n",
    "            model.flip_params()\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "            \n",
    "            model.flip_params()\n",
    "\n",
    "            if best == None or scores[4] > max(best[4], best[8]) or scores[8] > max(best[4], best[8]):\n",
    "                best = scores\n",
    "            print()\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1397c3a069b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best' is not defined"
     ]
    }
   ],
   "source": [
    "best_model = best[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.143\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.120\n",
      "F1: 0.784\n",
      "Recall: 0.738\n",
      "Precision: 0.838\n"
     ]
    }
   ],
   "source": [
    "Li_test = torch.LongTensor(MRI_data_temporal['Li_test'].cpu().numpy())\n",
    "R_pred_frame_label = best_model.predict_element_proba(Li_test.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_test.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Li_dev = torch.LongTensor(MRI_data_temporal['Li_train'].cpu().numpy())\n",
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "# for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "#     score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "#     print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([215065., 192578., 245527., 150370., 133418.,  55598.,  38484.,\n",
       "         18650.,  10069.,   3431.]),\n",
       " array([6.19463015e-06, 9.98403258e-02, 1.99674457e-01, 2.99508588e-01,\n",
       "        3.99342719e-01, 4.99176850e-01, 5.99010981e-01, 6.98845113e-01,\n",
       "        7.98679244e-01, 8.98513375e-01, 9.98347506e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_frame_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, 'models/ts_labelmodel_best_tuning_downsampled_same_val_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('models/ts_labelmodel_best_tuning_downsampled_same_val_test.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.143\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = model.eval().predict_element_proba(\n",
    "    MRI_data_temporal['Li_dev'].to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.120\n",
      "F1: 0.784\n",
      "Recall: 0.738\n",
      "Precision: 0.838\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = model.eval().predict_element_proba(\n",
    "    MRI_data_temporal['Li_test'].to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_test.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions for everything and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sparse\n",
    "import pickle\n",
    "import rekall\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "from rekall.interval_list import IntervalList\n",
    "from rekall.temporal_predicates import *\n",
    "from metal.label_model.baselines import MajorityLabelVoter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load manually annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 11655.47it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 12861.74it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/manually_annotated_shots.pkl', 'rb') as f:\n",
    "    shots = VideoIntervalCollection(pickle.load(f))\n",
    "with open('../../data/shot_detection_folds.pkl', 'rb') as f:\n",
    "    shot_detection_folds = pickle.load(f)\n",
    "clips = shots.dilate(1).coalesce().dilate(-1)\n",
    "shot_boundaries = shots.map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.start, intrvl.payload)\n",
    ").set_union(\n",
    "    shots.map(lambda intrvl: (intrvl.end + 1, intrvl.end + 1, intrvl.payload))\n",
    ").coalesce()\n",
    "boundary_frames = {\n",
    "    video_id: [\n",
    "        intrvl.start\n",
    "        for intrvl in shot_boundaries.get_intervallist(video_id).get_intervals()\n",
    "    ]\n",
    "    for video_id in shot_boundaries.get_allintervals()\n",
    "}\n",
    "video_ids = sorted(list(clips.get_allintervals().keys()))\n",
    "frames_per_video = {\n",
    "    video_id: sorted([\n",
    "        f\n",
    "        for interval in clips.get_intervallist(video_id).get_intervals()\n",
    "        for f in range(interval.start, interval.end + 2)\n",
    "    ])\n",
    "    for video_id in video_ids\n",
    "}\n",
    "ground_truth = {\n",
    "    video_id: [\n",
    "        1 if f in boundary_frames[video_id] else 2\n",
    "        for f in frames_per_video[video_id]\n",
    "    ] \n",
    "    for video_id in video_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load label matrix with all frames in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/all_labels.pkl', 'rb') as f:\n",
    "    weak_labels_all_movies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load videos and number of frames per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/frame_counts.pkl', 'rb') as f:\n",
    "    frame_counts = pickle.load(f)\n",
    "video_ids_all = sorted(list(frame_counts.keys()))\n",
    "video_ids_train = sorted(list(set(video_ids_all).difference(set(video_ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct windows for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, construct windows of 16 frames for each video\n",
    "windows = VideoIntervalCollection({\n",
    "    video_id: [\n",
    "        (f, f + 16, video_id)\n",
    "        for f in range(0, frame_counts[video_id] - 16, 16)\n",
    "    ]\n",
    "    for video_id in video_ids_all\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truth labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, intersect the windows with ground truth and get ground truth labels for the windows\n",
    "windows_intersecting_ground_truth = windows.filter_against(\n",
    "    clips,\n",
    "    predicate=overlaps()\n",
    ").map(lambda intrvl: (intrvl.start, intrvl.end, 2))\n",
    "windows_with_shot_boundaries = windows_intersecting_ground_truth.filter_against(\n",
    "    shot_boundaries,\n",
    "    predicate = lambda window, shot_boundary:\n",
    "        shot_boundary.start >= window.start and shot_boundary.start < window.end\n",
    ").map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.end, 1)\n",
    ")\n",
    "windows_with_labels = windows_with_shot_boundaries.set_union(\n",
    "    windows_intersecting_ground_truth\n",
    ").coalesce(\n",
    "    predicate = equal(),\n",
    "    payload_merge_op = lambda p1, p2: min(p1, p2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weak labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label windows with the weak labels in our labeling functions\n",
    "def label_window(per_frame_weak_labels):\n",
    "    if 1 in per_frame_weak_labels:\n",
    "        return 1\n",
    "    if len([l for l in per_frame_weak_labels if l == 2]) >= len(per_frame_weak_labels) / 2:\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "windows_with_weak_labels = windows.map(\n",
    "    lambda window: (\n",
    "        window.start,\n",
    "        window.end,\n",
    "        [\n",
    "            label_window([\n",
    "                lf[window.payload][f-1]\n",
    "                for f in range(window.start, window.end)\n",
    "            ])\n",
    "            for lf in weak_labels_all_movies\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_everything_windows = csr_matrix([\n",
    "    intrvl.payload\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows_downsampled.npy', 'wb') as f:\n",
    "    np.save(f, L_everything_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows_downsampled.npy', 'rb') as f:\n",
    "    L_everything_windows = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert L matrix to timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5\n",
    "m_per_task = L_everything_windows.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled = torch.FloatTensor(L_everything_windows[:L_everything_windows.shape[0] -\n",
    "                                                      (L_everything_windows.shape[0] % T)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_per_task_unlabelled = L_unlabelled.size(1)\n",
    "n_frames_unlabelled = L_unlabelled.size(0)\n",
    "n_patients_unlabelled = n_frames_unlabelled//T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled_ts = torch.LongTensor(\n",
    "    L_unlabelled.view(n_patients_unlabelled, (m_per_task*T)).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235081"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled_ts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val = model.eval().predict_element_proba(MRI_data_temporal['Li_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1278.,  148.,   35.,   29.,   23.,   25.,   33.,   40.,   59.,\n",
       "         145.]),\n",
       " array([1.88074608e-06, 9.98230374e-02, 1.99644194e-01, 2.99465351e-01,\n",
       "        3.99286507e-01, 4.99107664e-01, 5.98928821e-01, 6.98749977e-01,\n",
       "        7.98571134e-01, 8.98392291e-01, 9.98213447e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.931\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n"
     ]
    }
   ],
   "source": [
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu().where(Y_dev.cpu() == torch.tensor(1.), torch.tensor(0.)),\n",
    "                         np.round(predictions_val), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n"
     ]
    }
   ],
   "source": [
    "predictions_everything = []\n",
    "for i in range(0, L_unlabelled_ts.shape[0], 100000):\n",
    "    print(i)\n",
    "    start = i\n",
    "    end = i + 100000\n",
    "    labels = L_unlabelled_ts[start:end] if end < L_unlabelled_ts.shape[0] else L_unlabelled_ts[start:]\n",
    "    predictions_for_labels = model.eval().predict_element_proba(labels.to(device))\n",
    "    predictions_everything.append(predictions_for_labels)\n",
    "    del predictions_for_labels\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6175405,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(predictions_everything).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_pred_probs_per_frame = np.concatenate(predictions_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3759959.,  734106.,  692494.,  145111.,  139209.,   97392.,\n",
       "         131007.,  160536.,  158791.,  156800.]),\n",
       " array([1.41829968e-07, 9.99912640e-02, 1.99982386e-01, 2.99973508e-01,\n",
       "        3.99964631e-01, 4.99955753e-01, 5.99946875e-01, 6.99937997e-01,\n",
       "        7.99929120e-01, 8.99920242e-01, 9.99911364e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_probs_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "         0],\n",
       "        [1, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "         0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled_ts[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_nums = [\n",
    "    (video_id, intrvl.start, intrvl.end)\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows = [\n",
    "    (window_info, np.array([prediction, 1. - prediction]))\n",
    "    for window_info, prediction in zip(window_nums, R_pred_probs_per_frame)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we needed to cut the predictions to a multiple of T\n",
    "last_preds = []\n",
    "for window_info in window_nums[len(predictions_to_save_windows):]:\n",
    "    last_preds.append((window_info, np.array([0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows += last_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np_windows = np.array(predictions_to_save_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to disk\n",
    "with open('../../data/shot_detection_weak_labels/ts_weak_labels_all_windows_tuned_downsampled_same_val_test.npy', 'wb') as f:\n",
    "    np.save(f, preds_np_windows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
