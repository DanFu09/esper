{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, pickle, csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "sys.path.append('/lfs/1/danfu/metal')\n",
    "sys.path.append('/lfs/1/danfu/sequential_ws')\n",
    "from metal.metrics import metric_score\n",
    "from torch.nn.functional import normalize\n",
    "from DP.label_model import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_train_100_windows_high_pre_downsampled.npz'\n",
    "L_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_val_windows_high_pre_downsampled.npz'\n",
    "Y_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_val_windows_high_pre_downsampled.npy'\n",
    "L_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_test_windows_high_pre_downsampled.npz'\n",
    "Y_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_test_windows_high_pre_downsampled.npy'\n",
    "\n",
    "stride = 1\n",
    "L_train_raw = sp.sparse.load_npz(L_train_path).todense()[::stride]\n",
    "L_dev_raw = sp.sparse.load_npz(L_dev_path).todense()\n",
    "Y_dev_raw = np.load(Y_dev_path)\n",
    "L_test_raw = sp.sparse.load_npz(L_test_path).todense()\n",
    "Y_test_raw = np.load(Y_test_path)\n",
    "\n",
    "T = 5\n",
    "\n",
    "L_train = torch.FloatTensor(L_train_raw[:L_train_raw.shape[0] - (L_train_raw.shape[0] % T)]).to(device)\n",
    "L_dev = torch.FloatTensor(L_dev_raw[:L_dev_raw.shape[0] - (L_dev_raw.shape[0] % T)]).to(device)\n",
    "Y_dev = torch.FloatTensor(Y_dev_raw[:Y_dev_raw.shape[0] - (Y_dev_raw.shape[0] % T)]).to(device)\n",
    "L_test = torch.FloatTensor(L_test_raw[:L_test_raw.shape[0] - (L_test_raw.shape[0] % T)]).to(device)\n",
    "Y_test = torch.FloatTensor(Y_test_raw[:Y_test_raw.shape[0] - (Y_test_raw.shape[0] % T)]).to(device)\n",
    "m_per_task = L_train.size(1)\n",
    "n_frames_train = L_train.size(0)\n",
    "n_patients_train = n_frames_train//T\n",
    "n_frames_dev = L_dev.size(0)\n",
    "n_patients_dev = n_frames_dev//T\n",
    "n_frames_test = L_test.size(0)\n",
    "n_patients_test = n_frames_test//T\n",
    "\n",
    "# MRI_data_naive = {'Li_train': (L_train.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'Li_dev': (L_dev.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'R_dev': (Y_dev.unsqueeze(1) == torch.FloatTensor([-1,1]).to(device).unsqueeze(0)).argmax(1),\n",
    "#                   'm':m_per_task, 'T':1,\n",
    "#                  }\n",
    "\n",
    "# don't need to transform the raw data\n",
    "MRI_data_naive = {'Li_train': L_train.long().to(device),\n",
    "                  'Li_dev': L_dev.long().to(device),\n",
    "                  'R_dev': Y_dev.long().to(device),\n",
    "                  'Li_test': L_test.long().to(device),\n",
    "                  'R_test': Y_test.long().to(device),\n",
    "                  'm':m_per_task, 'T':1,\n",
    "                 }\n",
    "MRI_data_naive['class_balance'] = normalize((MRI_data_naive['R_dev'].unsqueeze(1)==torch.arange(2, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                            dim=0, p=1)\n",
    "MRI_data_temporal = {'Li_train': MRI_data_naive['Li_train'].view(n_patients_train, (m_per_task*T)),\n",
    "                     'Li_dev': MRI_data_naive['Li_dev'].view(n_patients_dev, (m_per_task*T)),\n",
    "                     'R_dev': MRI_data_naive['R_dev']*(2**T-1),\n",
    "                     'Li_test': MRI_data_naive['Li_test'].view(n_patients_test, (m_per_task*T)),\n",
    "                     'R_test': MRI_data_naive['R_test']*(2**T-1),\n",
    "                     'm': m_per_task * T, 'T': T,\n",
    "                    } \n",
    "MRI_data_temporal['class_balance'] = normalize((MRI_data_temporal['R_dev'].unsqueeze(1)==torch.arange(2**T, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                                dim=0, p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MRI_data_naive['class_balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=50.6702766418457\n",
      "iteration=300 loss=8.80715274810791\n",
      "iteration=600 loss=3.66485595703125\n",
      "iteration=900 loss=1.8281009197235107\n",
      "iteration=1200 loss=0.9653668999671936\n",
      "iteration=1500 loss=0.6120548844337463\n",
      "iteration=1800 loss=0.4767417907714844\n",
      "iteration=2100 loss=0.40450260043144226\n",
      "iteration=2400 loss=0.3607012629508972\n",
      "iteration=2700 loss=0.3310929536819458\n",
      "iteration=2999 loss=0.30959445238113403\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           # class_balance=MRI_data_naive['class_balance'], \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=3000, lr=4.087885261759692e-05,\n",
    "         momentum=0.9, clamp=True, seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_target = Y_dev.long()\n",
    "T = 5\n",
    "\n",
    "feasible_y = np.array([[-1, -1, -1, -1, -1],\n",
    "        [-1,  1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1],\n",
    "        [ 1,  1, -1, -1, -1],\n",
    "        [-1, -1,  1, -1, -1],\n",
    "        [-1,  1,  1, -1, -1],\n",
    "        [ 1, -1,  1, -1, -1],\n",
    "        [ 1,  1,  1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1],\n",
    "        [-1,  1, -1,  1, -1],\n",
    "        [ 1, -1, -1,  1, -1],\n",
    "        [ 1,  1, -1,  1, -1],\n",
    "        [-1, -1,  1,  1, -1],\n",
    "        [-1,  1,  1,  1, -1],\n",
    "        [ 1, -1,  1,  1, -1],\n",
    "        [ 1,  1,  1,  1, -1],\n",
    "        [-1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1,  1],\n",
    "        [ 1,  1, -1, -1,  1],\n",
    "        [-1, -1,  1, -1,  1],\n",
    "        [-1,  1,  1, -1,  1],\n",
    "        [ 1, -1,  1, -1,  1],\n",
    "        [ 1,  1,  1, -1,  1],\n",
    "        [-1, -1, -1,  1,  1],\n",
    "        [-1,  1, -1,  1,  1],\n",
    "        [ 1, -1, -1,  1,  1],\n",
    "        [ 1,  1, -1,  1,  1],\n",
    "        [-1, -1,  1,  1,  1],\n",
    "        [-1,  1,  1,  1,  1],\n",
    "        [ 1, -1,  1,  1,  1],\n",
    "        [ 1,  1,  1,  1,  1]])\n",
    "\n",
    "feasible_y[feasible_y==-1] = 0\n",
    "feasible_y = feasible_y.tolist()\n",
    "possibilities = list(map(lambda l : ''.join(map(str,l)), feasible_y))\n",
    "\n",
    "class_balance = np.empty(2 ** T)\n",
    "#compute class balance from dev set and use laplace smoothing\n",
    "\n",
    "valid_target_copy = np.copy(valid_target)\n",
    "valid_target_copy[valid_target_copy == 2] = 0\n",
    "\n",
    "assert len(valid_target_copy) % T == 0\n",
    "num_windows = len(valid_target_copy) / T\n",
    "\n",
    "freq = {}\n",
    "for i in range(0, len(valid_target_copy), T):\n",
    "    s = ''.join(map(str,valid_target_copy[i:i+T]))\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "for i in range(len(class_balance)):\n",
    "    if possibilities[i] in freq and freq[possibilities[i]] > 5:\n",
    "        class_balance[i] = (freq[possibilities[i]] + 1) / (num_windows + len(possibilities))\n",
    "    else:\n",
    "        class_balance[i] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0001\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.029\n",
      "F1: 0.278\n",
      "Recall: 0.201\n",
      "Precision: 0.447\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.163\n",
      "Recall: 0.136\n",
      "Precision: 0.204\n",
      "Accuracy: 0.025\n",
      "F1: 0.172\n",
      "Recall: 0.176\n",
      "Precision: 0.168\n",
      "\n",
      "Accuracy: 0.055\n",
      "F1: 0.341\n",
      "Recall: 0.377\n",
      "Precision: 0.311\n",
      "Accuracy: 0.011\n",
      "F1: 0.097\n",
      "Recall: 0.073\n",
      "Precision: 0.145\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.424\n",
      "Recall: 0.355\n",
      "Precision: 0.524\n",
      "Accuracy: 0.016\n",
      "F1: 0.075\n",
      "Recall: 0.110\n",
      "Precision: 0.057\n",
      "\n",
      "Accuracy: 0.028\n",
      "F1: 0.254\n",
      "Recall: 0.194\n",
      "Precision: 0.368\n",
      "Accuracy: 0.003\n",
      "F1: 0.035\n",
      "Recall: 0.018\n",
      "Precision: 0.455\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.038\n",
      "Recall: 0.040\n",
      "Precision: 0.035\n",
      "Accuracy: 0.074\n",
      "F1: 0.569\n",
      "Recall: 0.509\n",
      "Precision: 0.644\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.086\n",
      "Recall: 0.092\n",
      "Precision: 0.080\n",
      "Accuracy: 0.086\n",
      "F1: 0.573\n",
      "Recall: 0.593\n",
      "Precision: 0.555\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.064\n",
      "Recall: 0.099\n",
      "Precision: 0.047\n",
      "Accuracy: 0.020\n",
      "F1: 0.234\n",
      "Recall: 0.139\n",
      "Precision: 0.731\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.004\n",
      "Recall: 0.004\n",
      "Precision: 0.004\n",
      "Accuracy: 0.092\n",
      "F1: 0.540\n",
      "Recall: 0.634\n",
      "Precision: 0.470\n",
      "\n",
      "Accuracy: 0.061\n",
      "F1: 0.482\n",
      "Recall: 0.421\n",
      "Precision: 0.564\n",
      "Accuracy: 0.013\n",
      "F1: 0.079\n",
      "Recall: 0.092\n",
      "Precision: 0.069\n",
      "\n",
      "1 1e-05\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.029\n",
      "F1: 0.278\n",
      "Recall: 0.201\n",
      "Precision: 0.447\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.163\n",
      "Recall: 0.136\n",
      "Precision: 0.204\n",
      "Accuracy: 0.025\n",
      "F1: 0.172\n",
      "Recall: 0.176\n",
      "Precision: 0.168\n",
      "\n",
      "Accuracy: 0.055\n",
      "F1: 0.341\n",
      "Recall: 0.377\n",
      "Precision: 0.311\n",
      "Accuracy: 0.011\n",
      "F1: 0.097\n",
      "Recall: 0.073\n",
      "Precision: 0.145\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.424\n",
      "Recall: 0.355\n",
      "Precision: 0.524\n",
      "Accuracy: 0.016\n",
      "F1: 0.075\n",
      "Recall: 0.110\n",
      "Precision: 0.057\n",
      "\n",
      "Accuracy: 0.028\n",
      "F1: 0.254\n",
      "Recall: 0.194\n",
      "Precision: 0.368\n",
      "Accuracy: 0.003\n",
      "F1: 0.035\n",
      "Recall: 0.018\n",
      "Precision: 0.455\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.038\n",
      "Recall: 0.040\n",
      "Precision: 0.035\n",
      "Accuracy: 0.074\n",
      "F1: 0.569\n",
      "Recall: 0.509\n",
      "Precision: 0.644\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.086\n",
      "Recall: 0.092\n",
      "Precision: 0.080\n",
      "Accuracy: 0.086\n",
      "F1: 0.573\n",
      "Recall: 0.593\n",
      "Precision: 0.555\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.064\n",
      "Recall: 0.099\n",
      "Precision: 0.047\n",
      "Accuracy: 0.020\n",
      "F1: 0.234\n",
      "Recall: 0.139\n",
      "Precision: 0.731\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.004\n",
      "Recall: 0.004\n",
      "Precision: 0.004\n",
      "Accuracy: 0.092\n",
      "F1: 0.540\n",
      "Recall: 0.634\n",
      "Precision: 0.470\n",
      "\n",
      "Accuracy: 0.061\n",
      "F1: 0.482\n",
      "Recall: 0.421\n",
      "Precision: 0.564\n",
      "Accuracy: 0.013\n",
      "F1: 0.079\n",
      "Recall: 0.092\n",
      "Precision: 0.069\n",
      "\n",
      "1 1e-06\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.029\n",
      "F1: 0.278\n",
      "Recall: 0.201\n",
      "Precision: 0.447\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.163\n",
      "Recall: 0.136\n",
      "Precision: 0.204\n",
      "Accuracy: 0.025\n",
      "F1: 0.172\n",
      "Recall: 0.176\n",
      "Precision: 0.168\n",
      "\n",
      "Accuracy: 0.055\n",
      "F1: 0.341\n",
      "Recall: 0.377\n",
      "Precision: 0.311\n",
      "Accuracy: 0.011\n",
      "F1: 0.097\n",
      "Recall: 0.073\n",
      "Precision: 0.145\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.424\n",
      "Recall: 0.355\n",
      "Precision: 0.524\n",
      "Accuracy: 0.016\n",
      "F1: 0.075\n",
      "Recall: 0.110\n",
      "Precision: 0.057\n",
      "\n",
      "Accuracy: 0.028\n",
      "F1: 0.254\n",
      "Recall: 0.194\n",
      "Precision: 0.368\n",
      "Accuracy: 0.003\n",
      "F1: 0.035\n",
      "Recall: 0.018\n",
      "Precision: 0.455\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.038\n",
      "Recall: 0.040\n",
      "Precision: 0.035\n",
      "Accuracy: 0.074\n",
      "F1: 0.569\n",
      "Recall: 0.509\n",
      "Precision: 0.644\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.086\n",
      "Recall: 0.092\n",
      "Precision: 0.080\n",
      "Accuracy: 0.086\n",
      "F1: 0.573\n",
      "Recall: 0.593\n",
      "Precision: 0.555\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.064\n",
      "Recall: 0.099\n",
      "Precision: 0.047\n",
      "Accuracy: 0.020\n",
      "F1: 0.234\n",
      "Recall: 0.139\n",
      "Precision: 0.731\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.004\n",
      "Recall: 0.004\n",
      "Precision: 0.004\n",
      "Accuracy: 0.092\n",
      "F1: 0.540\n",
      "Recall: 0.634\n",
      "Precision: 0.470\n",
      "\n",
      "Accuracy: 0.061\n",
      "F1: 0.482\n",
      "Recall: 0.421\n",
      "Precision: 0.564\n",
      "Accuracy: 0.013\n",
      "F1: 0.079\n",
      "Recall: 0.092\n",
      "Precision: 0.069\n",
      "\n",
      "1 1e-07\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.029\n",
      "F1: 0.278\n",
      "Recall: 0.201\n",
      "Precision: 0.447\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.163\n",
      "Recall: 0.136\n",
      "Precision: 0.204\n",
      "Accuracy: 0.025\n",
      "F1: 0.172\n",
      "Recall: 0.176\n",
      "Precision: 0.168\n",
      "\n",
      "Accuracy: 0.055\n",
      "F1: 0.341\n",
      "Recall: 0.377\n",
      "Precision: 0.311\n",
      "Accuracy: 0.011\n",
      "F1: 0.097\n",
      "Recall: 0.073\n",
      "Precision: 0.145\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.424\n",
      "Recall: 0.355\n",
      "Precision: 0.524\n",
      "Accuracy: 0.016\n",
      "F1: 0.075\n",
      "Recall: 0.110\n",
      "Precision: 0.057\n",
      "\n",
      "Accuracy: 0.028\n",
      "F1: 0.254\n",
      "Recall: 0.194\n",
      "Precision: 0.368\n",
      "Accuracy: 0.003\n",
      "F1: 0.035\n",
      "Recall: 0.018\n",
      "Precision: 0.455\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.038\n",
      "Recall: 0.040\n",
      "Precision: 0.035\n",
      "Accuracy: 0.074\n",
      "F1: 0.569\n",
      "Recall: 0.509\n",
      "Precision: 0.644\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.086\n",
      "Recall: 0.092\n",
      "Precision: 0.080\n",
      "Accuracy: 0.086\n",
      "F1: 0.573\n",
      "Recall: 0.593\n",
      "Precision: 0.555\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.064\n",
      "Recall: 0.099\n",
      "Precision: 0.047\n",
      "Accuracy: 0.020\n",
      "F1: 0.234\n",
      "Recall: 0.139\n",
      "Precision: 0.731\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.004\n",
      "Recall: 0.004\n",
      "Precision: 0.004\n",
      "Accuracy: 0.092\n",
      "F1: 0.540\n",
      "Recall: 0.634\n",
      "Precision: 0.470\n",
      "\n",
      "Accuracy: 0.061\n",
      "F1: 0.482\n",
      "Recall: 0.421\n",
      "Precision: 0.564\n",
      "Accuracy: 0.013\n",
      "F1: 0.079\n",
      "Recall: 0.092\n",
      "Precision: 0.069\n",
      "\n",
      "5 0.0001\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.065\n",
      "F1: 0.516\n",
      "Recall: 0.451\n",
      "Precision: 0.603\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.509\n",
      "Recall: 0.487\n",
      "Precision: 0.532\n",
      "Accuracy: 0.011\n",
      "F1: 0.067\n",
      "Recall: 0.077\n",
      "Precision: 0.059\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.443\n",
      "Recall: 0.465\n",
      "Precision: 0.423\n",
      "Accuracy: 0.006\n",
      "F1: 0.048\n",
      "Recall: 0.044\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.537\n",
      "Recall: 0.487\n",
      "Precision: 0.599\n",
      "Accuracy: 0.008\n",
      "F1: 0.036\n",
      "Recall: 0.055\n",
      "Precision: 0.027\n",
      "\n",
      "Accuracy: 0.043\n",
      "F1: 0.410\n",
      "Recall: 0.297\n",
      "Precision: 0.664\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.004\n",
      "Precision: 0.008\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.124\n",
      "Recall: 0.088\n",
      "Precision: 0.209\n",
      "Accuracy: 0.037\n",
      "F1: 0.337\n",
      "Recall: 0.256\n",
      "Precision: 0.493\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.120\n",
      "Recall: 0.095\n",
      "Precision: 0.163\n",
      "Accuracy: 0.068\n",
      "F1: 0.415\n",
      "Recall: 0.469\n",
      "Precision: 0.372\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.421\n",
      "Recall: 0.568\n",
      "Precision: 0.335\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.004\n",
      "Precision: 0.023\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.088\n",
      "F1: 0.426\n",
      "Recall: 0.608\n",
      "Precision: 0.328\n",
      "\n",
      "Accuracy: 0.109\n",
      "F1: 0.693\n",
      "Recall: 0.751\n",
      "Precision: 0.643\n",
      "Accuracy: 0.004\n",
      "F1: 0.018\n",
      "Recall: 0.026\n",
      "Precision: 0.013\n",
      "\n",
      "5 1e-05\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.065\n",
      "F1: 0.516\n",
      "Recall: 0.451\n",
      "Precision: 0.603\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.509\n",
      "Recall: 0.487\n",
      "Precision: 0.532\n",
      "Accuracy: 0.011\n",
      "F1: 0.067\n",
      "Recall: 0.077\n",
      "Precision: 0.059\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.443\n",
      "Recall: 0.465\n",
      "Precision: 0.423\n",
      "Accuracy: 0.006\n",
      "F1: 0.048\n",
      "Recall: 0.044\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.537\n",
      "Recall: 0.487\n",
      "Precision: 0.599\n",
      "Accuracy: 0.008\n",
      "F1: 0.036\n",
      "Recall: 0.055\n",
      "Precision: 0.027\n",
      "\n",
      "Accuracy: 0.043\n",
      "F1: 0.410\n",
      "Recall: 0.297\n",
      "Precision: 0.664\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.004\n",
      "Precision: 0.008\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.124\n",
      "Recall: 0.088\n",
      "Precision: 0.209\n",
      "Accuracy: 0.037\n",
      "F1: 0.337\n",
      "Recall: 0.256\n",
      "Precision: 0.493\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.120\n",
      "Recall: 0.095\n",
      "Precision: 0.163\n",
      "Accuracy: 0.068\n",
      "F1: 0.415\n",
      "Recall: 0.469\n",
      "Precision: 0.372\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.421\n",
      "Recall: 0.568\n",
      "Precision: 0.335\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.004\n",
      "Precision: 0.023\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.088\n",
      "F1: 0.426\n",
      "Recall: 0.608\n",
      "Precision: 0.328\n",
      "\n",
      "Accuracy: 0.109\n",
      "F1: 0.693\n",
      "Recall: 0.751\n",
      "Precision: 0.643\n",
      "Accuracy: 0.004\n",
      "F1: 0.018\n",
      "Recall: 0.026\n",
      "Precision: 0.013\n",
      "\n",
      "5 1e-06\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.065\n",
      "F1: 0.516\n",
      "Recall: 0.451\n",
      "Precision: 0.603\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.509\n",
      "Recall: 0.487\n",
      "Precision: 0.532\n",
      "Accuracy: 0.011\n",
      "F1: 0.067\n",
      "Recall: 0.077\n",
      "Precision: 0.059\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.443\n",
      "Recall: 0.465\n",
      "Precision: 0.423\n",
      "Accuracy: 0.006\n",
      "F1: 0.048\n",
      "Recall: 0.044\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.537\n",
      "Recall: 0.487\n",
      "Precision: 0.599\n",
      "Accuracy: 0.008\n",
      "F1: 0.036\n",
      "Recall: 0.055\n",
      "Precision: 0.027\n",
      "\n",
      "Accuracy: 0.043\n",
      "F1: 0.410\n",
      "Recall: 0.297\n",
      "Precision: 0.664\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.004\n",
      "Precision: 0.008\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.124\n",
      "Recall: 0.088\n",
      "Precision: 0.209\n",
      "Accuracy: 0.037\n",
      "F1: 0.337\n",
      "Recall: 0.256\n",
      "Precision: 0.493\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.120\n",
      "Recall: 0.095\n",
      "Precision: 0.163\n",
      "Accuracy: 0.068\n",
      "F1: 0.415\n",
      "Recall: 0.469\n",
      "Precision: 0.372\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.421\n",
      "Recall: 0.568\n",
      "Precision: 0.335\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.004\n",
      "Precision: 0.023\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.088\n",
      "F1: 0.426\n",
      "Recall: 0.608\n",
      "Precision: 0.328\n",
      "\n",
      "Accuracy: 0.109\n",
      "F1: 0.693\n",
      "Recall: 0.751\n",
      "Precision: 0.643\n",
      "Accuracy: 0.004\n",
      "F1: 0.018\n",
      "Recall: 0.026\n",
      "Precision: 0.013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 1e-07\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.065\n",
      "F1: 0.516\n",
      "Recall: 0.451\n",
      "Precision: 0.603\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.509\n",
      "Recall: 0.487\n",
      "Precision: 0.532\n",
      "Accuracy: 0.011\n",
      "F1: 0.067\n",
      "Recall: 0.077\n",
      "Precision: 0.059\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.443\n",
      "Recall: 0.465\n",
      "Precision: 0.423\n",
      "Accuracy: 0.006\n",
      "F1: 0.048\n",
      "Recall: 0.044\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.537\n",
      "Recall: 0.487\n",
      "Precision: 0.599\n",
      "Accuracy: 0.008\n",
      "F1: 0.036\n",
      "Recall: 0.055\n",
      "Precision: 0.027\n",
      "\n",
      "Accuracy: 0.043\n",
      "F1: 0.410\n",
      "Recall: 0.297\n",
      "Precision: 0.664\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.004\n",
      "Precision: 0.008\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.124\n",
      "Recall: 0.088\n",
      "Precision: 0.209\n",
      "Accuracy: 0.037\n",
      "F1: 0.337\n",
      "Recall: 0.256\n",
      "Precision: 0.493\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.120\n",
      "Recall: 0.095\n",
      "Precision: 0.163\n",
      "Accuracy: 0.068\n",
      "F1: 0.415\n",
      "Recall: 0.469\n",
      "Precision: 0.372\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.421\n",
      "Recall: 0.568\n",
      "Precision: 0.335\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.004\n",
      "Precision: 0.023\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.088\n",
      "F1: 0.426\n",
      "Recall: 0.608\n",
      "Precision: 0.328\n",
      "\n",
      "Accuracy: 0.109\n",
      "F1: 0.693\n",
      "Recall: 0.751\n",
      "Precision: 0.643\n",
      "Accuracy: 0.004\n",
      "F1: 0.018\n",
      "Recall: 0.026\n",
      "Precision: 0.013\n",
      "\n",
      "10 0.0001\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=1 loss=3539.021484375\n",
      "iteration=2 loss=2704.11767578125\n",
      "iteration=3 loss=2059.25732421875\n",
      "iteration=4 loss=1668.4140625\n",
      "iteration=5 loss=1488.377685546875\n",
      "iteration=6 loss=1452.6092529296875\n",
      "iteration=7 loss=1492.25341796875\n",
      "iteration=8 loss=1544.5687255859375\n",
      "iteration=9 loss=1543.9228515625\n",
      "iteration=9 loss=1543.9228515625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=1 loss=4892.5185546875\n",
      "iteration=2 loss=3383.08642578125\n",
      "iteration=3 loss=2348.660888671875\n",
      "iteration=4 loss=1735.2056884765625\n",
      "iteration=5 loss=1416.473388671875\n",
      "iteration=6 loss=1304.972900390625\n",
      "iteration=7 loss=1327.30908203125\n",
      "iteration=8 loss=1402.6170654296875\n",
      "iteration=9 loss=1453.33349609375\n",
      "iteration=9 loss=1453.33349609375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=1 loss=3328.276123046875\n",
      "iteration=2 loss=2479.249267578125\n",
      "iteration=3 loss=1875.35595703125\n",
      "iteration=4 loss=1548.63671875\n",
      "iteration=5 loss=1428.4415283203125\n",
      "iteration=6 loss=1433.0277099609375\n",
      "iteration=7 loss=1487.889892578125\n",
      "iteration=8 loss=1533.5693359375\n",
      "iteration=9 loss=1536.08056640625\n",
      "iteration=9 loss=1536.08056640625\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=1 loss=2481.39501953125\n",
      "iteration=2 loss=2039.705078125\n",
      "iteration=3 loss=1697.630126953125\n",
      "iteration=4 loss=1492.5377197265625\n",
      "iteration=5 loss=1399.025634765625\n",
      "iteration=6 loss=1377.48876953125\n",
      "iteration=7 loss=1389.3927001953125\n",
      "iteration=8 loss=1402.904052734375\n",
      "iteration=9 loss=1398.01708984375\n",
      "iteration=9 loss=1398.01708984375\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=1 loss=4421.833984375\n",
      "iteration=2 loss=3048.15185546875\n",
      "iteration=3 loss=2148.61767578125\n",
      "iteration=4 loss=1655.8072509765625\n",
      "iteration=5 loss=1457.34814453125\n",
      "iteration=6 loss=1469.5201416015625\n",
      "iteration=7 loss=1591.4625244140625\n",
      "iteration=8 loss=1695.7647705078125\n",
      "iteration=9 loss=1690.73291015625\n",
      "iteration=9 loss=1690.73291015625\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=1 loss=4362.61328125\n",
      "iteration=2 loss=3065.883544921875\n",
      "iteration=3 loss=2187.741943359375\n",
      "iteration=4 loss=1671.1636962890625\n",
      "iteration=5 loss=1407.3260498046875\n",
      "iteration=6 loss=1323.041015625\n",
      "iteration=7 loss=1355.1290283203125\n",
      "iteration=8 loss=1431.5316162109375\n",
      "iteration=9 loss=1482.7205810546875\n",
      "iteration=9 loss=1482.7205810546875\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=1 loss=5604.759765625\n",
      "iteration=2 loss=3370.725341796875\n",
      "iteration=3 loss=2224.6298828125\n",
      "iteration=4 loss=1672.1434326171875\n",
      "iteration=5 loss=1478.7574462890625\n",
      "iteration=6 loss=1538.2391357421875\n",
      "iteration=7 loss=1730.170654296875\n",
      "iteration=8 loss=1895.58984375\n",
      "iteration=9 loss=1918.4873046875\n",
      "iteration=9 loss=1918.4873046875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=1 loss=5834.65625\n",
      "iteration=2 loss=3109.437255859375\n",
      "iteration=3 loss=1971.01416015625\n",
      "iteration=4 loss=1516.2352294921875\n",
      "iteration=5 loss=1462.040283203125\n",
      "iteration=6 loss=1676.838134765625\n",
      "iteration=7 loss=1964.9859619140625\n",
      "iteration=8 loss=2043.836669921875\n",
      "iteration=9 loss=1947.2506103515625\n",
      "iteration=9 loss=1947.2506103515625\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=1 loss=5482.2373046875\n",
      "iteration=2 loss=2892.278564453125\n",
      "iteration=3 loss=1851.15869140625\n",
      "iteration=4 loss=1502.74365234375\n",
      "iteration=5 loss=1518.0216064453125\n",
      "iteration=6 loss=1722.3021240234375\n",
      "iteration=7 loss=1851.421875\n",
      "iteration=8 loss=1837.415283203125\n",
      "iteration=9 loss=1730.5343017578125\n",
      "iteration=9 loss=1730.5343017578125\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=1 loss=3923.010498046875\n",
      "iteration=2 loss=2697.99658203125\n",
      "iteration=3 loss=1894.3621826171875\n",
      "iteration=4 loss=1441.399169921875\n",
      "iteration=5 loss=1229.6041259765625\n",
      "iteration=6 loss=1185.0130615234375\n",
      "iteration=7 loss=1240.6678466796875\n",
      "iteration=8 loss=1321.4434814453125\n",
      "iteration=9 loss=1361.527587890625\n",
      "iteration=9 loss=1361.527587890625\n",
      "Accuracy: 0.064\n",
      "F1: 0.507\n",
      "Recall: 0.440\n",
      "Precision: 0.600\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.081\n",
      "F1: 0.571\n",
      "Recall: 0.557\n",
      "Precision: 0.587\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.007\n",
      "Precision: 0.006\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.539\n",
      "Recall: 0.593\n",
      "Precision: 0.494\n",
      "Accuracy: 0.002\n",
      "F1: 0.011\n",
      "Recall: 0.011\n",
      "Precision: 0.010\n",
      "\n",
      "Accuracy: 0.091\n",
      "F1: 0.618\n",
      "Recall: 0.626\n",
      "Precision: 0.611\n",
      "Accuracy: 0.006\n",
      "F1: 0.027\n",
      "Recall: 0.040\n",
      "Precision: 0.020\n",
      "\n",
      "Accuracy: 0.085\n",
      "F1: 0.618\n",
      "Recall: 0.586\n",
      "Precision: 0.653\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.085\n",
      "F1: 0.557\n",
      "Recall: 0.590\n",
      "Precision: 0.528\n",
      "Accuracy: 0.006\n",
      "F1: 0.052\n",
      "Recall: 0.040\n",
      "Precision: 0.074\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.497\n",
      "Recall: 0.487\n",
      "Precision: 0.508\n",
      "Accuracy: 0.013\n",
      "F1: 0.075\n",
      "Recall: 0.088\n",
      "Precision: 0.065\n",
      "\n",
      "Accuracy: 0.118\n",
      "F1: 0.553\n",
      "Recall: 0.817\n",
      "Precision: 0.418\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.568\n",
      "Recall: 0.465\n",
      "Precision: 0.730\n",
      "Accuracy: 0.025\n",
      "F1: 0.134\n",
      "Recall: 0.176\n",
      "Precision: 0.108\n",
      "\n",
      "Accuracy: 0.119\n",
      "F1: 0.677\n",
      "Recall: 0.824\n",
      "Precision: 0.574\n",
      "Accuracy: 0.001\n",
      "F1: 0.003\n",
      "Recall: 0.004\n",
      "Precision: 0.002\n",
      "\n",
      "10 1e-05\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=1 loss=3539.021484375\n",
      "iteration=2 loss=2704.11767578125\n",
      "iteration=3 loss=2059.25732421875\n",
      "iteration=4 loss=1668.4140625\n",
      "iteration=5 loss=1488.377685546875\n",
      "iteration=6 loss=1452.6092529296875\n",
      "iteration=7 loss=1492.25341796875\n",
      "iteration=8 loss=1544.5687255859375\n",
      "iteration=9 loss=1543.9228515625\n",
      "iteration=9 loss=1543.9228515625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=1 loss=4892.5185546875\n",
      "iteration=2 loss=3383.08642578125\n",
      "iteration=3 loss=2348.660888671875\n",
      "iteration=4 loss=1735.2056884765625\n",
      "iteration=5 loss=1416.473388671875\n",
      "iteration=6 loss=1304.972900390625\n",
      "iteration=7 loss=1327.30908203125\n",
      "iteration=8 loss=1402.6170654296875\n",
      "iteration=9 loss=1453.33349609375\n",
      "iteration=9 loss=1453.33349609375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=1 loss=3328.276123046875\n",
      "iteration=2 loss=2479.249267578125\n",
      "iteration=3 loss=1875.35595703125\n",
      "iteration=4 loss=1548.63671875\n",
      "iteration=5 loss=1428.4415283203125\n",
      "iteration=6 loss=1433.0277099609375\n",
      "iteration=7 loss=1487.889892578125\n",
      "iteration=8 loss=1533.5693359375\n",
      "iteration=9 loss=1536.08056640625\n",
      "iteration=9 loss=1536.08056640625\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=1 loss=2481.39501953125\n",
      "iteration=2 loss=2039.705078125\n",
      "iteration=3 loss=1697.630126953125\n",
      "iteration=4 loss=1492.5377197265625\n",
      "iteration=5 loss=1399.025634765625\n",
      "iteration=6 loss=1377.48876953125\n",
      "iteration=7 loss=1389.3927001953125\n",
      "iteration=8 loss=1402.904052734375\n",
      "iteration=9 loss=1398.01708984375\n",
      "iteration=9 loss=1398.01708984375\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=1 loss=4421.833984375\n",
      "iteration=2 loss=3048.15185546875\n",
      "iteration=3 loss=2148.61767578125\n",
      "iteration=4 loss=1655.8072509765625\n",
      "iteration=5 loss=1457.34814453125\n",
      "iteration=6 loss=1469.5201416015625\n",
      "iteration=7 loss=1591.4625244140625\n",
      "iteration=8 loss=1695.7647705078125\n",
      "iteration=9 loss=1690.73291015625\n",
      "iteration=9 loss=1690.73291015625\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=1 loss=4362.61328125\n",
      "iteration=2 loss=3065.883544921875\n",
      "iteration=3 loss=2187.741943359375\n",
      "iteration=4 loss=1671.1636962890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=5 loss=1407.3260498046875\n",
      "iteration=6 loss=1323.041015625\n",
      "iteration=7 loss=1355.1290283203125\n",
      "iteration=8 loss=1431.5316162109375\n",
      "iteration=9 loss=1482.7205810546875\n",
      "iteration=9 loss=1482.7205810546875\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=1 loss=5604.759765625\n",
      "iteration=2 loss=3370.725341796875\n",
      "iteration=3 loss=2224.6298828125\n",
      "iteration=4 loss=1672.1434326171875\n",
      "iteration=5 loss=1478.7574462890625\n",
      "iteration=6 loss=1538.2391357421875\n",
      "iteration=7 loss=1730.170654296875\n",
      "iteration=8 loss=1895.58984375\n",
      "iteration=9 loss=1918.4873046875\n",
      "iteration=9 loss=1918.4873046875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=1 loss=5834.65625\n",
      "iteration=2 loss=3109.437255859375\n",
      "iteration=3 loss=1971.01416015625\n",
      "iteration=4 loss=1516.2352294921875\n",
      "iteration=5 loss=1462.040283203125\n",
      "iteration=6 loss=1676.838134765625\n",
      "iteration=7 loss=1964.9859619140625\n",
      "iteration=8 loss=2043.836669921875\n",
      "iteration=9 loss=1947.2506103515625\n",
      "iteration=9 loss=1947.2506103515625\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=1 loss=5482.2373046875\n",
      "iteration=2 loss=2892.278564453125\n",
      "iteration=3 loss=1851.15869140625\n",
      "iteration=4 loss=1502.74365234375\n",
      "iteration=5 loss=1518.0216064453125\n",
      "iteration=6 loss=1722.3021240234375\n",
      "iteration=7 loss=1851.421875\n",
      "iteration=8 loss=1837.415283203125\n",
      "iteration=9 loss=1730.5343017578125\n",
      "iteration=9 loss=1730.5343017578125\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=1 loss=3923.010498046875\n",
      "iteration=2 loss=2697.99658203125\n",
      "iteration=3 loss=1894.3621826171875\n",
      "iteration=4 loss=1441.399169921875\n",
      "iteration=5 loss=1229.6041259765625\n",
      "iteration=6 loss=1185.0130615234375\n",
      "iteration=7 loss=1240.6678466796875\n",
      "iteration=8 loss=1321.4434814453125\n",
      "iteration=9 loss=1361.527587890625\n",
      "iteration=9 loss=1361.527587890625\n",
      "Accuracy: 0.064\n",
      "F1: 0.507\n",
      "Recall: 0.440\n",
      "Precision: 0.600\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.081\n",
      "F1: 0.571\n",
      "Recall: 0.557\n",
      "Precision: 0.587\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.007\n",
      "Precision: 0.006\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.539\n",
      "Recall: 0.593\n",
      "Precision: 0.494\n",
      "Accuracy: 0.002\n",
      "F1: 0.011\n",
      "Recall: 0.011\n",
      "Precision: 0.010\n",
      "\n",
      "Accuracy: 0.091\n",
      "F1: 0.618\n",
      "Recall: 0.626\n",
      "Precision: 0.611\n",
      "Accuracy: 0.006\n",
      "F1: 0.027\n",
      "Recall: 0.040\n",
      "Precision: 0.020\n",
      "\n",
      "Accuracy: 0.085\n",
      "F1: 0.618\n",
      "Recall: 0.586\n",
      "Precision: 0.653\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.085\n",
      "F1: 0.557\n",
      "Recall: 0.590\n",
      "Precision: 0.528\n",
      "Accuracy: 0.006\n",
      "F1: 0.052\n",
      "Recall: 0.040\n",
      "Precision: 0.074\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.497\n",
      "Recall: 0.487\n",
      "Precision: 0.508\n",
      "Accuracy: 0.013\n",
      "F1: 0.075\n",
      "Recall: 0.088\n",
      "Precision: 0.065\n",
      "\n",
      "Accuracy: 0.118\n",
      "F1: 0.553\n",
      "Recall: 0.817\n",
      "Precision: 0.418\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.568\n",
      "Recall: 0.465\n",
      "Precision: 0.730\n",
      "Accuracy: 0.025\n",
      "F1: 0.134\n",
      "Recall: 0.176\n",
      "Precision: 0.108\n",
      "\n",
      "Accuracy: 0.119\n",
      "F1: 0.677\n",
      "Recall: 0.824\n",
      "Precision: 0.574\n",
      "Accuracy: 0.001\n",
      "F1: 0.003\n",
      "Recall: 0.004\n",
      "Precision: 0.002\n",
      "\n",
      "10 1e-06\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=1 loss=3539.021484375\n",
      "iteration=2 loss=2704.11767578125\n",
      "iteration=3 loss=2059.25732421875\n",
      "iteration=4 loss=1668.4140625\n",
      "iteration=5 loss=1488.377685546875\n",
      "iteration=6 loss=1452.6092529296875\n",
      "iteration=7 loss=1492.25341796875\n",
      "iteration=8 loss=1544.5687255859375\n",
      "iteration=9 loss=1543.9228515625\n",
      "iteration=9 loss=1543.9228515625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=1 loss=4892.5185546875\n",
      "iteration=2 loss=3383.08642578125\n",
      "iteration=3 loss=2348.660888671875\n",
      "iteration=4 loss=1735.2056884765625\n",
      "iteration=5 loss=1416.473388671875\n",
      "iteration=6 loss=1304.972900390625\n",
      "iteration=7 loss=1327.30908203125\n",
      "iteration=8 loss=1402.6170654296875\n",
      "iteration=9 loss=1453.33349609375\n",
      "iteration=9 loss=1453.33349609375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=1 loss=3328.276123046875\n",
      "iteration=2 loss=2479.249267578125\n",
      "iteration=3 loss=1875.35595703125\n",
      "iteration=4 loss=1548.63671875\n",
      "iteration=5 loss=1428.4415283203125\n",
      "iteration=6 loss=1433.0277099609375\n",
      "iteration=7 loss=1487.889892578125\n",
      "iteration=8 loss=1533.5693359375\n",
      "iteration=9 loss=1536.08056640625\n",
      "iteration=9 loss=1536.08056640625\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=1 loss=2481.39501953125\n",
      "iteration=2 loss=2039.705078125\n",
      "iteration=3 loss=1697.630126953125\n",
      "iteration=4 loss=1492.5377197265625\n",
      "iteration=5 loss=1399.025634765625\n",
      "iteration=6 loss=1377.48876953125\n",
      "iteration=7 loss=1389.3927001953125\n",
      "iteration=8 loss=1402.904052734375\n",
      "iteration=9 loss=1398.01708984375\n",
      "iteration=9 loss=1398.01708984375\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=1 loss=4421.833984375\n",
      "iteration=2 loss=3048.15185546875\n",
      "iteration=3 loss=2148.61767578125\n",
      "iteration=4 loss=1655.8072509765625\n",
      "iteration=5 loss=1457.34814453125\n",
      "iteration=6 loss=1469.5201416015625\n",
      "iteration=7 loss=1591.4625244140625\n",
      "iteration=8 loss=1695.7647705078125\n",
      "iteration=9 loss=1690.73291015625\n",
      "iteration=9 loss=1690.73291015625\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=1 loss=4362.61328125\n",
      "iteration=2 loss=3065.883544921875\n",
      "iteration=3 loss=2187.741943359375\n",
      "iteration=4 loss=1671.1636962890625\n",
      "iteration=5 loss=1407.3260498046875\n",
      "iteration=6 loss=1323.041015625\n",
      "iteration=7 loss=1355.1290283203125\n",
      "iteration=8 loss=1431.5316162109375\n",
      "iteration=9 loss=1482.7205810546875\n",
      "iteration=9 loss=1482.7205810546875\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=1 loss=5604.759765625\n",
      "iteration=2 loss=3370.725341796875\n",
      "iteration=3 loss=2224.6298828125\n",
      "iteration=4 loss=1672.1434326171875\n",
      "iteration=5 loss=1478.7574462890625\n",
      "iteration=6 loss=1538.2391357421875\n",
      "iteration=7 loss=1730.170654296875\n",
      "iteration=8 loss=1895.58984375\n",
      "iteration=9 loss=1918.4873046875\n",
      "iteration=9 loss=1918.4873046875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=1 loss=5834.65625\n",
      "iteration=2 loss=3109.437255859375\n",
      "iteration=3 loss=1971.01416015625\n",
      "iteration=4 loss=1516.2352294921875\n",
      "iteration=5 loss=1462.040283203125\n",
      "iteration=6 loss=1676.838134765625\n",
      "iteration=7 loss=1964.9859619140625\n",
      "iteration=8 loss=2043.836669921875\n",
      "iteration=9 loss=1947.2506103515625\n",
      "iteration=9 loss=1947.2506103515625\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=1 loss=5482.2373046875\n",
      "iteration=2 loss=2892.278564453125\n",
      "iteration=3 loss=1851.15869140625\n",
      "iteration=4 loss=1502.74365234375\n",
      "iteration=5 loss=1518.0216064453125\n",
      "iteration=6 loss=1722.3021240234375\n",
      "iteration=7 loss=1851.421875\n",
      "iteration=8 loss=1837.415283203125\n",
      "iteration=9 loss=1730.5343017578125\n",
      "iteration=9 loss=1730.5343017578125\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=1 loss=3923.010498046875\n",
      "iteration=2 loss=2697.99658203125\n",
      "iteration=3 loss=1894.3621826171875\n",
      "iteration=4 loss=1441.399169921875\n",
      "iteration=5 loss=1229.6041259765625\n",
      "iteration=6 loss=1185.0130615234375\n",
      "iteration=7 loss=1240.6678466796875\n",
      "iteration=8 loss=1321.4434814453125\n",
      "iteration=9 loss=1361.527587890625\n",
      "iteration=9 loss=1361.527587890625\n",
      "Accuracy: 0.064\n",
      "F1: 0.507\n",
      "Recall: 0.440\n",
      "Precision: 0.600\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.081\n",
      "F1: 0.571\n",
      "Recall: 0.557\n",
      "Precision: 0.587\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.007\n",
      "Precision: 0.006\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.539\n",
      "Recall: 0.593\n",
      "Precision: 0.494\n",
      "Accuracy: 0.002\n",
      "F1: 0.011\n",
      "Recall: 0.011\n",
      "Precision: 0.010\n",
      "\n",
      "Accuracy: 0.091\n",
      "F1: 0.618\n",
      "Recall: 0.626\n",
      "Precision: 0.611\n",
      "Accuracy: 0.006\n",
      "F1: 0.027\n",
      "Recall: 0.040\n",
      "Precision: 0.020\n",
      "\n",
      "Accuracy: 0.085\n",
      "F1: 0.618\n",
      "Recall: 0.586\n",
      "Precision: 0.653\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.085\n",
      "F1: 0.557\n",
      "Recall: 0.590\n",
      "Precision: 0.528\n",
      "Accuracy: 0.006\n",
      "F1: 0.052\n",
      "Recall: 0.040\n",
      "Precision: 0.074\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.497\n",
      "Recall: 0.487\n",
      "Precision: 0.508\n",
      "Accuracy: 0.013\n",
      "F1: 0.075\n",
      "Recall: 0.088\n",
      "Precision: 0.065\n",
      "\n",
      "Accuracy: 0.118\n",
      "F1: 0.553\n",
      "Recall: 0.817\n",
      "Precision: 0.418\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.568\n",
      "Recall: 0.465\n",
      "Precision: 0.730\n",
      "Accuracy: 0.025\n",
      "F1: 0.134\n",
      "Recall: 0.176\n",
      "Precision: 0.108\n",
      "\n",
      "Accuracy: 0.119\n",
      "F1: 0.677\n",
      "Recall: 0.824\n",
      "Precision: 0.574\n",
      "Accuracy: 0.001\n",
      "F1: 0.003\n",
      "Recall: 0.004\n",
      "Precision: 0.002\n",
      "\n",
      "10 1e-07\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=1 loss=3539.021484375\n",
      "iteration=2 loss=2704.11767578125\n",
      "iteration=3 loss=2059.25732421875\n",
      "iteration=4 loss=1668.4140625\n",
      "iteration=5 loss=1488.377685546875\n",
      "iteration=6 loss=1452.6092529296875\n",
      "iteration=7 loss=1492.25341796875\n",
      "iteration=8 loss=1544.5687255859375\n",
      "iteration=9 loss=1543.9228515625\n",
      "iteration=9 loss=1543.9228515625\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=6340.234375\n",
      "iteration=1 loss=4892.5185546875\n",
      "iteration=2 loss=3383.08642578125\n",
      "iteration=3 loss=2348.660888671875\n",
      "iteration=4 loss=1735.2056884765625\n",
      "iteration=5 loss=1416.473388671875\n",
      "iteration=6 loss=1304.972900390625\n",
      "iteration=7 loss=1327.30908203125\n",
      "iteration=8 loss=1402.6170654296875\n",
      "iteration=9 loss=1453.33349609375\n",
      "iteration=9 loss=1453.33349609375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=1 loss=3328.276123046875\n",
      "iteration=2 loss=2479.249267578125\n",
      "iteration=3 loss=1875.35595703125\n",
      "iteration=4 loss=1548.63671875\n",
      "iteration=5 loss=1428.4415283203125\n",
      "iteration=6 loss=1433.0277099609375\n",
      "iteration=7 loss=1487.889892578125\n",
      "iteration=8 loss=1533.5693359375\n",
      "iteration=9 loss=1536.08056640625\n",
      "iteration=9 loss=1536.08056640625\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=1 loss=2481.39501953125\n",
      "iteration=2 loss=2039.705078125\n",
      "iteration=3 loss=1697.630126953125\n",
      "iteration=4 loss=1492.5377197265625\n",
      "iteration=5 loss=1399.025634765625\n",
      "iteration=6 loss=1377.48876953125\n",
      "iteration=7 loss=1389.3927001953125\n",
      "iteration=8 loss=1402.904052734375\n",
      "iteration=9 loss=1398.01708984375\n",
      "iteration=9 loss=1398.01708984375\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=1 loss=4421.833984375\n",
      "iteration=2 loss=3048.15185546875\n",
      "iteration=3 loss=2148.61767578125\n",
      "iteration=4 loss=1655.8072509765625\n",
      "iteration=5 loss=1457.34814453125\n",
      "iteration=6 loss=1469.5201416015625\n",
      "iteration=7 loss=1591.4625244140625\n",
      "iteration=8 loss=1695.7647705078125\n",
      "iteration=9 loss=1690.73291015625\n",
      "iteration=9 loss=1690.73291015625\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=1 loss=4362.61328125\n",
      "iteration=2 loss=3065.883544921875\n",
      "iteration=3 loss=2187.741943359375\n",
      "iteration=4 loss=1671.1636962890625\n",
      "iteration=5 loss=1407.3260498046875\n",
      "iteration=6 loss=1323.041015625\n",
      "iteration=7 loss=1355.1290283203125\n",
      "iteration=8 loss=1431.5316162109375\n",
      "iteration=9 loss=1482.7205810546875\n",
      "iteration=9 loss=1482.7205810546875\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=1 loss=5604.759765625\n",
      "iteration=2 loss=3370.725341796875\n",
      "iteration=3 loss=2224.6298828125\n",
      "iteration=4 loss=1672.1434326171875\n",
      "iteration=5 loss=1478.7574462890625\n",
      "iteration=6 loss=1538.2391357421875\n",
      "iteration=7 loss=1730.170654296875\n",
      "iteration=8 loss=1895.58984375\n",
      "iteration=9 loss=1918.4873046875\n",
      "iteration=9 loss=1918.4873046875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=1 loss=5834.65625\n",
      "iteration=2 loss=3109.437255859375\n",
      "iteration=3 loss=1971.01416015625\n",
      "iteration=4 loss=1516.2352294921875\n",
      "iteration=5 loss=1462.040283203125\n",
      "iteration=6 loss=1676.838134765625\n",
      "iteration=7 loss=1964.9859619140625\n",
      "iteration=8 loss=2043.836669921875\n",
      "iteration=9 loss=1947.2506103515625\n",
      "iteration=9 loss=1947.2506103515625\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=1 loss=5482.2373046875\n",
      "iteration=2 loss=2892.278564453125\n",
      "iteration=3 loss=1851.15869140625\n",
      "iteration=4 loss=1502.74365234375\n",
      "iteration=5 loss=1518.0216064453125\n",
      "iteration=6 loss=1722.3021240234375\n",
      "iteration=7 loss=1851.421875\n",
      "iteration=8 loss=1837.415283203125\n",
      "iteration=9 loss=1730.5343017578125\n",
      "iteration=9 loss=1730.5343017578125\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=1 loss=3923.010498046875\n",
      "iteration=2 loss=2697.99658203125\n",
      "iteration=3 loss=1894.3621826171875\n",
      "iteration=4 loss=1441.399169921875\n",
      "iteration=5 loss=1229.6041259765625\n",
      "iteration=6 loss=1185.0130615234375\n",
      "iteration=7 loss=1240.6678466796875\n",
      "iteration=8 loss=1321.4434814453125\n",
      "iteration=9 loss=1361.527587890625\n",
      "iteration=9 loss=1361.527587890625\n",
      "Accuracy: 0.064\n",
      "F1: 0.507\n",
      "Recall: 0.440\n",
      "Precision: 0.600\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.081\n",
      "F1: 0.571\n",
      "Recall: 0.557\n",
      "Precision: 0.587\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.007\n",
      "Precision: 0.006\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.539\n",
      "Recall: 0.593\n",
      "Precision: 0.494\n",
      "Accuracy: 0.002\n",
      "F1: 0.011\n",
      "Recall: 0.011\n",
      "Precision: 0.010\n",
      "\n",
      "Accuracy: 0.091\n",
      "F1: 0.618\n",
      "Recall: 0.626\n",
      "Precision: 0.611\n",
      "Accuracy: 0.006\n",
      "F1: 0.027\n",
      "Recall: 0.040\n",
      "Precision: 0.020\n",
      "\n",
      "Accuracy: 0.085\n",
      "F1: 0.618\n",
      "Recall: 0.586\n",
      "Precision: 0.653\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.085\n",
      "F1: 0.557\n",
      "Recall: 0.590\n",
      "Precision: 0.528\n",
      "Accuracy: 0.006\n",
      "F1: 0.052\n",
      "Recall: 0.040\n",
      "Precision: 0.074\n",
      "\n",
      "Accuracy: 0.071\n",
      "F1: 0.497\n",
      "Recall: 0.487\n",
      "Precision: 0.508\n",
      "Accuracy: 0.013\n",
      "F1: 0.075\n",
      "Recall: 0.088\n",
      "Precision: 0.065\n",
      "\n",
      "Accuracy: 0.118\n",
      "F1: 0.553\n",
      "Recall: 0.817\n",
      "Precision: 0.418\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.568\n",
      "Recall: 0.465\n",
      "Precision: 0.730\n",
      "Accuracy: 0.025\n",
      "F1: 0.134\n",
      "Recall: 0.176\n",
      "Precision: 0.108\n",
      "\n",
      "Accuracy: 0.119\n",
      "F1: 0.677\n",
      "Recall: 0.824\n",
      "Precision: 0.574\n",
      "Accuracy: 0.001\n",
      "F1: 0.003\n",
      "Recall: 0.004\n",
      "Precision: 0.002\n",
      "\n",
      "25 0.0001\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=2 loss=2704.11767578125\n",
      "iteration=4 loss=1668.4140625\n",
      "iteration=6 loss=1452.6092529296875\n",
      "iteration=8 loss=1544.5687255859375\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=12 loss=1379.801025390625\n",
      "iteration=14 loss=1255.89013671875\n",
      "iteration=16 loss=1188.1248779296875\n",
      "iteration=18 loss=1153.87939453125\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=22 loss=1090.118896484375\n",
      "iteration=24 loss=1053.35400390625\n",
      "iteration=24 loss=1053.35400390625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=2 loss=3383.08642578125\n",
      "iteration=4 loss=1735.2056884765625\n",
      "iteration=6 loss=1304.972900390625\n",
      "iteration=8 loss=1402.6170654296875\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=12 loss=1234.1431884765625\n",
      "iteration=14 loss=1048.302734375\n",
      "iteration=16 loss=953.2472534179688\n",
      "iteration=18 loss=909.799072265625\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=22 loss=842.2056884765625\n",
      "iteration=24 loss=804.5577392578125\n",
      "iteration=24 loss=804.5577392578125\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=2 loss=2479.249267578125\n",
      "iteration=4 loss=1548.63671875\n",
      "iteration=6 loss=1433.0277099609375\n",
      "iteration=8 loss=1533.5693359375\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=12 loss=1331.304443359375\n",
      "iteration=14 loss=1199.44287109375\n",
      "iteration=16 loss=1133.0274658203125\n",
      "iteration=18 loss=1099.1651611328125\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=22 loss=1028.359375\n",
      "iteration=24 loss=988.0443725585938\n",
      "iteration=24 loss=988.0443725585938\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=2 loss=2039.705078125\n",
      "iteration=4 loss=1492.5377197265625\n",
      "iteration=6 loss=1377.48876953125\n",
      "iteration=8 loss=1402.904052734375\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=12 loss=1265.650634765625\n",
      "iteration=14 loss=1166.058837890625\n",
      "iteration=16 loss=1100.9317626953125\n",
      "iteration=18 loss=1058.5113525390625\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=22 loss=984.1653442382812\n",
      "iteration=24 loss=946.8028564453125\n",
      "iteration=24 loss=946.8028564453125\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=2 loss=3048.15185546875\n",
      "iteration=4 loss=1655.8072509765625\n",
      "iteration=6 loss=1469.5201416015625\n",
      "iteration=8 loss=1695.7647705078125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=12 loss=1288.0020751953125\n",
      "iteration=14 loss=1147.189208984375\n",
      "iteration=16 loss=1107.02978515625\n",
      "iteration=18 loss=1075.464111328125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=22 loss=969.2083740234375\n",
      "iteration=24 loss=919.67822265625\n",
      "iteration=24 loss=919.67822265625\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=2 loss=3065.883544921875\n",
      "iteration=4 loss=1671.1636962890625\n",
      "iteration=6 loss=1323.041015625\n",
      "iteration=8 loss=1431.5316162109375\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=12 loss=1292.7508544921875\n",
      "iteration=14 loss=1112.3350830078125\n",
      "iteration=16 loss=1019.9299926757812\n",
      "iteration=18 loss=978.8306884765625\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=22 loss=911.1427612304688\n",
      "iteration=24 loss=870.6551513671875\n",
      "iteration=24 loss=870.6551513671875\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=2 loss=3370.725341796875\n",
      "iteration=4 loss=1672.1434326171875\n",
      "iteration=6 loss=1538.2391357421875\n",
      "iteration=8 loss=1895.58984375\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=12 loss=1455.5382080078125\n",
      "iteration=14 loss=1275.712158203125\n",
      "iteration=16 loss=1226.0902099609375\n",
      "iteration=18 loss=1206.03125\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=22 loss=1129.743896484375\n",
      "iteration=24 loss=1080.873779296875\n",
      "iteration=24 loss=1080.873779296875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=2 loss=3109.437255859375\n",
      "iteration=4 loss=1516.2352294921875\n",
      "iteration=6 loss=1676.838134765625\n",
      "iteration=8 loss=2043.836669921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=10 loss=1738.464111328125\n",
      "iteration=12 loss=1359.32080078125\n",
      "iteration=14 loss=1210.613525390625\n",
      "iteration=16 loss=1171.8330078125\n",
      "iteration=18 loss=1145.58642578125\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=22 loss=1053.886474609375\n",
      "iteration=24 loss=1002.697509765625\n",
      "iteration=24 loss=1002.697509765625\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=2 loss=2892.278564453125\n",
      "iteration=4 loss=1502.74365234375\n",
      "iteration=6 loss=1722.3021240234375\n",
      "iteration=8 loss=1837.415283203125\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=12 loss=1302.0452880859375\n",
      "iteration=14 loss=1155.1011962890625\n",
      "iteration=16 loss=1100.134521484375\n",
      "iteration=18 loss=1070.5472412109375\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=22 loss=996.985595703125\n",
      "iteration=24 loss=955.2576904296875\n",
      "iteration=24 loss=955.2576904296875\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=2 loss=2697.99658203125\n",
      "iteration=4 loss=1441.399169921875\n",
      "iteration=6 loss=1185.0130615234375\n",
      "iteration=8 loss=1321.4434814453125\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=12 loss=1151.1697998046875\n",
      "iteration=14 loss=992.3060302734375\n",
      "iteration=16 loss=916.6572875976562\n",
      "iteration=18 loss=881.681884765625\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=22 loss=821.257080078125\n",
      "iteration=24 loss=788.5912475585938\n",
      "iteration=24 loss=788.5912475585938\n",
      "Accuracy: 0.081\n",
      "F1: 0.566\n",
      "Recall: 0.557\n",
      "Precision: 0.576\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.004\n",
      "Precision: 0.007\n",
      "\n",
      "Accuracy: 0.076\n",
      "F1: 0.561\n",
      "Recall: 0.527\n",
      "Precision: 0.600\n",
      "Accuracy: 0.011\n",
      "F1: 0.063\n",
      "Recall: 0.073\n",
      "Precision: 0.055\n",
      "\n",
      "Accuracy: 0.074\n",
      "F1: 0.578\n",
      "Recall: 0.509\n",
      "Precision: 0.668\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.015\n",
      "Precision: 0.019\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.539\n",
      "Recall: 0.505\n",
      "Precision: 0.577\n",
      "Accuracy: 0.009\n",
      "F1: 0.041\n",
      "Recall: 0.062\n",
      "Precision: 0.031\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.460\n",
      "Recall: 0.352\n",
      "Precision: 0.667\n",
      "Accuracy: 0.002\n",
      "F1: 0.012\n",
      "Recall: 0.011\n",
      "Precision: 0.014\n",
      "\n",
      "Accuracy: 0.028\n",
      "F1: 0.257\n",
      "Recall: 0.190\n",
      "Precision: 0.397\n",
      "Accuracy: 0.062\n",
      "F1: 0.427\n",
      "Recall: 0.429\n",
      "Precision: 0.425\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.203\n",
      "Recall: 0.136\n",
      "Precision: 0.402\n",
      "Accuracy: 0.036\n",
      "F1: 0.240\n",
      "Recall: 0.249\n",
      "Precision: 0.231\n",
      "\n",
      "Accuracy: 0.075\n",
      "F1: 0.418\n",
      "Recall: 0.516\n",
      "Precision: 0.352\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.167\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.462\n",
      "Recall: 0.630\n",
      "Precision: 0.365\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.699\n",
      "Recall: 0.740\n",
      "Precision: 0.662\n",
      "Accuracy: 0.004\n",
      "F1: 0.020\n",
      "Recall: 0.029\n",
      "Precision: 0.015\n",
      "\n",
      "25 1e-05\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=2 loss=2704.11767578125\n",
      "iteration=4 loss=1668.4140625\n",
      "iteration=6 loss=1452.6092529296875\n",
      "iteration=8 loss=1544.5687255859375\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=12 loss=1379.801025390625\n",
      "iteration=14 loss=1255.89013671875\n",
      "iteration=16 loss=1188.1248779296875\n",
      "iteration=18 loss=1153.87939453125\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=22 loss=1090.118896484375\n",
      "iteration=24 loss=1053.35400390625\n",
      "iteration=24 loss=1053.35400390625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=2 loss=3383.08642578125\n",
      "iteration=4 loss=1735.2056884765625\n",
      "iteration=6 loss=1304.972900390625\n",
      "iteration=8 loss=1402.6170654296875\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=12 loss=1234.1431884765625\n",
      "iteration=14 loss=1048.302734375\n",
      "iteration=16 loss=953.2472534179688\n",
      "iteration=18 loss=909.799072265625\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=22 loss=842.2056884765625\n",
      "iteration=24 loss=804.5577392578125\n",
      "iteration=24 loss=804.5577392578125\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=2 loss=2479.249267578125\n",
      "iteration=4 loss=1548.63671875\n",
      "iteration=6 loss=1433.0277099609375\n",
      "iteration=8 loss=1533.5693359375\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=12 loss=1331.304443359375\n",
      "iteration=14 loss=1199.44287109375\n",
      "iteration=16 loss=1133.0274658203125\n",
      "iteration=18 loss=1099.1651611328125\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=22 loss=1028.359375\n",
      "iteration=24 loss=988.0443725585938\n",
      "iteration=24 loss=988.0443725585938\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=2 loss=2039.705078125\n",
      "iteration=4 loss=1492.5377197265625\n",
      "iteration=6 loss=1377.48876953125\n",
      "iteration=8 loss=1402.904052734375\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=12 loss=1265.650634765625\n",
      "iteration=14 loss=1166.058837890625\n",
      "iteration=16 loss=1100.9317626953125\n",
      "iteration=18 loss=1058.5113525390625\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=22 loss=984.1653442382812\n",
      "iteration=24 loss=946.8028564453125\n",
      "iteration=24 loss=946.8028564453125\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=2 loss=3048.15185546875\n",
      "iteration=4 loss=1655.8072509765625\n",
      "iteration=6 loss=1469.5201416015625\n",
      "iteration=8 loss=1695.7647705078125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=12 loss=1288.0020751953125\n",
      "iteration=14 loss=1147.189208984375\n",
      "iteration=16 loss=1107.02978515625\n",
      "iteration=18 loss=1075.464111328125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=22 loss=969.2083740234375\n",
      "iteration=24 loss=919.67822265625\n",
      "iteration=24 loss=919.67822265625\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=2 loss=3065.883544921875\n",
      "iteration=4 loss=1671.1636962890625\n",
      "iteration=6 loss=1323.041015625\n",
      "iteration=8 loss=1431.5316162109375\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=12 loss=1292.7508544921875\n",
      "iteration=14 loss=1112.3350830078125\n",
      "iteration=16 loss=1019.9299926757812\n",
      "iteration=18 loss=978.8306884765625\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=22 loss=911.1427612304688\n",
      "iteration=24 loss=870.6551513671875\n",
      "iteration=24 loss=870.6551513671875\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=2 loss=3370.725341796875\n",
      "iteration=4 loss=1672.1434326171875\n",
      "iteration=6 loss=1538.2391357421875\n",
      "iteration=8 loss=1895.58984375\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=12 loss=1455.5382080078125\n",
      "iteration=14 loss=1275.712158203125\n",
      "iteration=16 loss=1226.0902099609375\n",
      "iteration=18 loss=1206.03125\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=22 loss=1129.743896484375\n",
      "iteration=24 loss=1080.873779296875\n",
      "iteration=24 loss=1080.873779296875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=2 loss=3109.437255859375\n",
      "iteration=4 loss=1516.2352294921875\n",
      "iteration=6 loss=1676.838134765625\n",
      "iteration=8 loss=2043.836669921875\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=12 loss=1359.32080078125\n",
      "iteration=14 loss=1210.613525390625\n",
      "iteration=16 loss=1171.8330078125\n",
      "iteration=18 loss=1145.58642578125\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=22 loss=1053.886474609375\n",
      "iteration=24 loss=1002.697509765625\n",
      "iteration=24 loss=1002.697509765625\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=2 loss=2892.278564453125\n",
      "iteration=4 loss=1502.74365234375\n",
      "iteration=6 loss=1722.3021240234375\n",
      "iteration=8 loss=1837.415283203125\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=12 loss=1302.0452880859375\n",
      "iteration=14 loss=1155.1011962890625\n",
      "iteration=16 loss=1100.134521484375\n",
      "iteration=18 loss=1070.5472412109375\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=22 loss=996.985595703125\n",
      "iteration=24 loss=955.2576904296875\n",
      "iteration=24 loss=955.2576904296875\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=2 loss=2697.99658203125\n",
      "iteration=4 loss=1441.399169921875\n",
      "iteration=6 loss=1185.0130615234375\n",
      "iteration=8 loss=1321.4434814453125\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=12 loss=1151.1697998046875\n",
      "iteration=14 loss=992.3060302734375\n",
      "iteration=16 loss=916.6572875976562\n",
      "iteration=18 loss=881.681884765625\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=22 loss=821.257080078125\n",
      "iteration=24 loss=788.5912475585938\n",
      "iteration=24 loss=788.5912475585938\n",
      "Accuracy: 0.081\n",
      "F1: 0.566\n",
      "Recall: 0.557\n",
      "Precision: 0.576\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.004\n",
      "Precision: 0.007\n",
      "\n",
      "Accuracy: 0.076\n",
      "F1: 0.561\n",
      "Recall: 0.527\n",
      "Precision: 0.600\n",
      "Accuracy: 0.011\n",
      "F1: 0.063\n",
      "Recall: 0.073\n",
      "Precision: 0.055\n",
      "\n",
      "Accuracy: 0.074\n",
      "F1: 0.578\n",
      "Recall: 0.509\n",
      "Precision: 0.668\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.015\n",
      "Precision: 0.019\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.539\n",
      "Recall: 0.505\n",
      "Precision: 0.577\n",
      "Accuracy: 0.009\n",
      "F1: 0.041\n",
      "Recall: 0.062\n",
      "Precision: 0.031\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.460\n",
      "Recall: 0.352\n",
      "Precision: 0.667\n",
      "Accuracy: 0.002\n",
      "F1: 0.012\n",
      "Recall: 0.011\n",
      "Precision: 0.014\n",
      "\n",
      "Accuracy: 0.028\n",
      "F1: 0.257\n",
      "Recall: 0.190\n",
      "Precision: 0.397\n",
      "Accuracy: 0.062\n",
      "F1: 0.427\n",
      "Recall: 0.429\n",
      "Precision: 0.425\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.203\n",
      "Recall: 0.136\n",
      "Precision: 0.402\n",
      "Accuracy: 0.036\n",
      "F1: 0.240\n",
      "Recall: 0.249\n",
      "Precision: 0.231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.075\n",
      "F1: 0.418\n",
      "Recall: 0.516\n",
      "Precision: 0.352\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.167\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.462\n",
      "Recall: 0.630\n",
      "Precision: 0.365\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.699\n",
      "Recall: 0.740\n",
      "Precision: 0.662\n",
      "Accuracy: 0.004\n",
      "F1: 0.020\n",
      "Recall: 0.029\n",
      "Precision: 0.015\n",
      "\n",
      "25 1e-06\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=2 loss=2704.11767578125\n",
      "iteration=4 loss=1668.4140625\n",
      "iteration=6 loss=1452.6092529296875\n",
      "iteration=8 loss=1544.5687255859375\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=12 loss=1379.801025390625\n",
      "iteration=14 loss=1255.89013671875\n",
      "iteration=16 loss=1188.1248779296875\n",
      "iteration=18 loss=1153.87939453125\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=22 loss=1090.118896484375\n",
      "iteration=24 loss=1053.35400390625\n",
      "iteration=24 loss=1053.35400390625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=2 loss=3383.08642578125\n",
      "iteration=4 loss=1735.2056884765625\n",
      "iteration=6 loss=1304.972900390625\n",
      "iteration=8 loss=1402.6170654296875\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=12 loss=1234.1431884765625\n",
      "iteration=14 loss=1048.302734375\n",
      "iteration=16 loss=953.2472534179688\n",
      "iteration=18 loss=909.799072265625\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=22 loss=842.2056884765625\n",
      "iteration=24 loss=804.5577392578125\n",
      "iteration=24 loss=804.5577392578125\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=2 loss=2479.249267578125\n",
      "iteration=4 loss=1548.63671875\n",
      "iteration=6 loss=1433.0277099609375\n",
      "iteration=8 loss=1533.5693359375\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=12 loss=1331.304443359375\n",
      "iteration=14 loss=1199.44287109375\n",
      "iteration=16 loss=1133.0274658203125\n",
      "iteration=18 loss=1099.1651611328125\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=22 loss=1028.359375\n",
      "iteration=24 loss=988.0443725585938\n",
      "iteration=24 loss=988.0443725585938\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=2 loss=2039.705078125\n",
      "iteration=4 loss=1492.5377197265625\n",
      "iteration=6 loss=1377.48876953125\n",
      "iteration=8 loss=1402.904052734375\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=12 loss=1265.650634765625\n",
      "iteration=14 loss=1166.058837890625\n",
      "iteration=16 loss=1100.9317626953125\n",
      "iteration=18 loss=1058.5113525390625\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=22 loss=984.1653442382812\n",
      "iteration=24 loss=946.8028564453125\n",
      "iteration=24 loss=946.8028564453125\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=2 loss=3048.15185546875\n",
      "iteration=4 loss=1655.8072509765625\n",
      "iteration=6 loss=1469.5201416015625\n",
      "iteration=8 loss=1695.7647705078125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=12 loss=1288.0020751953125\n",
      "iteration=14 loss=1147.189208984375\n",
      "iteration=16 loss=1107.02978515625\n",
      "iteration=18 loss=1075.464111328125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=22 loss=969.2083740234375\n",
      "iteration=24 loss=919.67822265625\n",
      "iteration=24 loss=919.67822265625\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=2 loss=3065.883544921875\n",
      "iteration=4 loss=1671.1636962890625\n",
      "iteration=6 loss=1323.041015625\n",
      "iteration=8 loss=1431.5316162109375\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=12 loss=1292.7508544921875\n",
      "iteration=14 loss=1112.3350830078125\n",
      "iteration=16 loss=1019.9299926757812\n",
      "iteration=18 loss=978.8306884765625\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=22 loss=911.1427612304688\n",
      "iteration=24 loss=870.6551513671875\n",
      "iteration=24 loss=870.6551513671875\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=2 loss=3370.725341796875\n",
      "iteration=4 loss=1672.1434326171875\n",
      "iteration=6 loss=1538.2391357421875\n",
      "iteration=8 loss=1895.58984375\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=12 loss=1455.5382080078125\n",
      "iteration=14 loss=1275.712158203125\n",
      "iteration=16 loss=1226.0902099609375\n",
      "iteration=18 loss=1206.03125\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=22 loss=1129.743896484375\n",
      "iteration=24 loss=1080.873779296875\n",
      "iteration=24 loss=1080.873779296875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=2 loss=3109.437255859375\n",
      "iteration=4 loss=1516.2352294921875\n",
      "iteration=6 loss=1676.838134765625\n",
      "iteration=8 loss=2043.836669921875\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=12 loss=1359.32080078125\n",
      "iteration=14 loss=1210.613525390625\n",
      "iteration=16 loss=1171.8330078125\n",
      "iteration=18 loss=1145.58642578125\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=22 loss=1053.886474609375\n",
      "iteration=24 loss=1002.697509765625\n",
      "iteration=24 loss=1002.697509765625\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=2 loss=2892.278564453125\n",
      "iteration=4 loss=1502.74365234375\n",
      "iteration=6 loss=1722.3021240234375\n",
      "iteration=8 loss=1837.415283203125\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=12 loss=1302.0452880859375\n",
      "iteration=14 loss=1155.1011962890625\n",
      "iteration=16 loss=1100.134521484375\n",
      "iteration=18 loss=1070.5472412109375\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=22 loss=996.985595703125\n",
      "iteration=24 loss=955.2576904296875\n",
      "iteration=24 loss=955.2576904296875\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=2 loss=2697.99658203125\n",
      "iteration=4 loss=1441.399169921875\n",
      "iteration=6 loss=1185.0130615234375\n",
      "iteration=8 loss=1321.4434814453125\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=12 loss=1151.1697998046875\n",
      "iteration=14 loss=992.3060302734375\n",
      "iteration=16 loss=916.6572875976562\n",
      "iteration=18 loss=881.681884765625\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=22 loss=821.257080078125\n",
      "iteration=24 loss=788.5912475585938\n",
      "iteration=24 loss=788.5912475585938\n",
      "Accuracy: 0.081\n",
      "F1: 0.566\n",
      "Recall: 0.557\n",
      "Precision: 0.576\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.004\n",
      "Precision: 0.007\n",
      "\n",
      "Accuracy: 0.076\n",
      "F1: 0.561\n",
      "Recall: 0.527\n",
      "Precision: 0.600\n",
      "Accuracy: 0.011\n",
      "F1: 0.063\n",
      "Recall: 0.073\n",
      "Precision: 0.055\n",
      "\n",
      "Accuracy: 0.074\n",
      "F1: 0.578\n",
      "Recall: 0.509\n",
      "Precision: 0.668\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.015\n",
      "Precision: 0.019\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.539\n",
      "Recall: 0.505\n",
      "Precision: 0.577\n",
      "Accuracy: 0.009\n",
      "F1: 0.041\n",
      "Recall: 0.062\n",
      "Precision: 0.031\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.460\n",
      "Recall: 0.352\n",
      "Precision: 0.667\n",
      "Accuracy: 0.002\n",
      "F1: 0.012\n",
      "Recall: 0.011\n",
      "Precision: 0.014\n",
      "\n",
      "Accuracy: 0.028\n",
      "F1: 0.257\n",
      "Recall: 0.190\n",
      "Precision: 0.397\n",
      "Accuracy: 0.062\n",
      "F1: 0.427\n",
      "Recall: 0.429\n",
      "Precision: 0.425\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.203\n",
      "Recall: 0.136\n",
      "Precision: 0.402\n",
      "Accuracy: 0.036\n",
      "F1: 0.240\n",
      "Recall: 0.249\n",
      "Precision: 0.231\n",
      "\n",
      "Accuracy: 0.075\n",
      "F1: 0.418\n",
      "Recall: 0.516\n",
      "Precision: 0.352\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.167\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.462\n",
      "Recall: 0.630\n",
      "Precision: 0.365\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.699\n",
      "Recall: 0.740\n",
      "Precision: 0.662\n",
      "Accuracy: 0.004\n",
      "F1: 0.020\n",
      "Recall: 0.029\n",
      "Precision: 0.015\n",
      "\n",
      "25 1e-07\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=2 loss=2704.11767578125\n",
      "iteration=4 loss=1668.4140625\n",
      "iteration=6 loss=1452.6092529296875\n",
      "iteration=8 loss=1544.5687255859375\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=12 loss=1379.801025390625\n",
      "iteration=14 loss=1255.89013671875\n",
      "iteration=16 loss=1188.1248779296875\n",
      "iteration=18 loss=1153.87939453125\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=22 loss=1090.118896484375\n",
      "iteration=24 loss=1053.35400390625\n",
      "iteration=24 loss=1053.35400390625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=2 loss=3383.08642578125\n",
      "iteration=4 loss=1735.2056884765625\n",
      "iteration=6 loss=1304.972900390625\n",
      "iteration=8 loss=1402.6170654296875\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=12 loss=1234.1431884765625\n",
      "iteration=14 loss=1048.302734375\n",
      "iteration=16 loss=953.2472534179688\n",
      "iteration=18 loss=909.799072265625\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=22 loss=842.2056884765625\n",
      "iteration=24 loss=804.5577392578125\n",
      "iteration=24 loss=804.5577392578125\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=2 loss=2479.249267578125\n",
      "iteration=4 loss=1548.63671875\n",
      "iteration=6 loss=1433.0277099609375\n",
      "iteration=8 loss=1533.5693359375\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=12 loss=1331.304443359375\n",
      "iteration=14 loss=1199.44287109375\n",
      "iteration=16 loss=1133.0274658203125\n",
      "iteration=18 loss=1099.1651611328125\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=22 loss=1028.359375\n",
      "iteration=24 loss=988.0443725585938\n",
      "iteration=24 loss=988.0443725585938\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=2 loss=2039.705078125\n",
      "iteration=4 loss=1492.5377197265625\n",
      "iteration=6 loss=1377.48876953125\n",
      "iteration=8 loss=1402.904052734375\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=12 loss=1265.650634765625\n",
      "iteration=14 loss=1166.058837890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=16 loss=1100.9317626953125\n",
      "iteration=18 loss=1058.5113525390625\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=22 loss=984.1653442382812\n",
      "iteration=24 loss=946.8028564453125\n",
      "iteration=24 loss=946.8028564453125\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=2 loss=3048.15185546875\n",
      "iteration=4 loss=1655.8072509765625\n",
      "iteration=6 loss=1469.5201416015625\n",
      "iteration=8 loss=1695.7647705078125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=12 loss=1288.0020751953125\n",
      "iteration=14 loss=1147.189208984375\n",
      "iteration=16 loss=1107.02978515625\n",
      "iteration=18 loss=1075.464111328125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=22 loss=969.2083740234375\n",
      "iteration=24 loss=919.67822265625\n",
      "iteration=24 loss=919.67822265625\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=2 loss=3065.883544921875\n",
      "iteration=4 loss=1671.1636962890625\n",
      "iteration=6 loss=1323.041015625\n",
      "iteration=8 loss=1431.5316162109375\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=12 loss=1292.7508544921875\n",
      "iteration=14 loss=1112.3350830078125\n",
      "iteration=16 loss=1019.9299926757812\n",
      "iteration=18 loss=978.8306884765625\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=22 loss=911.1427612304688\n",
      "iteration=24 loss=870.6551513671875\n",
      "iteration=24 loss=870.6551513671875\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=2 loss=3370.725341796875\n",
      "iteration=4 loss=1672.1434326171875\n",
      "iteration=6 loss=1538.2391357421875\n",
      "iteration=8 loss=1895.58984375\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=12 loss=1455.5382080078125\n",
      "iteration=14 loss=1275.712158203125\n",
      "iteration=16 loss=1226.0902099609375\n",
      "iteration=18 loss=1206.03125\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=22 loss=1129.743896484375\n",
      "iteration=24 loss=1080.873779296875\n",
      "iteration=24 loss=1080.873779296875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=2 loss=3109.437255859375\n",
      "iteration=4 loss=1516.2352294921875\n",
      "iteration=6 loss=1676.838134765625\n",
      "iteration=8 loss=2043.836669921875\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=12 loss=1359.32080078125\n",
      "iteration=14 loss=1210.613525390625\n",
      "iteration=16 loss=1171.8330078125\n",
      "iteration=18 loss=1145.58642578125\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=22 loss=1053.886474609375\n",
      "iteration=24 loss=1002.697509765625\n",
      "iteration=24 loss=1002.697509765625\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=2 loss=2892.278564453125\n",
      "iteration=4 loss=1502.74365234375\n",
      "iteration=6 loss=1722.3021240234375\n",
      "iteration=8 loss=1837.415283203125\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=12 loss=1302.0452880859375\n",
      "iteration=14 loss=1155.1011962890625\n",
      "iteration=16 loss=1100.134521484375\n",
      "iteration=18 loss=1070.5472412109375\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=22 loss=996.985595703125\n",
      "iteration=24 loss=955.2576904296875\n",
      "iteration=24 loss=955.2576904296875\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=2 loss=2697.99658203125\n",
      "iteration=4 loss=1441.399169921875\n",
      "iteration=6 loss=1185.0130615234375\n",
      "iteration=8 loss=1321.4434814453125\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=12 loss=1151.1697998046875\n",
      "iteration=14 loss=992.3060302734375\n",
      "iteration=16 loss=916.6572875976562\n",
      "iteration=18 loss=881.681884765625\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=22 loss=821.257080078125\n",
      "iteration=24 loss=788.5912475585938\n",
      "iteration=24 loss=788.5912475585938\n",
      "Accuracy: 0.081\n",
      "F1: 0.566\n",
      "Recall: 0.557\n",
      "Precision: 0.576\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.004\n",
      "Precision: 0.007\n",
      "\n",
      "Accuracy: 0.076\n",
      "F1: 0.561\n",
      "Recall: 0.527\n",
      "Precision: 0.600\n",
      "Accuracy: 0.011\n",
      "F1: 0.063\n",
      "Recall: 0.073\n",
      "Precision: 0.055\n",
      "\n",
      "Accuracy: 0.074\n",
      "F1: 0.578\n",
      "Recall: 0.509\n",
      "Precision: 0.668\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.015\n",
      "Precision: 0.019\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.539\n",
      "Recall: 0.505\n",
      "Precision: 0.577\n",
      "Accuracy: 0.009\n",
      "F1: 0.041\n",
      "Recall: 0.062\n",
      "Precision: 0.031\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.460\n",
      "Recall: 0.352\n",
      "Precision: 0.667\n",
      "Accuracy: 0.002\n",
      "F1: 0.012\n",
      "Recall: 0.011\n",
      "Precision: 0.014\n",
      "\n",
      "Accuracy: 0.028\n",
      "F1: 0.257\n",
      "Recall: 0.190\n",
      "Precision: 0.397\n",
      "Accuracy: 0.062\n",
      "F1: 0.427\n",
      "Recall: 0.429\n",
      "Precision: 0.425\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.203\n",
      "Recall: 0.136\n",
      "Precision: 0.402\n",
      "Accuracy: 0.036\n",
      "F1: 0.240\n",
      "Recall: 0.249\n",
      "Precision: 0.231\n",
      "\n",
      "Accuracy: 0.075\n",
      "F1: 0.418\n",
      "Recall: 0.516\n",
      "Precision: 0.352\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.167\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.462\n",
      "Recall: 0.630\n",
      "Precision: 0.365\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.699\n",
      "Recall: 0.740\n",
      "Precision: 0.662\n",
      "Accuracy: 0.004\n",
      "F1: 0.020\n",
      "Recall: 0.029\n",
      "Precision: 0.015\n",
      "\n",
      "50 0.0001\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=5 loss=1488.377685546875\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=15 loss=1215.630859375\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=25 loss=1035.736083984375\n",
      "iteration=30 loss=962.5591430664062\n",
      "iteration=35 loss=901.8601684570312\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=45 loss=799.792724609375\n",
      "iteration=49 loss=765.1610107421875\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=5 loss=1416.473388671875\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=15 loss=990.912109375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=25 loss=786.279541015625\n",
      "iteration=30 loss=711.4971923828125\n",
      "iteration=35 loss=656.4550170898438\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=45 loss=573.3760375976562\n",
      "iteration=49 loss=547.507568359375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=5 loss=1428.4415283203125\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=15 loss=1159.5621337890625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=25 loss=969.0916748046875\n",
      "iteration=30 loss=891.175537109375\n",
      "iteration=35 loss=826.788330078125\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=45 loss=721.4885864257812\n",
      "iteration=49 loss=686.7156982421875\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=5 loss=1399.025634765625\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=15 loss=1129.4256591796875\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=25 loss=929.0772705078125\n",
      "iteration=30 loss=853.9574584960938\n",
      "iteration=35 loss=794.245361328125\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=45 loss=700.9581909179688\n",
      "iteration=49 loss=670.6062622070312\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=5 loss=1457.34814453125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=15 loss=1121.798828125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=25 loss=899.3330078125\n",
      "iteration=30 loss=821.4959716796875\n",
      "iteration=35 loss=755.3636474609375\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=45 loss=658.1285400390625\n",
      "iteration=49 loss=627.7077026367188\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=5 loss=1407.3260498046875\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=15 loss=1056.3516845703125\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=25 loss=850.7615356445312\n",
      "iteration=30 loss=770.3568725585938\n",
      "iteration=35 loss=712.444580078125\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=45 loss=624.2373046875\n",
      "iteration=49 loss=596.5762939453125\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=5 loss=1478.7574462890625\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=15 loss=1242.2484130859375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=25 loss=1057.9931640625\n",
      "iteration=30 loss=971.2628784179688\n",
      "iteration=35 loss=903.5933837890625\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=45 loss=790.6905517578125\n",
      "iteration=49 loss=753.569580078125\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=5 loss=1462.040283203125\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=15 loss=1185.701904296875\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=25 loss=979.509033203125\n",
      "iteration=30 loss=891.273193359375\n",
      "iteration=35 loss=823.0724487304688\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=45 loss=719.5332641601562\n",
      "iteration=49 loss=687.3419799804688\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=5 loss=1518.0216064453125\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=15 loss=1121.11083984375\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=25 loss=935.4912719726562\n",
      "iteration=30 loss=853.908203125\n",
      "iteration=35 loss=790.4431762695312\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=45 loss=686.6931762695312\n",
      "iteration=49 loss=652.919677734375\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=5 loss=1229.6041259765625\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=15 loss=946.2476196289062\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=25 loss=773.2669677734375\n",
      "iteration=30 loss=712.002197265625\n",
      "iteration=35 loss=666.6353149414062\n",
      "iteration=40 loss=630.2410888671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=45 loss=599.4479370117188\n",
      "iteration=49 loss=577.7220458984375\n",
      "Accuracy: 0.085\n",
      "F1: 0.546\n",
      "Recall: 0.590\n",
      "Precision: 0.508\n",
      "Accuracy: 0.004\n",
      "F1: 0.029\n",
      "Recall: 0.026\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.584\n",
      "Recall: 0.568\n",
      "Precision: 0.601\n",
      "Accuracy: 0.013\n",
      "F1: 0.070\n",
      "Recall: 0.088\n",
      "Precision: 0.058\n",
      "\n",
      "Accuracy: 0.080\n",
      "F1: 0.616\n",
      "Recall: 0.549\n",
      "Precision: 0.701\n",
      "Accuracy: 0.001\n",
      "F1: 0.008\n",
      "Recall: 0.007\n",
      "Precision: 0.010\n",
      "\n",
      "Accuracy: 0.077\n",
      "F1: 0.562\n",
      "Recall: 0.535\n",
      "Precision: 0.591\n",
      "Accuracy: 0.013\n",
      "F1: 0.060\n",
      "Recall: 0.088\n",
      "Precision: 0.045\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.487\n",
      "Recall: 0.392\n",
      "Precision: 0.645\n",
      "Accuracy: 0.004\n",
      "F1: 0.025\n",
      "Recall: 0.026\n",
      "Precision: 0.024\n",
      "\n",
      "Accuracy: 0.034\n",
      "F1: 0.304\n",
      "Recall: 0.234\n",
      "Precision: 0.432\n",
      "Accuracy: 0.065\n",
      "F1: 0.351\n",
      "Recall: 0.447\n",
      "Precision: 0.289\n",
      "\n",
      "Accuracy: 0.023\n",
      "F1: 0.244\n",
      "Recall: 0.158\n",
      "Precision: 0.537\n",
      "Accuracy: 0.020\n",
      "F1: 0.161\n",
      "Recall: 0.139\n",
      "Precision: 0.192\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.534\n",
      "Recall: 0.608\n",
      "Precision: 0.476\n",
      "Accuracy: 0.010\n",
      "F1: 0.112\n",
      "Recall: 0.070\n",
      "Precision: 0.288\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.053\n",
      "Accuracy: 0.084\n",
      "F1: 0.440\n",
      "Recall: 0.582\n",
      "Precision: 0.353\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.700\n",
      "Recall: 0.736\n",
      "Precision: 0.668\n",
      "Accuracy: 0.007\n",
      "F1: 0.033\n",
      "Recall: 0.048\n",
      "Precision: 0.025\n",
      "\n",
      "50 1e-05\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=5 loss=1488.377685546875\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=15 loss=1215.630859375\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=25 loss=1035.736083984375\n",
      "iteration=30 loss=962.5591430664062\n",
      "iteration=35 loss=901.8601684570312\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=45 loss=799.792724609375\n",
      "iteration=49 loss=765.1610107421875\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=5 loss=1416.473388671875\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=15 loss=990.912109375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=25 loss=786.279541015625\n",
      "iteration=30 loss=711.4971923828125\n",
      "iteration=35 loss=656.4550170898438\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=45 loss=573.3760375976562\n",
      "iteration=49 loss=547.507568359375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=5 loss=1428.4415283203125\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=15 loss=1159.5621337890625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=25 loss=969.0916748046875\n",
      "iteration=30 loss=891.175537109375\n",
      "iteration=35 loss=826.788330078125\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=45 loss=721.4885864257812\n",
      "iteration=49 loss=686.7156982421875\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=5 loss=1399.025634765625\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=15 loss=1129.4256591796875\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=25 loss=929.0772705078125\n",
      "iteration=30 loss=853.9574584960938\n",
      "iteration=35 loss=794.245361328125\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=45 loss=700.9581909179688\n",
      "iteration=49 loss=670.6062622070312\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=5 loss=1457.34814453125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=15 loss=1121.798828125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=25 loss=899.3330078125\n",
      "iteration=30 loss=821.4959716796875\n",
      "iteration=35 loss=755.3636474609375\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=45 loss=658.1285400390625\n",
      "iteration=49 loss=627.7077026367188\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=5 loss=1407.3260498046875\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=15 loss=1056.3516845703125\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=25 loss=850.7615356445312\n",
      "iteration=30 loss=770.3568725585938\n",
      "iteration=35 loss=712.444580078125\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=45 loss=624.2373046875\n",
      "iteration=49 loss=596.5762939453125\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=5 loss=1478.7574462890625\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=15 loss=1242.2484130859375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=25 loss=1057.9931640625\n",
      "iteration=30 loss=971.2628784179688\n",
      "iteration=35 loss=903.5933837890625\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=45 loss=790.6905517578125\n",
      "iteration=49 loss=753.569580078125\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=5 loss=1462.040283203125\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=15 loss=1185.701904296875\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=25 loss=979.509033203125\n",
      "iteration=30 loss=891.273193359375\n",
      "iteration=35 loss=823.0724487304688\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=45 loss=719.5332641601562\n",
      "iteration=49 loss=687.3419799804688\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=5 loss=1518.0216064453125\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=15 loss=1121.11083984375\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=25 loss=935.4912719726562\n",
      "iteration=30 loss=853.908203125\n",
      "iteration=35 loss=790.4431762695312\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=45 loss=686.6931762695312\n",
      "iteration=49 loss=652.919677734375\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=5 loss=1229.6041259765625\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=15 loss=946.2476196289062\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=25 loss=773.2669677734375\n",
      "iteration=30 loss=712.002197265625\n",
      "iteration=35 loss=666.6353149414062\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=45 loss=599.4479370117188\n",
      "iteration=49 loss=577.7220458984375\n",
      "Accuracy: 0.085\n",
      "F1: 0.546\n",
      "Recall: 0.590\n",
      "Precision: 0.508\n",
      "Accuracy: 0.004\n",
      "F1: 0.029\n",
      "Recall: 0.026\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.584\n",
      "Recall: 0.568\n",
      "Precision: 0.601\n",
      "Accuracy: 0.013\n",
      "F1: 0.070\n",
      "Recall: 0.088\n",
      "Precision: 0.058\n",
      "\n",
      "Accuracy: 0.080\n",
      "F1: 0.616\n",
      "Recall: 0.549\n",
      "Precision: 0.701\n",
      "Accuracy: 0.001\n",
      "F1: 0.008\n",
      "Recall: 0.007\n",
      "Precision: 0.010\n",
      "\n",
      "Accuracy: 0.077\n",
      "F1: 0.562\n",
      "Recall: 0.535\n",
      "Precision: 0.591\n",
      "Accuracy: 0.013\n",
      "F1: 0.060\n",
      "Recall: 0.088\n",
      "Precision: 0.045\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.487\n",
      "Recall: 0.392\n",
      "Precision: 0.645\n",
      "Accuracy: 0.004\n",
      "F1: 0.025\n",
      "Recall: 0.026\n",
      "Precision: 0.024\n",
      "\n",
      "Accuracy: 0.034\n",
      "F1: 0.304\n",
      "Recall: 0.234\n",
      "Precision: 0.432\n",
      "Accuracy: 0.065\n",
      "F1: 0.351\n",
      "Recall: 0.447\n",
      "Precision: 0.289\n",
      "\n",
      "Accuracy: 0.023\n",
      "F1: 0.244\n",
      "Recall: 0.158\n",
      "Precision: 0.537\n",
      "Accuracy: 0.020\n",
      "F1: 0.161\n",
      "Recall: 0.139\n",
      "Precision: 0.192\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.534\n",
      "Recall: 0.608\n",
      "Precision: 0.476\n",
      "Accuracy: 0.010\n",
      "F1: 0.112\n",
      "Recall: 0.070\n",
      "Precision: 0.288\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.053\n",
      "Accuracy: 0.084\n",
      "F1: 0.440\n",
      "Recall: 0.582\n",
      "Precision: 0.353\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.700\n",
      "Recall: 0.736\n",
      "Precision: 0.668\n",
      "Accuracy: 0.007\n",
      "F1: 0.033\n",
      "Recall: 0.048\n",
      "Precision: 0.025\n",
      "\n",
      "50 1e-06\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=5 loss=1488.377685546875\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=15 loss=1215.630859375\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=25 loss=1035.736083984375\n",
      "iteration=30 loss=962.5591430664062\n",
      "iteration=35 loss=901.8601684570312\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=45 loss=799.792724609375\n",
      "iteration=49 loss=765.1610107421875\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=5 loss=1416.473388671875\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=15 loss=990.912109375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=25 loss=786.279541015625\n",
      "iteration=30 loss=711.4971923828125\n",
      "iteration=35 loss=656.4550170898438\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=45 loss=573.3760375976562\n",
      "iteration=49 loss=547.507568359375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=5 loss=1428.4415283203125\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=15 loss=1159.5621337890625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=25 loss=969.0916748046875\n",
      "iteration=30 loss=891.175537109375\n",
      "iteration=35 loss=826.788330078125\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=45 loss=721.4885864257812\n",
      "iteration=49 loss=686.7156982421875\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=5 loss=1399.025634765625\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=15 loss=1129.4256591796875\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=25 loss=929.0772705078125\n",
      "iteration=30 loss=853.9574584960938\n",
      "iteration=35 loss=794.245361328125\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=45 loss=700.9581909179688\n",
      "iteration=49 loss=670.6062622070312\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=5 loss=1457.34814453125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=15 loss=1121.798828125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=25 loss=899.3330078125\n",
      "iteration=30 loss=821.4959716796875\n",
      "iteration=35 loss=755.3636474609375\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=45 loss=658.1285400390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=49 loss=627.7077026367188\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=5 loss=1407.3260498046875\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=15 loss=1056.3516845703125\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=25 loss=850.7615356445312\n",
      "iteration=30 loss=770.3568725585938\n",
      "iteration=35 loss=712.444580078125\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=45 loss=624.2373046875\n",
      "iteration=49 loss=596.5762939453125\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=5 loss=1478.7574462890625\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=15 loss=1242.2484130859375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=25 loss=1057.9931640625\n",
      "iteration=30 loss=971.2628784179688\n",
      "iteration=35 loss=903.5933837890625\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=45 loss=790.6905517578125\n",
      "iteration=49 loss=753.569580078125\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=5 loss=1462.040283203125\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=15 loss=1185.701904296875\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=25 loss=979.509033203125\n",
      "iteration=30 loss=891.273193359375\n",
      "iteration=35 loss=823.0724487304688\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=45 loss=719.5332641601562\n",
      "iteration=49 loss=687.3419799804688\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=5 loss=1518.0216064453125\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=15 loss=1121.11083984375\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=25 loss=935.4912719726562\n",
      "iteration=30 loss=853.908203125\n",
      "iteration=35 loss=790.4431762695312\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=45 loss=686.6931762695312\n",
      "iteration=49 loss=652.919677734375\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=5 loss=1229.6041259765625\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=15 loss=946.2476196289062\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=25 loss=773.2669677734375\n",
      "iteration=30 loss=712.002197265625\n",
      "iteration=35 loss=666.6353149414062\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=45 loss=599.4479370117188\n",
      "iteration=49 loss=577.7220458984375\n",
      "Accuracy: 0.085\n",
      "F1: 0.546\n",
      "Recall: 0.590\n",
      "Precision: 0.508\n",
      "Accuracy: 0.004\n",
      "F1: 0.029\n",
      "Recall: 0.026\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.584\n",
      "Recall: 0.568\n",
      "Precision: 0.601\n",
      "Accuracy: 0.013\n",
      "F1: 0.070\n",
      "Recall: 0.088\n",
      "Precision: 0.058\n",
      "\n",
      "Accuracy: 0.080\n",
      "F1: 0.616\n",
      "Recall: 0.549\n",
      "Precision: 0.701\n",
      "Accuracy: 0.001\n",
      "F1: 0.008\n",
      "Recall: 0.007\n",
      "Precision: 0.010\n",
      "\n",
      "Accuracy: 0.077\n",
      "F1: 0.562\n",
      "Recall: 0.535\n",
      "Precision: 0.591\n",
      "Accuracy: 0.013\n",
      "F1: 0.060\n",
      "Recall: 0.088\n",
      "Precision: 0.045\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.487\n",
      "Recall: 0.392\n",
      "Precision: 0.645\n",
      "Accuracy: 0.004\n",
      "F1: 0.025\n",
      "Recall: 0.026\n",
      "Precision: 0.024\n",
      "\n",
      "Accuracy: 0.034\n",
      "F1: 0.304\n",
      "Recall: 0.234\n",
      "Precision: 0.432\n",
      "Accuracy: 0.065\n",
      "F1: 0.351\n",
      "Recall: 0.447\n",
      "Precision: 0.289\n",
      "\n",
      "Accuracy: 0.023\n",
      "F1: 0.244\n",
      "Recall: 0.158\n",
      "Precision: 0.537\n",
      "Accuracy: 0.020\n",
      "F1: 0.161\n",
      "Recall: 0.139\n",
      "Precision: 0.192\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.534\n",
      "Recall: 0.608\n",
      "Precision: 0.476\n",
      "Accuracy: 0.010\n",
      "F1: 0.112\n",
      "Recall: 0.070\n",
      "Precision: 0.288\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.053\n",
      "Accuracy: 0.084\n",
      "F1: 0.440\n",
      "Recall: 0.582\n",
      "Precision: 0.353\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.700\n",
      "Recall: 0.736\n",
      "Precision: 0.668\n",
      "Accuracy: 0.007\n",
      "F1: 0.033\n",
      "Recall: 0.048\n",
      "Precision: 0.025\n",
      "\n",
      "50 1e-07\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=5 loss=1488.377685546875\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=15 loss=1215.630859375\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=25 loss=1035.736083984375\n",
      "iteration=30 loss=962.5591430664062\n",
      "iteration=35 loss=901.8601684570312\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=45 loss=799.792724609375\n",
      "iteration=49 loss=765.1610107421875\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=5 loss=1416.473388671875\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=15 loss=990.912109375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=25 loss=786.279541015625\n",
      "iteration=30 loss=711.4971923828125\n",
      "iteration=35 loss=656.4550170898438\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=45 loss=573.3760375976562\n",
      "iteration=49 loss=547.507568359375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=5 loss=1428.4415283203125\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=15 loss=1159.5621337890625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=25 loss=969.0916748046875\n",
      "iteration=30 loss=891.175537109375\n",
      "iteration=35 loss=826.788330078125\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=45 loss=721.4885864257812\n",
      "iteration=49 loss=686.7156982421875\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=5 loss=1399.025634765625\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=15 loss=1129.4256591796875\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=25 loss=929.0772705078125\n",
      "iteration=30 loss=853.9574584960938\n",
      "iteration=35 loss=794.245361328125\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=45 loss=700.9581909179688\n",
      "iteration=49 loss=670.6062622070312\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=5 loss=1457.34814453125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=15 loss=1121.798828125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=25 loss=899.3330078125\n",
      "iteration=30 loss=821.4959716796875\n",
      "iteration=35 loss=755.3636474609375\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=45 loss=658.1285400390625\n",
      "iteration=49 loss=627.7077026367188\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=5 loss=1407.3260498046875\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=15 loss=1056.3516845703125\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=25 loss=850.7615356445312\n",
      "iteration=30 loss=770.3568725585938\n",
      "iteration=35 loss=712.444580078125\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=45 loss=624.2373046875\n",
      "iteration=49 loss=596.5762939453125\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=5 loss=1478.7574462890625\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=15 loss=1242.2484130859375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=25 loss=1057.9931640625\n",
      "iteration=30 loss=971.2628784179688\n",
      "iteration=35 loss=903.5933837890625\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=45 loss=790.6905517578125\n",
      "iteration=49 loss=753.569580078125\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=5 loss=1462.040283203125\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=15 loss=1185.701904296875\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=25 loss=979.509033203125\n",
      "iteration=30 loss=891.273193359375\n",
      "iteration=35 loss=823.0724487304688\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=45 loss=719.5332641601562\n",
      "iteration=49 loss=687.3419799804688\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=5 loss=1518.0216064453125\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=15 loss=1121.11083984375\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=25 loss=935.4912719726562\n",
      "iteration=30 loss=853.908203125\n",
      "iteration=35 loss=790.4431762695312\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=45 loss=686.6931762695312\n",
      "iteration=49 loss=652.919677734375\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=5 loss=1229.6041259765625\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=15 loss=946.2476196289062\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=25 loss=773.2669677734375\n",
      "iteration=30 loss=712.002197265625\n",
      "iteration=35 loss=666.6353149414062\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=45 loss=599.4479370117188\n",
      "iteration=49 loss=577.7220458984375\n",
      "Accuracy: 0.085\n",
      "F1: 0.546\n",
      "Recall: 0.590\n",
      "Precision: 0.508\n",
      "Accuracy: 0.004\n",
      "F1: 0.029\n",
      "Recall: 0.026\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.584\n",
      "Recall: 0.568\n",
      "Precision: 0.601\n",
      "Accuracy: 0.013\n",
      "F1: 0.070\n",
      "Recall: 0.088\n",
      "Precision: 0.058\n",
      "\n",
      "Accuracy: 0.080\n",
      "F1: 0.616\n",
      "Recall: 0.549\n",
      "Precision: 0.701\n",
      "Accuracy: 0.001\n",
      "F1: 0.008\n",
      "Recall: 0.007\n",
      "Precision: 0.010\n",
      "\n",
      "Accuracy: 0.077\n",
      "F1: 0.562\n",
      "Recall: 0.535\n",
      "Precision: 0.591\n",
      "Accuracy: 0.013\n",
      "F1: 0.060\n",
      "Recall: 0.088\n",
      "Precision: 0.045\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.487\n",
      "Recall: 0.392\n",
      "Precision: 0.645\n",
      "Accuracy: 0.004\n",
      "F1: 0.025\n",
      "Recall: 0.026\n",
      "Precision: 0.024\n",
      "\n",
      "Accuracy: 0.034\n",
      "F1: 0.304\n",
      "Recall: 0.234\n",
      "Precision: 0.432\n",
      "Accuracy: 0.065\n",
      "F1: 0.351\n",
      "Recall: 0.447\n",
      "Precision: 0.289\n",
      "\n",
      "Accuracy: 0.023\n",
      "F1: 0.244\n",
      "Recall: 0.158\n",
      "Precision: 0.537\n",
      "Accuracy: 0.020\n",
      "F1: 0.161\n",
      "Recall: 0.139\n",
      "Precision: 0.192\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.534\n",
      "Recall: 0.608\n",
      "Precision: 0.476\n",
      "Accuracy: 0.010\n",
      "F1: 0.112\n",
      "Recall: 0.070\n",
      "Precision: 0.288\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.053\n",
      "Accuracy: 0.084\n",
      "F1: 0.440\n",
      "Recall: 0.582\n",
      "Precision: 0.353\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.700\n",
      "Recall: 0.736\n",
      "Precision: 0.668\n",
      "Accuracy: 0.007\n",
      "F1: 0.033\n",
      "Recall: 0.048\n",
      "Precision: 0.025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100 0.0001\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=30 loss=962.5591430664062\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=50 loss=756.958740234375\n",
      "iteration=60 loss=683.4595947265625\n",
      "iteration=70 loss=622.5860595703125\n",
      "iteration=80 loss=571.4036865234375\n",
      "iteration=90 loss=527.97314453125\n",
      "iteration=99 loss=494.3837890625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=30 loss=711.4971923828125\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=50 loss=541.5350341796875\n",
      "iteration=60 loss=490.2699890136719\n",
      "iteration=70 loss=450.1976623535156\n",
      "iteration=80 loss=417.74920654296875\n",
      "iteration=90 loss=391.764892578125\n",
      "iteration=99 loss=372.412109375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=30 loss=891.175537109375\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=50 loss=678.5484008789062\n",
      "iteration=60 loss=606.5533447265625\n",
      "iteration=70 loss=548.3773803710938\n",
      "iteration=80 loss=500.4305725097656\n",
      "iteration=90 loss=460.5531311035156\n",
      "iteration=99 loss=430.33734130859375\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=30 loss=853.9574584960938\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=50 loss=663.4794921875\n",
      "iteration=60 loss=600.36669921875\n",
      "iteration=70 loss=548.9490966796875\n",
      "iteration=80 loss=506.4349365234375\n",
      "iteration=90 loss=471.0526123046875\n",
      "iteration=99 loss=444.2095642089844\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=30 loss=821.4959716796875\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=50 loss=620.6760864257812\n",
      "iteration=60 loss=559.2557373046875\n",
      "iteration=70 loss=510.1672668457031\n",
      "iteration=80 loss=470.14959716796875\n",
      "iteration=90 loss=437.2822570800781\n",
      "iteration=99 loss=412.6304016113281\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=30 loss=770.3568725585938\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=50 loss=590.1494750976562\n",
      "iteration=60 loss=534.2129516601562\n",
      "iteration=70 loss=489.568359375\n",
      "iteration=80 loss=453.0172119140625\n",
      "iteration=90 loss=422.712646484375\n",
      "iteration=99 loss=399.71630859375\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=30 loss=971.2628784179688\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=50 loss=744.858154296875\n",
      "iteration=60 loss=668.5250244140625\n",
      "iteration=70 loss=607.5267944335938\n",
      "iteration=80 loss=557.8056030273438\n",
      "iteration=90 loss=516.6969604492188\n",
      "iteration=99 loss=485.53076171875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=30 loss=891.273193359375\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=50 loss=679.93505859375\n",
      "iteration=60 loss=616.667236328125\n",
      "iteration=70 loss=567.5816040039062\n",
      "iteration=80 loss=527.8756103515625\n",
      "iteration=90 loss=494.90191650390625\n",
      "iteration=99 loss=469.6249084472656\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=30 loss=853.908203125\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=50 loss=645.0450439453125\n",
      "iteration=60 loss=576.9147338867188\n",
      "iteration=70 loss=523.98193359375\n",
      "iteration=80 loss=481.43017578125\n",
      "iteration=90 loss=446.5225830078125\n",
      "iteration=99 loss=420.3897399902344\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=30 loss=712.002197265625\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=50 loss=572.6261596679688\n",
      "iteration=60 loss=527.1788940429688\n",
      "iteration=70 loss=489.6593017578125\n",
      "iteration=80 loss=458.2295227050781\n",
      "iteration=90 loss=431.66748046875\n",
      "iteration=99 loss=411.1684875488281\n",
      "Accuracy: 0.078\n",
      "F1: 0.495\n",
      "Recall: 0.538\n",
      "Precision: 0.458\n",
      "Accuracy: 0.014\n",
      "F1: 0.091\n",
      "Recall: 0.095\n",
      "Precision: 0.088\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.536\n",
      "Recall: 0.505\n",
      "Precision: 0.570\n",
      "Accuracy: 0.030\n",
      "F1: 0.151\n",
      "Recall: 0.205\n",
      "Precision: 0.119\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.640\n",
      "Recall: 0.568\n",
      "Precision: 0.735\n",
      "Accuracy: 0.008\n",
      "F1: 0.048\n",
      "Recall: 0.059\n",
      "Precision: 0.041\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.533\n",
      "Recall: 0.505\n",
      "Precision: 0.563\n",
      "Accuracy: 0.016\n",
      "F1: 0.073\n",
      "Recall: 0.110\n",
      "Precision: 0.055\n",
      "\n",
      "Accuracy: 0.064\n",
      "F1: 0.512\n",
      "Recall: 0.440\n",
      "Precision: 0.612\n",
      "Accuracy: 0.007\n",
      "F1: 0.041\n",
      "Recall: 0.051\n",
      "Precision: 0.034\n",
      "\n",
      "Accuracy: 0.032\n",
      "F1: 0.282\n",
      "Recall: 0.220\n",
      "Precision: 0.392\n",
      "Accuracy: 0.069\n",
      "F1: 0.348\n",
      "Recall: 0.476\n",
      "Precision: 0.274\n",
      "\n",
      "Accuracy: 0.022\n",
      "F1: 0.240\n",
      "Recall: 0.150\n",
      "Precision: 0.603\n",
      "Accuracy: 0.039\n",
      "F1: 0.253\n",
      "Recall: 0.267\n",
      "Precision: 0.241\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.578\n",
      "Recall: 0.564\n",
      "Precision: 0.592\n",
      "Accuracy: 0.024\n",
      "F1: 0.191\n",
      "Recall: 0.165\n",
      "Precision: 0.227\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.024\n",
      "Recall: 0.015\n",
      "Precision: 0.066\n",
      "Accuracy: 0.064\n",
      "F1: 0.334\n",
      "Recall: 0.440\n",
      "Precision: 0.270\n",
      "\n",
      "Accuracy: 0.104\n",
      "F1: 0.699\n",
      "Recall: 0.718\n",
      "Precision: 0.681\n",
      "Accuracy: 0.012\n",
      "F1: 0.055\n",
      "Recall: 0.081\n",
      "Precision: 0.041\n",
      "\n",
      "100 1e-05\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=30 loss=962.5591430664062\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=50 loss=756.958740234375\n",
      "iteration=60 loss=683.4595947265625\n",
      "iteration=70 loss=622.5860595703125\n",
      "iteration=80 loss=571.4036865234375\n",
      "iteration=90 loss=527.97314453125\n",
      "iteration=99 loss=494.3837890625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=30 loss=711.4971923828125\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=50 loss=541.5350341796875\n",
      "iteration=60 loss=490.2699890136719\n",
      "iteration=70 loss=450.1976623535156\n",
      "iteration=80 loss=417.74920654296875\n",
      "iteration=90 loss=391.764892578125\n",
      "iteration=99 loss=372.412109375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=30 loss=891.175537109375\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=50 loss=678.5484008789062\n",
      "iteration=60 loss=606.5533447265625\n",
      "iteration=70 loss=548.3773803710938\n",
      "iteration=80 loss=500.4305725097656\n",
      "iteration=90 loss=460.5531311035156\n",
      "iteration=99 loss=430.33734130859375\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=30 loss=853.9574584960938\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=50 loss=663.4794921875\n",
      "iteration=60 loss=600.36669921875\n",
      "iteration=70 loss=548.9490966796875\n",
      "iteration=80 loss=506.4349365234375\n",
      "iteration=90 loss=471.0526123046875\n",
      "iteration=99 loss=444.2095642089844\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=30 loss=821.4959716796875\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=50 loss=620.6760864257812\n",
      "iteration=60 loss=559.2557373046875\n",
      "iteration=70 loss=510.1672668457031\n",
      "iteration=80 loss=470.14959716796875\n",
      "iteration=90 loss=437.2822570800781\n",
      "iteration=99 loss=412.6304016113281\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=30 loss=770.3568725585938\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=50 loss=590.1494750976562\n",
      "iteration=60 loss=534.2129516601562\n",
      "iteration=70 loss=489.568359375\n",
      "iteration=80 loss=453.0172119140625\n",
      "iteration=90 loss=422.712646484375\n",
      "iteration=99 loss=399.71630859375\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=30 loss=971.2628784179688\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=50 loss=744.858154296875\n",
      "iteration=60 loss=668.5250244140625\n",
      "iteration=70 loss=607.5267944335938\n",
      "iteration=80 loss=557.8056030273438\n",
      "iteration=90 loss=516.6969604492188\n",
      "iteration=99 loss=485.53076171875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=30 loss=891.273193359375\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=50 loss=679.93505859375\n",
      "iteration=60 loss=616.667236328125\n",
      "iteration=70 loss=567.5816040039062\n",
      "iteration=80 loss=527.8756103515625\n",
      "iteration=90 loss=494.90191650390625\n",
      "iteration=99 loss=469.6249084472656\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=9917.166015625\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=30 loss=853.908203125\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=50 loss=645.0450439453125\n",
      "iteration=60 loss=576.9147338867188\n",
      "iteration=70 loss=523.98193359375\n",
      "iteration=80 loss=481.43017578125\n",
      "iteration=90 loss=446.5225830078125\n",
      "iteration=99 loss=420.3897399902344\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=30 loss=712.002197265625\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=50 loss=572.6261596679688\n",
      "iteration=60 loss=527.1788940429688\n",
      "iteration=70 loss=489.6593017578125\n",
      "iteration=80 loss=458.2295227050781\n",
      "iteration=90 loss=431.66748046875\n",
      "iteration=99 loss=411.1684875488281\n",
      "Accuracy: 0.078\n",
      "F1: 0.495\n",
      "Recall: 0.538\n",
      "Precision: 0.458\n",
      "Accuracy: 0.014\n",
      "F1: 0.091\n",
      "Recall: 0.095\n",
      "Precision: 0.088\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.536\n",
      "Recall: 0.505\n",
      "Precision: 0.570\n",
      "Accuracy: 0.030\n",
      "F1: 0.151\n",
      "Recall: 0.205\n",
      "Precision: 0.119\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.640\n",
      "Recall: 0.568\n",
      "Precision: 0.735\n",
      "Accuracy: 0.008\n",
      "F1: 0.048\n",
      "Recall: 0.059\n",
      "Precision: 0.041\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.533\n",
      "Recall: 0.505\n",
      "Precision: 0.563\n",
      "Accuracy: 0.016\n",
      "F1: 0.073\n",
      "Recall: 0.110\n",
      "Precision: 0.055\n",
      "\n",
      "Accuracy: 0.064\n",
      "F1: 0.512\n",
      "Recall: 0.440\n",
      "Precision: 0.612\n",
      "Accuracy: 0.007\n",
      "F1: 0.041\n",
      "Recall: 0.051\n",
      "Precision: 0.034\n",
      "\n",
      "Accuracy: 0.032\n",
      "F1: 0.282\n",
      "Recall: 0.220\n",
      "Precision: 0.392\n",
      "Accuracy: 0.069\n",
      "F1: 0.348\n",
      "Recall: 0.476\n",
      "Precision: 0.274\n",
      "\n",
      "Accuracy: 0.022\n",
      "F1: 0.240\n",
      "Recall: 0.150\n",
      "Precision: 0.603\n",
      "Accuracy: 0.039\n",
      "F1: 0.253\n",
      "Recall: 0.267\n",
      "Precision: 0.241\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.578\n",
      "Recall: 0.564\n",
      "Precision: 0.592\n",
      "Accuracy: 0.024\n",
      "F1: 0.191\n",
      "Recall: 0.165\n",
      "Precision: 0.227\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.024\n",
      "Recall: 0.015\n",
      "Precision: 0.066\n",
      "Accuracy: 0.064\n",
      "F1: 0.334\n",
      "Recall: 0.440\n",
      "Precision: 0.270\n",
      "\n",
      "Accuracy: 0.104\n",
      "F1: 0.699\n",
      "Recall: 0.718\n",
      "Precision: 0.681\n",
      "Accuracy: 0.012\n",
      "F1: 0.055\n",
      "Recall: 0.081\n",
      "Precision: 0.041\n",
      "\n",
      "100 1e-06\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=30 loss=962.5591430664062\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=50 loss=756.958740234375\n",
      "iteration=60 loss=683.4595947265625\n",
      "iteration=70 loss=622.5860595703125\n",
      "iteration=80 loss=571.4036865234375\n",
      "iteration=90 loss=527.97314453125\n",
      "iteration=99 loss=494.3837890625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=30 loss=711.4971923828125\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=50 loss=541.5350341796875\n",
      "iteration=60 loss=490.2699890136719\n",
      "iteration=70 loss=450.1976623535156\n",
      "iteration=80 loss=417.74920654296875\n",
      "iteration=90 loss=391.764892578125\n",
      "iteration=99 loss=372.412109375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=30 loss=891.175537109375\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=50 loss=678.5484008789062\n",
      "iteration=60 loss=606.5533447265625\n",
      "iteration=70 loss=548.3773803710938\n",
      "iteration=80 loss=500.4305725097656\n",
      "iteration=90 loss=460.5531311035156\n",
      "iteration=99 loss=430.33734130859375\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=30 loss=853.9574584960938\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=50 loss=663.4794921875\n",
      "iteration=60 loss=600.36669921875\n",
      "iteration=70 loss=548.9490966796875\n",
      "iteration=80 loss=506.4349365234375\n",
      "iteration=90 loss=471.0526123046875\n",
      "iteration=99 loss=444.2095642089844\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=30 loss=821.4959716796875\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=50 loss=620.6760864257812\n",
      "iteration=60 loss=559.2557373046875\n",
      "iteration=70 loss=510.1672668457031\n",
      "iteration=80 loss=470.14959716796875\n",
      "iteration=90 loss=437.2822570800781\n",
      "iteration=99 loss=412.6304016113281\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=30 loss=770.3568725585938\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=50 loss=590.1494750976562\n",
      "iteration=60 loss=534.2129516601562\n",
      "iteration=70 loss=489.568359375\n",
      "iteration=80 loss=453.0172119140625\n",
      "iteration=90 loss=422.712646484375\n",
      "iteration=99 loss=399.71630859375\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=30 loss=971.2628784179688\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=50 loss=744.858154296875\n",
      "iteration=60 loss=668.5250244140625\n",
      "iteration=70 loss=607.5267944335938\n",
      "iteration=80 loss=557.8056030273438\n",
      "iteration=90 loss=516.6969604492188\n",
      "iteration=99 loss=485.53076171875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=30 loss=891.273193359375\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=50 loss=679.93505859375\n",
      "iteration=60 loss=616.667236328125\n",
      "iteration=70 loss=567.5816040039062\n",
      "iteration=80 loss=527.8756103515625\n",
      "iteration=90 loss=494.90191650390625\n",
      "iteration=99 loss=469.6249084472656\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=30 loss=853.908203125\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=50 loss=645.0450439453125\n",
      "iteration=60 loss=576.9147338867188\n",
      "iteration=70 loss=523.98193359375\n",
      "iteration=80 loss=481.43017578125\n",
      "iteration=90 loss=446.5225830078125\n",
      "iteration=99 loss=420.3897399902344\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=30 loss=712.002197265625\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=50 loss=572.6261596679688\n",
      "iteration=60 loss=527.1788940429688\n",
      "iteration=70 loss=489.6593017578125\n",
      "iteration=80 loss=458.2295227050781\n",
      "iteration=90 loss=431.66748046875\n",
      "iteration=99 loss=411.1684875488281\n",
      "Accuracy: 0.078\n",
      "F1: 0.495\n",
      "Recall: 0.538\n",
      "Precision: 0.458\n",
      "Accuracy: 0.014\n",
      "F1: 0.091\n",
      "Recall: 0.095\n",
      "Precision: 0.088\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.536\n",
      "Recall: 0.505\n",
      "Precision: 0.570\n",
      "Accuracy: 0.030\n",
      "F1: 0.151\n",
      "Recall: 0.205\n",
      "Precision: 0.119\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.640\n",
      "Recall: 0.568\n",
      "Precision: 0.735\n",
      "Accuracy: 0.008\n",
      "F1: 0.048\n",
      "Recall: 0.059\n",
      "Precision: 0.041\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.533\n",
      "Recall: 0.505\n",
      "Precision: 0.563\n",
      "Accuracy: 0.016\n",
      "F1: 0.073\n",
      "Recall: 0.110\n",
      "Precision: 0.055\n",
      "\n",
      "Accuracy: 0.064\n",
      "F1: 0.512\n",
      "Recall: 0.440\n",
      "Precision: 0.612\n",
      "Accuracy: 0.007\n",
      "F1: 0.041\n",
      "Recall: 0.051\n",
      "Precision: 0.034\n",
      "\n",
      "Accuracy: 0.032\n",
      "F1: 0.282\n",
      "Recall: 0.220\n",
      "Precision: 0.392\n",
      "Accuracy: 0.069\n",
      "F1: 0.348\n",
      "Recall: 0.476\n",
      "Precision: 0.274\n",
      "\n",
      "Accuracy: 0.022\n",
      "F1: 0.240\n",
      "Recall: 0.150\n",
      "Precision: 0.603\n",
      "Accuracy: 0.039\n",
      "F1: 0.253\n",
      "Recall: 0.267\n",
      "Precision: 0.241\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.578\n",
      "Recall: 0.564\n",
      "Precision: 0.592\n",
      "Accuracy: 0.024\n",
      "F1: 0.191\n",
      "Recall: 0.165\n",
      "Precision: 0.227\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.024\n",
      "Recall: 0.015\n",
      "Precision: 0.066\n",
      "Accuracy: 0.064\n",
      "F1: 0.334\n",
      "Recall: 0.440\n",
      "Precision: 0.270\n",
      "\n",
      "Accuracy: 0.104\n",
      "F1: 0.699\n",
      "Recall: 0.718\n",
      "Precision: 0.681\n",
      "Accuracy: 0.012\n",
      "F1: 0.055\n",
      "Recall: 0.081\n",
      "Precision: 0.041\n",
      "\n",
      "100 1e-07\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=10 loss=1508.8524169921875\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=30 loss=962.5591430664062\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=50 loss=756.958740234375\n",
      "iteration=60 loss=683.4595947265625\n",
      "iteration=70 loss=622.5860595703125\n",
      "iteration=80 loss=571.4036865234375\n",
      "iteration=90 loss=527.97314453125\n",
      "iteration=99 loss=494.3837890625\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=10 loss=1421.4359130859375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=30 loss=711.4971923828125\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=50 loss=541.5350341796875\n",
      "iteration=60 loss=490.2699890136719\n",
      "iteration=70 loss=450.1976623535156\n",
      "iteration=80 loss=417.74920654296875\n",
      "iteration=90 loss=391.764892578125\n",
      "iteration=99 loss=372.412109375\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=10 loss=1491.0260009765625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=30 loss=891.175537109375\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=50 loss=678.5484008789062\n",
      "iteration=60 loss=606.5533447265625\n",
      "iteration=70 loss=548.3773803710938\n",
      "iteration=80 loss=500.4305725097656\n",
      "iteration=90 loss=460.5531311035156\n",
      "iteration=99 loss=430.33734130859375\n",
      "3\n",
      "iteration=0 loss=2843.359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=10 loss=1368.9178466796875\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=30 loss=853.9574584960938\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=50 loss=663.4794921875\n",
      "iteration=60 loss=600.36669921875\n",
      "iteration=70 loss=548.9490966796875\n",
      "iteration=80 loss=506.4349365234375\n",
      "iteration=90 loss=471.0526123046875\n",
      "iteration=99 loss=444.2095642089844\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=10 loss=1577.599853515625\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=30 loss=821.4959716796875\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=50 loss=620.6760864257812\n",
      "iteration=60 loss=559.2557373046875\n",
      "iteration=70 loss=510.1672668457031\n",
      "iteration=80 loss=470.14959716796875\n",
      "iteration=90 loss=437.2822570800781\n",
      "iteration=99 loss=412.6304016113281\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=10 loss=1469.4818115234375\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=30 loss=770.3568725585938\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=50 loss=590.1494750976562\n",
      "iteration=60 loss=534.2129516601562\n",
      "iteration=70 loss=489.568359375\n",
      "iteration=80 loss=453.0172119140625\n",
      "iteration=90 loss=422.712646484375\n",
      "iteration=99 loss=399.71630859375\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=10 loss=1798.77099609375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=30 loss=971.2628784179688\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=50 loss=744.858154296875\n",
      "iteration=60 loss=668.5250244140625\n",
      "iteration=70 loss=607.5267944335938\n",
      "iteration=80 loss=557.8056030273438\n",
      "iteration=90 loss=516.6969604492188\n",
      "iteration=99 loss=485.53076171875\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=10 loss=1738.464111328125\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=30 loss=891.273193359375\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=50 loss=679.93505859375\n",
      "iteration=60 loss=616.667236328125\n",
      "iteration=70 loss=567.5816040039062\n",
      "iteration=80 loss=527.8756103515625\n",
      "iteration=90 loss=494.90191650390625\n",
      "iteration=99 loss=469.6249084472656\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=10 loss=1581.4688720703125\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=30 loss=853.908203125\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=50 loss=645.0450439453125\n",
      "iteration=60 loss=576.9147338867188\n",
      "iteration=70 loss=523.98193359375\n",
      "iteration=80 loss=481.43017578125\n",
      "iteration=90 loss=446.5225830078125\n",
      "iteration=99 loss=420.3897399902344\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=10 loss=1333.179443359375\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=30 loss=712.002197265625\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=50 loss=572.6261596679688\n",
      "iteration=60 loss=527.1788940429688\n",
      "iteration=70 loss=489.6593017578125\n",
      "iteration=80 loss=458.2295227050781\n",
      "iteration=90 loss=431.66748046875\n",
      "iteration=99 loss=411.1684875488281\n",
      "Accuracy: 0.078\n",
      "F1: 0.495\n",
      "Recall: 0.538\n",
      "Precision: 0.458\n",
      "Accuracy: 0.014\n",
      "F1: 0.091\n",
      "Recall: 0.095\n",
      "Precision: 0.088\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.536\n",
      "Recall: 0.505\n",
      "Precision: 0.570\n",
      "Accuracy: 0.030\n",
      "F1: 0.151\n",
      "Recall: 0.205\n",
      "Precision: 0.119\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.640\n",
      "Recall: 0.568\n",
      "Precision: 0.735\n",
      "Accuracy: 0.008\n",
      "F1: 0.048\n",
      "Recall: 0.059\n",
      "Precision: 0.041\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.533\n",
      "Recall: 0.505\n",
      "Precision: 0.563\n",
      "Accuracy: 0.016\n",
      "F1: 0.073\n",
      "Recall: 0.110\n",
      "Precision: 0.055\n",
      "\n",
      "Accuracy: 0.064\n",
      "F1: 0.512\n",
      "Recall: 0.440\n",
      "Precision: 0.612\n",
      "Accuracy: 0.007\n",
      "F1: 0.041\n",
      "Recall: 0.051\n",
      "Precision: 0.034\n",
      "\n",
      "Accuracy: 0.032\n",
      "F1: 0.282\n",
      "Recall: 0.220\n",
      "Precision: 0.392\n",
      "Accuracy: 0.069\n",
      "F1: 0.348\n",
      "Recall: 0.476\n",
      "Precision: 0.274\n",
      "\n",
      "Accuracy: 0.022\n",
      "F1: 0.240\n",
      "Recall: 0.150\n",
      "Precision: 0.603\n",
      "Accuracy: 0.039\n",
      "F1: 0.253\n",
      "Recall: 0.267\n",
      "Precision: 0.241\n",
      "\n",
      "Accuracy: 0.082\n",
      "F1: 0.578\n",
      "Recall: 0.564\n",
      "Precision: 0.592\n",
      "Accuracy: 0.024\n",
      "F1: 0.191\n",
      "Recall: 0.165\n",
      "Precision: 0.227\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.024\n",
      "Recall: 0.015\n",
      "Precision: 0.066\n",
      "Accuracy: 0.064\n",
      "F1: 0.334\n",
      "Recall: 0.440\n",
      "Precision: 0.270\n",
      "\n",
      "Accuracy: 0.104\n",
      "F1: 0.699\n",
      "Recall: 0.718\n",
      "Precision: 0.681\n",
      "Accuracy: 0.012\n",
      "F1: 0.055\n",
      "Recall: 0.081\n",
      "Precision: 0.041\n",
      "\n",
      "200 0.0001\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=60 loss=683.4595947265625\n",
      "iteration=80 loss=571.4036865234375\n",
      "iteration=100 loss=490.936767578125\n",
      "iteration=120 loss=432.13812255859375\n",
      "iteration=140 loss=388.86376953125\n",
      "iteration=160 loss=356.7801513671875\n",
      "iteration=180 loss=332.7320861816406\n",
      "iteration=199 loss=315.2413635253906\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=60 loss=490.2699890136719\n",
      "iteration=80 loss=417.74920654296875\n",
      "iteration=100 loss=370.448486328125\n",
      "iteration=120 loss=337.5798034667969\n",
      "iteration=140 loss=314.08551025390625\n",
      "iteration=160 loss=296.92120361328125\n",
      "iteration=180 loss=284.0343017578125\n",
      "iteration=199 loss=274.5367431640625\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=60 loss=606.5533447265625\n",
      "iteration=80 loss=500.4305725097656\n",
      "iteration=100 loss=427.27203369140625\n",
      "iteration=120 loss=376.1902770996094\n",
      "iteration=140 loss=340.33721923828125\n",
      "iteration=160 loss=314.924072265625\n",
      "iteration=180 loss=296.630126953125\n",
      "iteration=199 loss=283.8272705078125\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=60 loss=600.36669921875\n",
      "iteration=80 loss=506.4349365234375\n",
      "iteration=100 loss=441.48260498046875\n",
      "iteration=120 loss=395.79022216796875\n",
      "iteration=140 loss=363.061279296875\n",
      "iteration=160 loss=339.0338439941406\n",
      "iteration=180 loss=320.9342956542969\n",
      "iteration=199 loss=307.9380798339844\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=60 loss=559.2557373046875\n",
      "iteration=80 loss=470.14959716796875\n",
      "iteration=100 loss=410.13983154296875\n",
      "iteration=120 loss=369.2391052246094\n",
      "iteration=140 loss=340.70111083984375\n",
      "iteration=160 loss=320.0384216308594\n",
      "iteration=180 loss=304.5850524902344\n",
      "iteration=199 loss=293.1958923339844\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=60 loss=534.2129516601562\n",
      "iteration=80 loss=453.0172119140625\n",
      "iteration=100 loss=397.37713623046875\n",
      "iteration=120 loss=358.08795166015625\n",
      "iteration=140 loss=329.7733459472656\n",
      "iteration=160 loss=308.8278503417969\n",
      "iteration=180 loss=293.0078430175781\n",
      "iteration=199 loss=281.3431396484375\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=60 loss=668.5250244140625\n",
      "iteration=80 loss=557.8056030273438\n",
      "iteration=100 loss=482.36065673828125\n",
      "iteration=120 loss=429.87432861328125\n",
      "iteration=140 loss=392.03912353515625\n",
      "iteration=160 loss=362.87164306640625\n",
      "iteration=180 loss=339.8700256347656\n",
      "iteration=199 loss=322.2371520996094\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=60 loss=616.667236328125\n",
      "iteration=80 loss=527.8756103515625\n",
      "iteration=100 loss=467.033203125\n",
      "iteration=120 loss=422.45404052734375\n",
      "iteration=140 loss=388.3623046875\n",
      "iteration=160 loss=361.5158996582031\n",
      "iteration=180 loss=339.9199523925781\n",
      "iteration=199 loss=323.1628112792969\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=60 loss=576.9147338867188\n",
      "iteration=80 loss=481.43017578125\n",
      "iteration=100 loss=417.78302001953125\n",
      "iteration=120 loss=374.09326171875\n",
      "iteration=140 loss=342.72491455078125\n",
      "iteration=160 loss=320.03350830078125\n",
      "iteration=180 loss=303.2298583984375\n",
      "iteration=199 loss=290.9288024902344\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=60 loss=527.1788940429688\n",
      "iteration=80 loss=458.2295227050781\n",
      "iteration=100 loss=409.0736083984375\n",
      "iteration=120 loss=373.2250061035156\n",
      "iteration=140 loss=346.37652587890625\n",
      "iteration=160 loss=325.8358154296875\n",
      "iteration=180 loss=309.8284606933594\n",
      "iteration=199 loss=297.70782470703125\n",
      "Accuracy: 0.049\n",
      "F1: 0.335\n",
      "Recall: 0.337\n",
      "Precision: 0.332\n",
      "Accuracy: 0.037\n",
      "F1: 0.192\n",
      "Recall: 0.256\n",
      "Precision: 0.154\n",
      "\n",
      "Accuracy: 0.063\n",
      "F1: 0.488\n",
      "Recall: 0.436\n",
      "Precision: 0.553\n",
      "Accuracy: 0.049\n",
      "F1: 0.216\n",
      "Recall: 0.341\n",
      "Precision: 0.158\n",
      "\n",
      "Accuracy: 0.074\n",
      "F1: 0.570\n",
      "Recall: 0.509\n",
      "Precision: 0.647\n",
      "Accuracy: 0.039\n",
      "F1: 0.173\n",
      "Recall: 0.267\n",
      "Precision: 0.128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.062\n",
      "F1: 0.492\n",
      "Recall: 0.429\n",
      "Precision: 0.576\n",
      "Accuracy: 0.034\n",
      "F1: 0.147\n",
      "Recall: 0.234\n",
      "Precision: 0.107\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.464\n",
      "Recall: 0.396\n",
      "Precision: 0.560\n",
      "Accuracy: 0.025\n",
      "F1: 0.115\n",
      "Recall: 0.176\n",
      "Precision: 0.086\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.156\n",
      "Recall: 0.132\n",
      "Precision: 0.191\n",
      "Accuracy: 0.031\n",
      "F1: 0.252\n",
      "Recall: 0.216\n",
      "Precision: 0.301\n",
      "\n",
      "Accuracy: 0.024\n",
      "F1: 0.253\n",
      "Recall: 0.168\n",
      "Precision: 0.511\n",
      "Accuracy: 0.057\n",
      "F1: 0.295\n",
      "Recall: 0.392\n",
      "Precision: 0.236\n",
      "\n",
      "Accuracy: 0.047\n",
      "F1: 0.385\n",
      "Recall: 0.326\n",
      "Precision: 0.471\n",
      "Accuracy: 0.039\n",
      "F1: 0.228\n",
      "Recall: 0.267\n",
      "Precision: 0.199\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.075\n",
      "Recall: 0.051\n",
      "Precision: 0.137\n",
      "Accuracy: 0.064\n",
      "F1: 0.340\n",
      "Recall: 0.443\n",
      "Precision: 0.276\n",
      "\n",
      "Accuracy: 0.094\n",
      "F1: 0.682\n",
      "Recall: 0.648\n",
      "Precision: 0.720\n",
      "Accuracy: 0.019\n",
      "F1: 0.084\n",
      "Recall: 0.132\n",
      "Precision: 0.062\n",
      "\n",
      "200 1e-05\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=60 loss=683.4595947265625\n",
      "iteration=80 loss=571.4036865234375\n",
      "iteration=100 loss=490.936767578125\n",
      "iteration=120 loss=432.13812255859375\n",
      "iteration=140 loss=388.86376953125\n",
      "iteration=160 loss=356.7801513671875\n",
      "iteration=180 loss=332.7320861816406\n",
      "iteration=199 loss=315.2413635253906\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=60 loss=490.2699890136719\n",
      "iteration=80 loss=417.74920654296875\n",
      "iteration=100 loss=370.448486328125\n",
      "iteration=120 loss=337.5798034667969\n",
      "iteration=140 loss=314.08551025390625\n",
      "iteration=160 loss=296.92120361328125\n",
      "iteration=180 loss=284.0343017578125\n",
      "iteration=199 loss=274.5367431640625\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=60 loss=606.5533447265625\n",
      "iteration=80 loss=500.4305725097656\n",
      "iteration=100 loss=427.27203369140625\n",
      "iteration=120 loss=376.1902770996094\n",
      "iteration=140 loss=340.33721923828125\n",
      "iteration=160 loss=314.924072265625\n",
      "iteration=180 loss=296.630126953125\n",
      "iteration=199 loss=283.8272705078125\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=60 loss=600.36669921875\n",
      "iteration=80 loss=506.4349365234375\n",
      "iteration=100 loss=441.48260498046875\n",
      "iteration=120 loss=395.79022216796875\n",
      "iteration=140 loss=363.061279296875\n",
      "iteration=160 loss=339.0338439941406\n",
      "iteration=180 loss=320.9342956542969\n",
      "iteration=199 loss=307.9380798339844\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=60 loss=559.2557373046875\n",
      "iteration=80 loss=470.14959716796875\n",
      "iteration=100 loss=410.13983154296875\n",
      "iteration=120 loss=369.2391052246094\n",
      "iteration=140 loss=340.70111083984375\n",
      "iteration=160 loss=320.0384216308594\n",
      "iteration=180 loss=304.5850524902344\n",
      "iteration=199 loss=293.1958923339844\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=60 loss=534.2129516601562\n",
      "iteration=80 loss=453.0172119140625\n",
      "iteration=100 loss=397.37713623046875\n",
      "iteration=120 loss=358.08795166015625\n",
      "iteration=140 loss=329.7733459472656\n",
      "iteration=160 loss=308.8278503417969\n",
      "iteration=180 loss=293.0078430175781\n",
      "iteration=199 loss=281.3431396484375\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=60 loss=668.5250244140625\n",
      "iteration=80 loss=557.8056030273438\n",
      "iteration=100 loss=482.36065673828125\n",
      "iteration=120 loss=429.87432861328125\n",
      "iteration=140 loss=392.03912353515625\n",
      "iteration=160 loss=362.87164306640625\n",
      "iteration=180 loss=339.8700256347656\n",
      "iteration=199 loss=322.2371520996094\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=60 loss=616.667236328125\n",
      "iteration=80 loss=527.8756103515625\n",
      "iteration=100 loss=467.033203125\n",
      "iteration=120 loss=422.45404052734375\n",
      "iteration=140 loss=388.3623046875\n",
      "iteration=160 loss=361.5158996582031\n",
      "iteration=180 loss=339.9199523925781\n",
      "iteration=199 loss=323.1628112792969\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=60 loss=576.9147338867188\n",
      "iteration=80 loss=481.43017578125\n",
      "iteration=100 loss=417.78302001953125\n",
      "iteration=120 loss=374.09326171875\n",
      "iteration=140 loss=342.72491455078125\n",
      "iteration=160 loss=320.03350830078125\n",
      "iteration=180 loss=303.2298583984375\n",
      "iteration=199 loss=290.9288024902344\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=60 loss=527.1788940429688\n",
      "iteration=80 loss=458.2295227050781\n",
      "iteration=100 loss=409.0736083984375\n",
      "iteration=120 loss=373.2250061035156\n",
      "iteration=140 loss=346.37652587890625\n",
      "iteration=160 loss=325.8358154296875\n",
      "iteration=180 loss=309.8284606933594\n",
      "iteration=199 loss=297.70782470703125\n",
      "Accuracy: 0.049\n",
      "F1: 0.335\n",
      "Recall: 0.337\n",
      "Precision: 0.332\n",
      "Accuracy: 0.037\n",
      "F1: 0.192\n",
      "Recall: 0.256\n",
      "Precision: 0.154\n",
      "\n",
      "Accuracy: 0.063\n",
      "F1: 0.488\n",
      "Recall: 0.436\n",
      "Precision: 0.553\n",
      "Accuracy: 0.049\n",
      "F1: 0.216\n",
      "Recall: 0.341\n",
      "Precision: 0.158\n",
      "\n",
      "Accuracy: 0.074\n",
      "F1: 0.570\n",
      "Recall: 0.509\n",
      "Precision: 0.647\n",
      "Accuracy: 0.039\n",
      "F1: 0.173\n",
      "Recall: 0.267\n",
      "Precision: 0.128\n",
      "\n",
      "Accuracy: 0.062\n",
      "F1: 0.492\n",
      "Recall: 0.429\n",
      "Precision: 0.576\n",
      "Accuracy: 0.034\n",
      "F1: 0.147\n",
      "Recall: 0.234\n",
      "Precision: 0.107\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.464\n",
      "Recall: 0.396\n",
      "Precision: 0.560\n",
      "Accuracy: 0.025\n",
      "F1: 0.115\n",
      "Recall: 0.176\n",
      "Precision: 0.086\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.156\n",
      "Recall: 0.132\n",
      "Precision: 0.191\n",
      "Accuracy: 0.031\n",
      "F1: 0.252\n",
      "Recall: 0.216\n",
      "Precision: 0.301\n",
      "\n",
      "Accuracy: 0.024\n",
      "F1: 0.253\n",
      "Recall: 0.168\n",
      "Precision: 0.511\n",
      "Accuracy: 0.057\n",
      "F1: 0.295\n",
      "Recall: 0.392\n",
      "Precision: 0.236\n",
      "\n",
      "Accuracy: 0.047\n",
      "F1: 0.385\n",
      "Recall: 0.326\n",
      "Precision: 0.471\n",
      "Accuracy: 0.039\n",
      "F1: 0.228\n",
      "Recall: 0.267\n",
      "Precision: 0.199\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.075\n",
      "Recall: 0.051\n",
      "Precision: 0.137\n",
      "Accuracy: 0.064\n",
      "F1: 0.340\n",
      "Recall: 0.443\n",
      "Precision: 0.276\n",
      "\n",
      "Accuracy: 0.094\n",
      "F1: 0.682\n",
      "Recall: 0.648\n",
      "Precision: 0.720\n",
      "Accuracy: 0.019\n",
      "F1: 0.084\n",
      "Recall: 0.132\n",
      "Precision: 0.062\n",
      "\n",
      "200 1e-06\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=60 loss=683.4595947265625\n",
      "iteration=80 loss=571.4036865234375\n",
      "iteration=100 loss=490.936767578125\n",
      "iteration=120 loss=432.13812255859375\n",
      "iteration=140 loss=388.86376953125\n",
      "iteration=160 loss=356.7801513671875\n",
      "iteration=180 loss=332.7320861816406\n",
      "iteration=199 loss=315.2413635253906\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=60 loss=490.2699890136719\n",
      "iteration=80 loss=417.74920654296875\n",
      "iteration=100 loss=370.448486328125\n",
      "iteration=120 loss=337.5798034667969\n",
      "iteration=140 loss=314.08551025390625\n",
      "iteration=160 loss=296.92120361328125\n",
      "iteration=180 loss=284.0343017578125\n",
      "iteration=199 loss=274.5367431640625\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=60 loss=606.5533447265625\n",
      "iteration=80 loss=500.4305725097656\n",
      "iteration=100 loss=427.27203369140625\n",
      "iteration=120 loss=376.1902770996094\n",
      "iteration=140 loss=340.33721923828125\n",
      "iteration=160 loss=314.924072265625\n",
      "iteration=180 loss=296.630126953125\n",
      "iteration=199 loss=283.8272705078125\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=60 loss=600.36669921875\n",
      "iteration=80 loss=506.4349365234375\n",
      "iteration=100 loss=441.48260498046875\n",
      "iteration=120 loss=395.79022216796875\n",
      "iteration=140 loss=363.061279296875\n",
      "iteration=160 loss=339.0338439941406\n",
      "iteration=180 loss=320.9342956542969\n",
      "iteration=199 loss=307.9380798339844\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=60 loss=559.2557373046875\n",
      "iteration=80 loss=470.14959716796875\n",
      "iteration=100 loss=410.13983154296875\n",
      "iteration=120 loss=369.2391052246094\n",
      "iteration=140 loss=340.70111083984375\n",
      "iteration=160 loss=320.0384216308594\n",
      "iteration=180 loss=304.5850524902344\n",
      "iteration=199 loss=293.1958923339844\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=60 loss=534.2129516601562\n",
      "iteration=80 loss=453.0172119140625\n",
      "iteration=100 loss=397.37713623046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=120 loss=358.08795166015625\n",
      "iteration=140 loss=329.7733459472656\n",
      "iteration=160 loss=308.8278503417969\n",
      "iteration=180 loss=293.0078430175781\n",
      "iteration=199 loss=281.3431396484375\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=60 loss=668.5250244140625\n",
      "iteration=80 loss=557.8056030273438\n",
      "iteration=100 loss=482.36065673828125\n",
      "iteration=120 loss=429.87432861328125\n",
      "iteration=140 loss=392.03912353515625\n",
      "iteration=160 loss=362.87164306640625\n",
      "iteration=180 loss=339.8700256347656\n",
      "iteration=199 loss=322.2371520996094\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=60 loss=616.667236328125\n",
      "iteration=80 loss=527.8756103515625\n",
      "iteration=100 loss=467.033203125\n",
      "iteration=120 loss=422.45404052734375\n",
      "iteration=140 loss=388.3623046875\n",
      "iteration=160 loss=361.5158996582031\n",
      "iteration=180 loss=339.9199523925781\n",
      "iteration=199 loss=323.1628112792969\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=60 loss=576.9147338867188\n",
      "iteration=80 loss=481.43017578125\n",
      "iteration=100 loss=417.78302001953125\n",
      "iteration=120 loss=374.09326171875\n",
      "iteration=140 loss=342.72491455078125\n",
      "iteration=160 loss=320.03350830078125\n",
      "iteration=180 loss=303.2298583984375\n",
      "iteration=199 loss=290.9288024902344\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=60 loss=527.1788940429688\n",
      "iteration=80 loss=458.2295227050781\n",
      "iteration=100 loss=409.0736083984375\n",
      "iteration=120 loss=373.2250061035156\n",
      "iteration=140 loss=346.37652587890625\n",
      "iteration=160 loss=325.8358154296875\n",
      "iteration=180 loss=309.8284606933594\n",
      "iteration=199 loss=297.70782470703125\n",
      "Accuracy: 0.049\n",
      "F1: 0.335\n",
      "Recall: 0.337\n",
      "Precision: 0.332\n",
      "Accuracy: 0.037\n",
      "F1: 0.192\n",
      "Recall: 0.256\n",
      "Precision: 0.154\n",
      "\n",
      "Accuracy: 0.063\n",
      "F1: 0.488\n",
      "Recall: 0.436\n",
      "Precision: 0.553\n",
      "Accuracy: 0.049\n",
      "F1: 0.216\n",
      "Recall: 0.341\n",
      "Precision: 0.158\n",
      "\n",
      "Accuracy: 0.074\n",
      "F1: 0.570\n",
      "Recall: 0.509\n",
      "Precision: 0.647\n",
      "Accuracy: 0.039\n",
      "F1: 0.173\n",
      "Recall: 0.267\n",
      "Precision: 0.128\n",
      "\n",
      "Accuracy: 0.062\n",
      "F1: 0.492\n",
      "Recall: 0.429\n",
      "Precision: 0.576\n",
      "Accuracy: 0.034\n",
      "F1: 0.147\n",
      "Recall: 0.234\n",
      "Precision: 0.107\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.464\n",
      "Recall: 0.396\n",
      "Precision: 0.560\n",
      "Accuracy: 0.025\n",
      "F1: 0.115\n",
      "Recall: 0.176\n",
      "Precision: 0.086\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.156\n",
      "Recall: 0.132\n",
      "Precision: 0.191\n",
      "Accuracy: 0.031\n",
      "F1: 0.252\n",
      "Recall: 0.216\n",
      "Precision: 0.301\n",
      "\n",
      "Accuracy: 0.024\n",
      "F1: 0.253\n",
      "Recall: 0.168\n",
      "Precision: 0.511\n",
      "Accuracy: 0.057\n",
      "F1: 0.295\n",
      "Recall: 0.392\n",
      "Precision: 0.236\n",
      "\n",
      "Accuracy: 0.047\n",
      "F1: 0.385\n",
      "Recall: 0.326\n",
      "Precision: 0.471\n",
      "Accuracy: 0.039\n",
      "F1: 0.228\n",
      "Recall: 0.267\n",
      "Precision: 0.199\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.075\n",
      "Recall: 0.051\n",
      "Precision: 0.137\n",
      "Accuracy: 0.064\n",
      "F1: 0.340\n",
      "Recall: 0.443\n",
      "Precision: 0.276\n",
      "\n",
      "Accuracy: 0.094\n",
      "F1: 0.682\n",
      "Recall: 0.648\n",
      "Precision: 0.720\n",
      "Accuracy: 0.019\n",
      "F1: 0.084\n",
      "Recall: 0.132\n",
      "Precision: 0.062\n",
      "\n",
      "200 1e-07\n",
      "0\n",
      "iteration=0 loss=4231.5927734375\n",
      "iteration=20 loss=1124.5621337890625\n",
      "iteration=40 loss=847.6368408203125\n",
      "iteration=60 loss=683.4595947265625\n",
      "iteration=80 loss=571.4036865234375\n",
      "iteration=100 loss=490.936767578125\n",
      "iteration=120 loss=432.13812255859375\n",
      "iteration=140 loss=388.86376953125\n",
      "iteration=160 loss=356.7801513671875\n",
      "iteration=180 loss=332.7320861816406\n",
      "iteration=199 loss=315.2413635253906\n",
      "1\n",
      "iteration=0 loss=6340.234375\n",
      "iteration=20 loss=877.5382080078125\n",
      "iteration=40 loss=611.0204467773438\n",
      "iteration=60 loss=490.2699890136719\n",
      "iteration=80 loss=417.74920654296875\n",
      "iteration=100 loss=370.448486328125\n",
      "iteration=120 loss=337.5798034667969\n",
      "iteration=140 loss=314.08551025390625\n",
      "iteration=160 loss=296.92120361328125\n",
      "iteration=180 loss=284.0343017578125\n",
      "iteration=199 loss=274.5367431640625\n",
      "2\n",
      "iteration=0 loss=4075.01025390625\n",
      "iteration=20 loss=1067.015380859375\n",
      "iteration=40 loss=770.3414916992188\n",
      "iteration=60 loss=606.5533447265625\n",
      "iteration=80 loss=500.4305725097656\n",
      "iteration=100 loss=427.27203369140625\n",
      "iteration=120 loss=376.1902770996094\n",
      "iteration=140 loss=340.33721923828125\n",
      "iteration=160 loss=314.924072265625\n",
      "iteration=180 loss=296.630126953125\n",
      "iteration=199 loss=283.8272705078125\n",
      "3\n",
      "iteration=0 loss=2843.359375\n",
      "iteration=20 loss=1021.7870483398438\n",
      "iteration=40 loss=743.9141235351562\n",
      "iteration=60 loss=600.36669921875\n",
      "iteration=80 loss=506.4349365234375\n",
      "iteration=100 loss=441.48260498046875\n",
      "iteration=120 loss=395.79022216796875\n",
      "iteration=140 loss=363.061279296875\n",
      "iteration=160 loss=339.0338439941406\n",
      "iteration=180 loss=320.9342956542969\n",
      "iteration=199 loss=307.9380798339844\n",
      "4\n",
      "iteration=0 loss=5789.923828125\n",
      "iteration=20 loss=1025.9586181640625\n",
      "iteration=40 loss=702.6373291015625\n",
      "iteration=60 loss=559.2557373046875\n",
      "iteration=80 loss=470.14959716796875\n",
      "iteration=100 loss=410.13983154296875\n",
      "iteration=120 loss=369.2391052246094\n",
      "iteration=140 loss=340.70111083984375\n",
      "iteration=160 loss=320.0384216308594\n",
      "iteration=180 loss=304.5850524902344\n",
      "iteration=199 loss=293.1958923339844\n",
      "5\n",
      "iteration=0 loss=5625.7490234375\n",
      "iteration=20 loss=947.6441650390625\n",
      "iteration=40 loss=664.2020263671875\n",
      "iteration=60 loss=534.2129516601562\n",
      "iteration=80 loss=453.0172119140625\n",
      "iteration=100 loss=397.37713623046875\n",
      "iteration=120 loss=358.08795166015625\n",
      "iteration=140 loss=329.7733459472656\n",
      "iteration=160 loss=308.8278503417969\n",
      "iteration=180 loss=293.0078430175781\n",
      "iteration=199 loss=281.3431396484375\n",
      "6\n",
      "iteration=0 loss=8614.396484375\n",
      "iteration=20 loss=1174.764892578125\n",
      "iteration=40 loss=842.7100830078125\n",
      "iteration=60 loss=668.5250244140625\n",
      "iteration=80 loss=557.8056030273438\n",
      "iteration=100 loss=482.36065673828125\n",
      "iteration=120 loss=429.87432861328125\n",
      "iteration=140 loss=392.03912353515625\n",
      "iteration=160 loss=362.87164306640625\n",
      "iteration=180 loss=339.8700256347656\n",
      "iteration=199 loss=322.2371520996094\n",
      "7\n",
      "iteration=0 loss=10686.685546875\n",
      "iteration=20 loss=1105.00390625\n",
      "iteration=40 loss=766.2845458984375\n",
      "iteration=60 loss=616.667236328125\n",
      "iteration=80 loss=527.8756103515625\n",
      "iteration=100 loss=467.033203125\n",
      "iteration=120 loss=422.45404052734375\n",
      "iteration=140 loss=388.3623046875\n",
      "iteration=160 loss=361.5158996582031\n",
      "iteration=180 loss=339.9199523925781\n",
      "iteration=199 loss=323.1628112792969\n",
      "8\n",
      "iteration=0 loss=9917.166015625\n",
      "iteration=20 loss=1037.3057861328125\n",
      "iteration=40 loss=734.7451171875\n",
      "iteration=60 loss=576.9147338867188\n",
      "iteration=80 loss=481.43017578125\n",
      "iteration=100 loss=417.78302001953125\n",
      "iteration=120 loss=374.09326171875\n",
      "iteration=140 loss=342.72491455078125\n",
      "iteration=160 loss=320.03350830078125\n",
      "iteration=180 loss=303.2298583984375\n",
      "iteration=199 loss=290.9288024902344\n",
      "9\n",
      "iteration=0 loss=5148.71630859375\n",
      "iteration=20 loss=853.104248046875\n",
      "iteration=40 loss=630.2410888671875\n",
      "iteration=60 loss=527.1788940429688\n",
      "iteration=80 loss=458.2295227050781\n",
      "iteration=100 loss=409.0736083984375\n",
      "iteration=120 loss=373.2250061035156\n",
      "iteration=140 loss=346.37652587890625\n",
      "iteration=160 loss=325.8358154296875\n",
      "iteration=180 loss=309.8284606933594\n",
      "iteration=199 loss=297.70782470703125\n",
      "Accuracy: 0.049\n",
      "F1: 0.335\n",
      "Recall: 0.337\n",
      "Precision: 0.332\n",
      "Accuracy: 0.037\n",
      "F1: 0.192\n",
      "Recall: 0.256\n",
      "Precision: 0.154\n",
      "\n",
      "Accuracy: 0.063\n",
      "F1: 0.488\n",
      "Recall: 0.436\n",
      "Precision: 0.553\n",
      "Accuracy: 0.049\n",
      "F1: 0.216\n",
      "Recall: 0.341\n",
      "Precision: 0.158\n",
      "\n",
      "Accuracy: 0.074\n",
      "F1: 0.570\n",
      "Recall: 0.509\n",
      "Precision: 0.647\n",
      "Accuracy: 0.039\n",
      "F1: 0.173\n",
      "Recall: 0.267\n",
      "Precision: 0.128\n",
      "\n",
      "Accuracy: 0.062\n",
      "F1: 0.492\n",
      "Recall: 0.429\n",
      "Precision: 0.576\n",
      "Accuracy: 0.034\n",
      "F1: 0.147\n",
      "Recall: 0.234\n",
      "Precision: 0.107\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.464\n",
      "Recall: 0.396\n",
      "Precision: 0.560\n",
      "Accuracy: 0.025\n",
      "F1: 0.115\n",
      "Recall: 0.176\n",
      "Precision: 0.086\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.156\n",
      "Recall: 0.132\n",
      "Precision: 0.191\n",
      "Accuracy: 0.031\n",
      "F1: 0.252\n",
      "Recall: 0.216\n",
      "Precision: 0.301\n",
      "\n",
      "Accuracy: 0.024\n",
      "F1: 0.253\n",
      "Recall: 0.168\n",
      "Precision: 0.511\n",
      "Accuracy: 0.057\n",
      "F1: 0.295\n",
      "Recall: 0.392\n",
      "Precision: 0.236\n",
      "\n",
      "Accuracy: 0.047\n",
      "F1: 0.385\n",
      "Recall: 0.326\n",
      "Precision: 0.471\n",
      "Accuracy: 0.039\n",
      "F1: 0.228\n",
      "Recall: 0.267\n",
      "Precision: 0.199\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.075\n",
      "Recall: 0.051\n",
      "Precision: 0.137\n",
      "Accuracy: 0.064\n",
      "F1: 0.340\n",
      "Recall: 0.443\n",
      "Precision: 0.276\n",
      "\n",
      "Accuracy: 0.094\n",
      "F1: 0.682\n",
      "Recall: 0.648\n",
      "Precision: 0.720\n",
      "Accuracy: 0.019\n",
      "F1: 0.084\n",
      "Recall: 0.132\n",
      "Precision: 0.062\n",
      "\n",
      "[50, 9, DPLabelModel(), 0.10663129973474801, 0.7003484320557491, 0.7362637362637363, 0.6677740863787376, 0.006896551724137931, 0.03254067584480601, 0.047619047619047616, 0.024714828897338403]\n",
      "CPU times: user 1h 12min 47s, sys: 1min 22s, total: 1h 14min 10s\n",
      "Wall time: 1h 14min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best = None\n",
    "for iterations in [1, 5,10, 25, 50, 100, 200]:\n",
    "    for learning_rate in [1e-4, 1e-5, 1e-6, 1e-7]:\n",
    "        print(iterations, learning_rate)\n",
    "        max_seed = 10\n",
    "        temporal_models = [None,]*max_seed\n",
    "        for seed in range(max_seed):\n",
    "            print(seed)\n",
    "            markov_model = DPLabelModel(m=m_per_task*T, \n",
    "                                        T=T,\n",
    "                                        edges=[(i,i+m_per_task) for i in range((T-1)*m_per_task)],\n",
    "                                        coverage_sets=[[t,] for t in range(T) for _ in range(m_per_task)],\n",
    "                                        mu_sharing=[[t*m_per_task+i for t in range(T)] for i in range(m_per_task)],\n",
    "                                        phi_sharing=[[(t*m_per_task+i, (t+1)*m_per_task+i)\n",
    "                                                      for t in range(T-1)] for i in range(m_per_task)],\n",
    "                                        device=device,\n",
    "                                        class_balance=torch.tensor(class_balance).float().to(device),\n",
    "                                        seed=seed)\n",
    "            optimize(markov_model, L_hat=MRI_data_temporal['Li_train'], num_iter=iterations,\n",
    "                     lr=1e-5, momentum=0.8, clamp=True, \n",
    "                     verbose=iterations >= 10, seed=seed)\n",
    "            temporal_models[seed] = markov_model\n",
    "\n",
    "        for seed, model in enumerate(temporal_models):\n",
    "            Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            scores = [iterations, seed, model]\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "                \n",
    "            model.flip_params()\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "            \n",
    "            model.flip_params()\n",
    "\n",
    "            if best == None or scores[4] > max(best[4], best[8]) or scores[8] > max(best[4], best[8]):\n",
    "                best = scores\n",
    "            print()\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.107\n",
      "F1: 0.700\n",
      "Recall: 0.736\n",
      "Precision: 0.668\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.083\n",
      "F1: 0.690\n",
      "Recall: 0.575\n",
      "Precision: 0.863\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1355.,   85.,   73.,   26.,   45.,   25.,   28.,   40.,   47.,\n",
       "         161.]),\n",
       " array([2.64933485e-07, 9.99409961e-02, 1.99881727e-01, 2.99822458e-01,\n",
       "        3.99763190e-01, 4.99703921e-01, 5.99644652e-01, 6.99585383e-01,\n",
       "        7.99526114e-01, 8.99466845e-01, 9.99407576e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_frame_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, 'models/ts_labelmodel_best_tuning_high_pre_downsampled.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/ts_labelmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel_best_tuning_high_pre_downsampled.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.107\n",
      "F1: 0.700\n",
      "Recall: 0.736\n",
      "Precision: 0.668\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = model.eval().predict_element_proba(\n",
    "    MRI_data_temporal['Li_dev'].to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.113\n",
      "F1: 0.599\n",
      "Recall: 0.542\n",
      "Precision: 0.670\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = model.eval().predict_element_proba(\n",
    "    MRI_data_temporal['Li_test'].to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_test.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions for everything and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sparse\n",
    "import pickle\n",
    "import rekall\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "from rekall.interval_list import IntervalList\n",
    "from rekall.temporal_predicates import *\n",
    "from metal.label_model.baselines import MajorityLabelVoter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load manually annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 10533.73it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 28637.04it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/manually_annotated_shots.pkl', 'rb') as f:\n",
    "    shots = VideoIntervalCollection(pickle.load(f))\n",
    "with open('../../data/shot_detection_folds.pkl', 'rb') as f:\n",
    "    shot_detection_folds = pickle.load(f)\n",
    "clips = shots.dilate(1).coalesce().dilate(-1)\n",
    "shot_boundaries = shots.map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.start, intrvl.payload)\n",
    ").set_union(\n",
    "    shots.map(lambda intrvl: (intrvl.end + 1, intrvl.end + 1, intrvl.payload))\n",
    ").coalesce()\n",
    "boundary_frames = {\n",
    "    video_id: [\n",
    "        intrvl.start\n",
    "        for intrvl in shot_boundaries.get_intervallist(video_id).get_intervals()\n",
    "    ]\n",
    "    for video_id in shot_boundaries.get_allintervals()\n",
    "}\n",
    "video_ids = sorted(list(clips.get_allintervals().keys()))\n",
    "frames_per_video = {\n",
    "    video_id: sorted([\n",
    "        f\n",
    "        for interval in clips.get_intervallist(video_id).get_intervals()\n",
    "        for f in range(interval.start, interval.end + 2)\n",
    "    ])\n",
    "    for video_id in video_ids\n",
    "}\n",
    "ground_truth = {\n",
    "    video_id: [\n",
    "        1 if f in boundary_frames[video_id] else 2\n",
    "        for f in frames_per_video[video_id]\n",
    "    ] \n",
    "    for video_id in video_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load label matrix with all frames in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/all_labels_high_pre.pkl', 'rb') as f:\n",
    "    weak_labels_all_movies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load videos and number of frames per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/frame_counts.pkl', 'rb') as f:\n",
    "    frame_counts = pickle.load(f)\n",
    "video_ids_all = sorted(list(frame_counts.keys()))\n",
    "video_ids_train = sorted(list(set(video_ids_all).difference(set(video_ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct windows for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, construct windows of 16 frames for each video\n",
    "windows = VideoIntervalCollection({\n",
    "    video_id: [\n",
    "        (f, f + 16, video_id)\n",
    "        for f in range(0, frame_counts[video_id] - 16, 16)\n",
    "    ]\n",
    "    for video_id in video_ids_all\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truth labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, intersect the windows with ground truth and get ground truth labels for the windows\n",
    "windows_intersecting_ground_truth = windows.filter_against(\n",
    "    clips,\n",
    "    predicate=overlaps()\n",
    ").map(lambda intrvl: (intrvl.start, intrvl.end, 2))\n",
    "windows_with_shot_boundaries = windows_intersecting_ground_truth.filter_against(\n",
    "    shot_boundaries,\n",
    "    predicate = lambda window, shot_boundary:\n",
    "        shot_boundary.start >= window.start and shot_boundary.start < window.end\n",
    ").map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.end, 1)\n",
    ")\n",
    "windows_with_labels = windows_with_shot_boundaries.set_union(\n",
    "    windows_intersecting_ground_truth\n",
    ").coalesce(\n",
    "    predicate = equal(),\n",
    "    payload_merge_op = lambda p1, p2: min(p1, p2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weak labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label windows with the weak labels in our labeling functions\n",
    "def label_window(per_frame_weak_labels):\n",
    "    if 1 in per_frame_weak_labels:\n",
    "        return 1\n",
    "    if len([l for l in per_frame_weak_labels if l == 2]) >= len(per_frame_weak_labels) / 2:\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "windows_with_weak_labels = windows.map(\n",
    "    lambda window: (\n",
    "        window.start,\n",
    "        window.end,\n",
    "        [\n",
    "            label_window([\n",
    "                lf[window.payload][f-1]\n",
    "                for f in range(window.start, window.end)\n",
    "            ])\n",
    "            for lf in weak_labels_all_movies\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_everything_windows = csr_matrix([\n",
    "    intrvl.payload\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows_high_pre_downsampled.npy', 'wb') as f:\n",
    "    np.save(f, L_everything_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows_high_pre_downsampled.npy', 'rb') as f:\n",
    "    L_everything_windows = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert L matrix to timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5\n",
    "m_per_task = L_everything_windows.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled = torch.FloatTensor(L_everything_windows[:L_everything_windows.shape[0] -\n",
    "                                                      (L_everything_windows.shape[0] % T)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_per_task_unlabelled = L_unlabelled.size(1)\n",
    "n_frames_unlabelled = L_unlabelled.size(0)\n",
    "n_patients_unlabelled = n_frames_unlabelled//T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled_ts = torch.LongTensor(\n",
    "    L_unlabelled.view(n_patients_unlabelled, (m_per_task*T)).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235081"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled_ts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val = model.eval().predict_element_proba(MRI_data_temporal['Li_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1355.,   85.,   73.,   26.,   45.,   25.,   28.,   40.,   47.,\n",
       "         161.]),\n",
       " array([2.64933485e-07, 9.99409961e-02, 1.99881727e-01, 2.99822458e-01,\n",
       "        3.99763190e-01, 4.99703921e-01, 5.99644652e-01, 6.99585383e-01,\n",
       "        7.99526114e-01, 8.99466845e-01, 9.99407576e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.909\n",
      "F1: 0.700\n",
      "Recall: 0.736\n",
      "Precision: 0.668\n"
     ]
    }
   ],
   "source": [
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu().where(Y_dev.cpu() == torch.tensor(1.), torch.tensor(0.)),\n",
    "                         np.round(predictions_val), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n"
     ]
    }
   ],
   "source": [
    "predictions_everything = []\n",
    "for i in range(0, L_unlabelled_ts.shape[0], 100000):\n",
    "    print(i)\n",
    "    start = i\n",
    "    end = i + 100000\n",
    "    labels = L_unlabelled_ts[start:end] if end < L_unlabelled_ts.shape[0] else L_unlabelled_ts[start:]\n",
    "    predictions_for_labels = model.eval().predict_element_proba(labels.to(device))\n",
    "    predictions_everything.append(predictions_for_labels)\n",
    "    del predictions_for_labels\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6175405,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(predictions_everything).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_pred_probs_per_frame = np.concatenate(predictions_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_pred_frame = predictions_everything_together\n",
    "\n",
    "# #find sequence label config. with highest prob.\n",
    "# config_index = np.argmax(R_pred_frame, axis=1)\n",
    "# R_pred_config = model.feasible_y[config_index].detach().cpu()\n",
    "# R_pred_max = torch.FloatTensor(np.max(R_pred_frame.numpy(), axis=1))\n",
    "\n",
    "# #for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "# R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "# for idx in range(R_pred_config.shape[0]):\n",
    "#     R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:]).float()*R_pred_max[idx]\n",
    "\n",
    "# R_pred_probs = R_pred_probs.numpy()\n",
    "# R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "# R_pred_frame_label = np.round(R_pred_probs.ravel())\n",
    "# R_pred_frame_label[R_pred_frame_label == 0.] = 2.\n",
    "\n",
    "# R_pred_probs_per_frame = R_pred_probs.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4611900.,  259713.,  209570.,  127121.,  111559.,   84019.,\n",
       "          91010.,   97798.,  161422.,  421293.]),\n",
       " array([2.47917929e-10, 9.99992120e-02, 1.99998424e-01, 2.99997635e-01,\n",
       "        3.99996847e-01, 4.99996059e-01, 5.99995271e-01, 6.99994482e-01,\n",
       "        7.99993694e-01, 8.99992906e-01, 9.99992118e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_probs_per_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_nums = [\n",
    "    (video_id, intrvl.start, intrvl.end)\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows = [\n",
    "    (window_info, np.array([prediction, 1. - prediction]))\n",
    "    for window_info, prediction in zip(window_nums, R_pred_probs_per_frame)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we needed to cut the predictions to a multiple of T\n",
    "last_preds = []\n",
    "for window_info in window_nums[len(predictions_to_save_windows):]:\n",
    "    last_preds.append((window_info, np.array([0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows += last_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np_windows = np.array(predictions_to_save_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to disk\n",
    "with open('../../data/shot_detection_weak_labels/ts_weak_labels_all_windows_tuned_high_pre_downsampled.npy', 'wb') as f:\n",
    "    np.save(f, preds_np_windows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
