{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T02:24:09.628663Z",
     "start_time": "2018-12-05T02:24:01.299243Z"
    }
   },
   "outputs": [],
   "source": [
    "from esper.prelude import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "import pyro.infer as infer\n",
    "from torch.utils.data import DataLoader\n",
    "from transcript_utils import *\n",
    "from timeit import default_timer as now\n",
    "from custom_mlp import MLP, Exp\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T02:26:02.788078Z",
     "start_time": "2018-12-05T02:25:37.982360Z"
    }
   },
   "outputs": [],
   "source": [
    "mi_dict = {ngram: score for [ngram, score] in mutual_info('immigration')}\n",
    "mi_priors = torch.tensor([mi_dict[ngram] if ngram in mi_dict else 0 for ngram in vocabulary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T02:26:02.928163Z",
     "start_time": "2018-12-05T02:26:02.792557Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in ['immigration', 'border', 'healthcare']:\n",
    "    print('{} {:.4f}'.format(k, mi_priors[vocabulary.index(k)].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T05:06:28.625805Z",
     "start_time": "2018-12-03T05:06:28.597004Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute_vectors(video_list(), vocabulary, SEGMENT_SIZE, SEGMENT_STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T02:38:20.757461Z",
     "start_time": "2018-12-05T02:38:20.665784Z"
    }
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        # p = number of features\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(p, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "    \n",
    "    def model(self, x, y):\n",
    "        # Create unit normal priors over the parameters\n",
    "        loc, scale = torch.zeros(self.p), torch.ones(self.p) * 10\n",
    "        bias_loc, bias_scale = torch.zeros(1), torch.ones(1) * 10\n",
    "        w_prior = dist.Normal(loc, scale).independent(1)\n",
    "        b_prior = dist.Normal(bias_loc, bias_scale).independent(1)\n",
    "        priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "        # lift module parameters to random variables sampled from the priors\n",
    "        lifted_module = pyro.random_module(\"module\", self, priors)\n",
    "        # sample a regressor (which also samples w and b)\n",
    "        lifted_reg_model = lifted_module()\n",
    "        with pyro.iarange(\"map\", x.shape[0]):\n",
    "            # run the regressor forward conditioned on data\n",
    "            prediction_mean = lifted_reg_model(x).squeeze(-1)\n",
    "            # condition on the observed data\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(prediction_mean), obs=y)                                                \n",
    "\n",
    "    def guide(self, x, y):\n",
    "        # define our variational parameters\n",
    "        w_loc = torch.tensor(mi_priors)\n",
    "        # note that we initialize our scales to be pretty narrow\n",
    "        w_log_sig = torch.tensor(-3.0 * torch.ones(1, self.p) + 0.05 * torch.randn(1, self.p))\n",
    "        b_loc = torch.tensor(0.5) + 0.05 * torch.randn(1)\n",
    "        b_log_sig = torch.tensor(-3.0 * torch.ones(1) + 0.05 * torch.randn(1))\n",
    "        # register learnable params in the param store\n",
    "        mw_param = pyro.param(\"guide_mean_weight\", w_loc)\n",
    "        sw_param = self.softplus(pyro.param(\"guide_log_scale_weight\", w_log_sig))\n",
    "        mb_param = pyro.param(\"guide_mean_bias\", b_loc)\n",
    "        sb_param = self.softplus(pyro.param(\"guide_log_scale_bias\", b_log_sig))\n",
    "        # guide distributions for w and b\n",
    "        w_dist = dist.Normal(mw_param, sw_param).independent(1)\n",
    "        b_dist = dist.Normal(mb_param, sb_param).independent(1)\n",
    "        dists = {'linear.weight': w_dist, 'linear.bias': b_dist}\n",
    "        # overload the parameters in the module with random samples\n",
    "        # from the guide distributions\n",
    "        lifted_module = pyro.random_module(\"module\", self, dists)\n",
    "        # sample a regressor (which also samples w and b)\n",
    "        return lifted_module()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T08:38:53.159161Z",
     "start_time": "2018-12-06T08:38:51.313278Z"
    }
   },
   "outputs": [],
   "source": [
    "unsup_dataset = SegmentVectorDataset(video_list(), vocab_size=vocab_size, inmemory=True)\n",
    "text_dataset = SegmentTextDataset(video_list())\n",
    "unsup_loader = DataLoader(unsup_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T08:38:54.239423Z",
     "start_time": "2018-12-06T08:38:54.195397Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model_gen, x, y, iters=100):\n",
    "    samples = []\n",
    "    for _ in range(iters):\n",
    "        model = model_gen()\n",
    "        y_pred = model(x).squeeze(-1).round()\n",
    "        fp = torch.sum((y_pred != y) & (y_pred == 1)).item()\n",
    "        fn =  torch.sum((y_pred != y) & (y_pred == 0)).item()\n",
    "        acc = torch.sum(y_pred == y).item()\n",
    "        n = float(y_pred.shape[0])\n",
    "        samples.append(torch.tensor([acc/n, fp/n, fn/n]))\n",
    "    return torch.mean(torch.stack(samples), dim=0).tolist(), torch.std(torch.stack(samples), dim=0).tolist(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T09:06:32.802277Z",
     "start_time": "2018-12-06T09:06:32.754567Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_data(labels):\n",
    "    dataset = LabeledSegmentDataset(unsup_dataset, labels, categories=2)\n",
    "\n",
    "    x_data, y_data, _ = unzip(list(dataset))\n",
    "    y_data = torch.tensor([y[1] for y in y_data])\n",
    "    x_data = torch.stack(x_data)\n",
    "    \n",
    "    split = int(len(x_data) * 2 / 3)\n",
    "\n",
    "    (train_x, val_x) = (x_data[:split], x_data[split:])\n",
    "    (train_y, val_y) = (y_data[:split], y_data[split:])\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y\n",
    "\n",
    "def add_labels(labels, train_x, train_y, val_x, val_y):\n",
    "    active_train_x, active_train_y, active_val_x, active_val_y = format_data(labels)\n",
    "    train_x = torch.cat((train_x, active_train_x))\n",
    "    train_y = torch.cat((train_y, active_train_y))\n",
    "    val_x = torch.cat((val_x, active_val_x))\n",
    "    val_y = torch.cat((val_y, active_val_y))\n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T07:32:46.876914Z",
     "start_time": "2018-12-06T07:32:46.822264Z"
    }
   },
   "outputs": [],
   "source": [
    "# mi_priors_raw = torch.tensor([mi_dict[ngram] if ngram in mi_dict else 0 for ngram in vocabulary])\n",
    "# def baseline_model(x):\n",
    "#     return torch.mm(x, mi_priors_raw.unsqueeze(0).t()).squeeze()\n",
    "# acc = get_accuracy(lambda: baseline_model,\n",
    "#              torch.cat((train_x, val_x)), torch.cat((train_y, val_y)), iters=2)[0][0]\n",
    "\n",
    "# print('Baseline accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T09:04:26.413779Z",
     "start_time": "2018-12-06T09:04:26.274545Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'regression_active'\n",
    "regression_model = RegressionModel(vocab_size)\n",
    "\n",
    "def weights_path(iteration, epoch):\n",
    "    return '/app/data/models/transcript_{}_weights_iter{:02d}_epoch{:04d}.pt'.format(model_name, iteration, epoch)\n",
    "\n",
    "def best_weights_path(iteration):\n",
    "    return '/app/data/models/transcript_{}_best_weights_iter{:02d}.pt'.format(model_name, iteration)\n",
    "\n",
    "def torch_trainer():\n",
    "    loss_fn = nn.MSELoss(size_average=False)\n",
    "    optim = torch.optim.Adam(regression_model.parameters(), lr=0.05)\n",
    "    \n",
    "    def step(x, y):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model(x).squeeze(-1)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        return loss.item()\n",
    "        \n",
    "    return step, lambda: regression_model\n",
    "\n",
    "def pyro_trainer():\n",
    "    pyro.clear_param_store()\n",
    "    opt = optim.Adam({\"lr\": 0.01})\n",
    "    svi = infer.SVI(regression_model.model, regression_model.guide, opt, loss=infer.Trace_ELBO())\n",
    "    \n",
    "    def step(x, y):\n",
    "        return svi.step(x, y)\n",
    "    \n",
    "    return step, lambda: regression_model.guide(None, None)\n",
    "        \n",
    "def train(iteration, step, model_gen, train_x, train_y, val_x, val_y, epochs=100, checkpoint_frequency=5, verbose=False):\n",
    "    accs = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = step(train_x, train_y)\n",
    "        if epoch % checkpoint_frequency == 0:\n",
    "            [tacc, tfp, tfn], _ = get_accuracy(model_gen, train_x, train_y)\n",
    "            [vacc, vfp, vfn], [vaccstd, vfpstd, vfnstd] = get_accuracy(model_gen, val_x, val_y)\n",
    "            if verbose:\n",
    "                print(\"[iteration %04d] loss: %.0f, train: acc %.3f, val: acc %.3f (+/- %.3f) fp %.3f (+/- %.3f) fn %.3f (+/ %.3f)\" % \n",
    "                      (epoch, loss, tacc, vacc, vaccstd, vfp, vfpstd, vfn, vfnstd))\n",
    "            pyro.get_param_store().save(weights_path(iteration, epoch))\n",
    "            accs.append(vacc)\n",
    "    best_epoch = torch.tensor(accs).argmax().item() * checkpoint_frequency\n",
    "    pyro.get_param_store().load(weights_path(iteration, best_epoch))\n",
    "    pyro.get_param_store().save(best_weights_path(iteration))\n",
    "    return best_epoch\n",
    "\n",
    "def model_uncertainty(model_gen, x, iters=20):    \n",
    "    ys_pred = []\n",
    "    for _ in range(iters):\n",
    "        model = model_gen()\n",
    "        ys_pred.append(model(x).squeeze(-1).round())\n",
    "    return torch.stack(ys_pred).std(dim=0)\n",
    "\n",
    "def max_uncertainty(model_gen, batches=None):\n",
    "    all_std = []\n",
    "    all_idx = []\n",
    "    loader = itertools.islice(unsup_loader, batches) if batches is not None else unsup_loader\n",
    "    for x, i in tqdm(loader):\n",
    "        all_std.append(model_uncertainty(model_gen, x))\n",
    "        all_idx.append(i)\n",
    "\n",
    "    all_std = torch.cat(all_std)\n",
    "    all_idx = torch.cat(all_idx)\n",
    "    \n",
    "    top_std, top_idx = all_std.topk(1000)\n",
    "    top_idx = top_idx.tolist()\n",
    "    random.shuffle(top_idx)\n",
    "    \n",
    "    return top_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T09:04:37.750798Z",
     "start_time": "2018-12-06T09:04:29.780726Z"
    }
   },
   "outputs": [],
   "source": [
    "step, model_gen = pyro_trainer()\n",
    "print(train(0, step, model_gen, *format_data(pcache.get('labeled_segments')), verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T09:05:17.637202Z",
     "start_time": "2018-12-06T09:05:17.579571Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = format_data(pcache.get('labeled_segments'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T09:10:48.327049Z",
     "start_time": "2018-12-06T09:08:08.015011Z"
    }
   },
   "outputs": [],
   "source": [
    "step, model_gen = pyro_trainer()\n",
    "\n",
    "def train_and_label(iteration, train_x, train_y, val_x, val_y):\n",
    "    best_epoch = train(iteration, step, model_gen, train_x, train_y, val_x, val_y)\n",
    "    \n",
    "    def eval_model(it):        \n",
    "        pyro.get_param_store().load(best_weights_path(it))\n",
    "        [vacc, vfp, vfn], [vaccstd, vfpstd, vfnstd] = get_accuracy(model_gen, val_x, val_y, iters=1000)\n",
    "        print(\"[iteration %d] acc %.3f (+/- %.3f) fp %.3f (+/- %.3f) fn %.3f (+/ %.3f)\" % \n",
    "            (it, vacc, vaccstd, vfp, vfpstd, vfn, vfnstd))\n",
    "        \n",
    "    if iteration > 0:\n",
    "        eval_model(iteration - 1)\n",
    "    eval_model(iteration)\n",
    "    \n",
    "    indices = max_uncertainty(model_gen)\n",
    "    \n",
    "    def done_callback(labels):\n",
    "        print('Added {} labels'.format(len(labels)))\n",
    "        pcache.set('active_labels_{}'.format(iteration), labels)\n",
    "        train_and_label(iteration + 1, *add_labels(labels, train_x, train_y, val_x, val_y))\n",
    "    \n",
    "    label_widget(text_dataset, indices, done_callback)\n",
    "    \n",
    "train_and_label(0, *format_data(pcache.get('labeled_segments')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T02:38:50.759553Z",
     "start_time": "2018-12-05T02:38:50.722747Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(path):\n",
    "    pyro.get_param_store().load(path)\n",
    "    old_model = RegressionModel(vocab_size)\n",
    "    return get_accuracy((lambda: old_model.guide(None, None)), val_x, val_y, iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T02:39:45.108490Z",
     "start_time": "2018-12-05T02:39:43.415738Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_model('/app/data/models/transcript_regression_weights_epoch00080.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T02:39:43.412544Z",
     "start_time": "2018-12-05T02:39:41.715925Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_model('/app/data/models/transcript_regression_active_weights_epoch00060.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T22:17:30.902335Z",
     "start_time": "2018-12-03T22:17:30.872732Z"
    }
   },
   "outputs": [],
   "source": [
    "data = regression_model.guide(None, None)(x_data).squeeze(-1)\n",
    "print(data)\n",
    "print(data.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T22:07:54.922355Z",
     "start_time": "2018-12-03T22:07:54.883478Z"
    }
   },
   "outputs": [],
   "source": [
    "list(regression_model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-03T05:06:29.046312Z",
     "start_time": "2018-12-03T05:06:18.344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for name, param in regression_model.named_parameters():\n",
    "    if name == 'linear.weight':\n",
    "        weights = param.data.numpy().squeeze()\n",
    "        idx = np.argsort(weights)[::-1]\n",
    "        print(weights[idx])\n",
    "        print(np.array(vocabulary)[idx][:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
