{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, pickle, csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "sys.path.append('/lfs/1/danfu/metal')\n",
    "sys.path.append('/lfs/1/danfu/sequential_ws')\n",
    "from metal.metrics import metric_score\n",
    "from torch.nn.functional import normalize\n",
    "from DP.label_model import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_train_100_windows_downsampled.npz'\n",
    "L_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_val_windows_downsampled.npz'\n",
    "Y_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_val_windows_downsampled.npy'\n",
    "L_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_test_windows_downsampled.npz'\n",
    "Y_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_test_windows_downsampled.npy'\n",
    "\n",
    "stride = 1\n",
    "L_train_raw = sp.sparse.load_npz(L_train_path).todense()[::stride]\n",
    "L_dev_raw = sp.sparse.load_npz(L_dev_path).todense()\n",
    "Y_dev_raw = np.load(Y_dev_path)\n",
    "L_test_raw = sp.sparse.load_npz(L_test_path).todense()\n",
    "Y_test_raw = np.load(Y_test_path)\n",
    "\n",
    "T = 5\n",
    "\n",
    "L_train = torch.FloatTensor(L_train_raw[:L_train_raw.shape[0] - (L_train_raw.shape[0] % T)]).to(device)\n",
    "L_dev = torch.FloatTensor(L_dev_raw[:L_dev_raw.shape[0] - (L_dev_raw.shape[0] % T)]).to(device)\n",
    "Y_dev = torch.FloatTensor(Y_dev_raw[:Y_dev_raw.shape[0] - (Y_dev_raw.shape[0] % T)]).to(device)\n",
    "L_test = torch.FloatTensor(L_test_raw[:L_test_raw.shape[0] - (L_test_raw.shape[0] % T)]).to(device)\n",
    "Y_test = torch.FloatTensor(Y_test_raw[:Y_test_raw.shape[0] - (Y_test_raw.shape[0] % T)]).to(device)\n",
    "m_per_task = L_train.size(1)\n",
    "n_frames_train = L_train.size(0)\n",
    "n_patients_train = n_frames_train//T\n",
    "n_frames_dev = L_dev.size(0)\n",
    "n_patients_dev = n_frames_dev//T\n",
    "n_frames_test = L_test.size(0)\n",
    "n_patients_test = n_frames_test//T\n",
    "\n",
    "# MRI_data_naive = {'Li_train': (L_train.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'Li_dev': (L_dev.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'R_dev': (Y_dev.unsqueeze(1) == torch.FloatTensor([-1,1]).to(device).unsqueeze(0)).argmax(1),\n",
    "#                   'm':m_per_task, 'T':1,\n",
    "#                  }\n",
    "\n",
    "# don't need to transform the raw data\n",
    "MRI_data_naive = {'Li_train': L_train.long().to(device),\n",
    "                  'Li_dev': L_dev.long().to(device),\n",
    "                  'R_dev': Y_dev.long().to(device),\n",
    "                  'Li_test': L_test.long().to(device),\n",
    "                  'R_test': Y_test.long().to(device),\n",
    "                  'm':m_per_task, 'T':1,\n",
    "                 }\n",
    "MRI_data_naive['class_balance'] = normalize((MRI_data_naive['R_dev'].unsqueeze(1)==torch.arange(2, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                            dim=0, p=1)\n",
    "MRI_data_temporal = {'Li_train': MRI_data_naive['Li_train'].view(n_patients_train, (m_per_task*T)),\n",
    "                     'Li_dev': MRI_data_naive['Li_dev'].view(n_patients_dev, (m_per_task*T)),\n",
    "                     'R_dev': MRI_data_naive['R_dev']*(2**T-1),\n",
    "                     'Li_test': MRI_data_naive['Li_test'].view(n_patients_test, (m_per_task*T)),\n",
    "                     'R_test': MRI_data_naive['R_test']*(2**T-1),\n",
    "                     'm': m_per_task * T, 'T': T,\n",
    "                    } \n",
    "MRI_data_temporal['class_balance'] = normalize((MRI_data_temporal['R_dev'].unsqueeze(1)==torch.arange(2**T, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                                dim=0, p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MRI_data_naive['class_balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=50.6702766418457\n",
      "iteration=300 loss=8.80715274810791\n",
      "iteration=600 loss=3.66485595703125\n",
      "iteration=900 loss=1.8281009197235107\n",
      "iteration=1200 loss=0.9653668999671936\n",
      "iteration=1500 loss=0.6120548844337463\n",
      "iteration=1800 loss=0.4767417907714844\n",
      "iteration=2100 loss=0.40450260043144226\n",
      "iteration=2400 loss=0.3607012629508972\n",
      "iteration=2700 loss=0.3310929536819458\n",
      "iteration=2999 loss=0.30959445238113403\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           # class_balance=MRI_data_naive['class_balance'], \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=3000, lr=4.087885261759692e-05,\n",
    "         momentum=0.9, clamp=True, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.125\n",
      "F1: 0.389\n",
      "Recall: 0.712\n",
      "Precision: 0.268\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping Parameters\n",
      "Accuracy: 0.051\n",
      "F1: 0.143\n",
      "Recall: 0.288\n",
      "Precision: 0.095\n"
     ]
    }
   ],
   "source": [
    "# Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           class_balance=torch.tensor([.4, .6]).to(device), \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=5000, lr=4.087885261759692e-04,\n",
    "         momentum=0.9, clamp=False, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.171\n",
      "F1: 0.488\n",
      "Recall: 0.971\n",
      "Precision: 0.326\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping Parameters\n",
      "Accuracy: 0.069\n",
      "F1: 0.185\n",
      "Recall: 0.389\n",
      "Precision: 0.121\n"
     ]
    }
   ],
   "source": [
    "# Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=53.89699172973633\n",
      "iteration=100 loss=0.3030668795108795\n",
      "iteration=200 loss=0.23939409852027893\n",
      "iteration=300 loss=0.22569435834884644\n",
      "iteration=400 loss=0.22112223505973816\n",
      "iteration=500 loss=0.21906107664108276\n",
      "iteration=600 loss=0.2178979068994522\n",
      "iteration=700 loss=0.2171371728181839\n",
      "iteration=800 loss=0.21659354865550995\n",
      "iteration=900 loss=0.21618370711803436\n",
      "iteration=999 loss=0.21586711704730988\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           class_balance=torch.tensor([.3, .7]).to(device), \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=1000, lr=2.087885261759692e-02,\n",
    "         momentum=0., clamp=False, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.172\n",
      "F1: 0.489\n",
      "Recall: 0.975\n",
      "Precision: 0.326\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping Parameters\n",
      "Accuracy: 0.137\n",
      "F1: 0.245\n",
      "Recall: 0.777\n",
      "Precision: 0.145\n"
     ]
    }
   ],
   "source": [
    "# Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=66.03443145751953\n",
      "iteration=500 loss=2.6357004642486572\n",
      "iteration=1000 loss=0.3299635946750641\n",
      "iteration=1500 loss=0.29519757628440857\n",
      "iteration=2000 loss=0.2911560535430908\n",
      "iteration=2500 loss=0.28930333256721497\n",
      "iteration=3000 loss=0.2881740927696228\n",
      "iteration=3500 loss=0.28736594319343567\n",
      "iteration=4000 loss=0.28671225905418396\n",
      "iteration=4500 loss=0.2861360013484955\n",
      "iteration=4999 loss=0.2856000065803528\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           class_balance=torch.tensor([-.1, 1.1]).to(device), \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=5000, lr=1.087885261759692e-02,\n",
    "         momentum=0.3, clamp=False, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.154\n",
      "F1: 0.452\n",
      "Recall: 0.874\n",
      "Precision: 0.304\n",
      "Flipping Parameters\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "    # Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=52.20237731933594\n",
      "iteration=200 loss=0.23215782642364502\n",
      "iteration=400 loss=0.19091296195983887\n",
      "iteration=600 loss=0.183028444647789\n",
      "iteration=800 loss=0.18011479079723358\n",
      "iteration=1000 loss=0.17864780128002167\n",
      "iteration=1200 loss=0.17776866257190704\n",
      "iteration=1400 loss=0.17718084156513214\n",
      "iteration=1600 loss=0.17675688862800598\n",
      "iteration=1800 loss=0.1764335036277771\n",
      "iteration=1999 loss=0.176177516579628\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           class_balance=torch.tensor([.4, .6]).to(device), \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'][::4000], num_iter=2000, lr=1.087885261759692e-03,\n",
    "         momentum=.9, clamp=False, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.171\n",
      "F1: 0.488\n",
      "Recall: 0.971\n",
      "Precision: 0.326\n",
      "Flipping Parameters\n",
      "Accuracy: 0.069\n",
      "F1: 0.185\n",
      "Recall: 0.389\n",
      "Precision: 0.121\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "    # Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_target = Y_dev.long()\n",
    "T = 5\n",
    "\n",
    "feasible_y = np.array([[-1, -1, -1, -1, -1],\n",
    "        [-1,  1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1],\n",
    "        [ 1,  1, -1, -1, -1],\n",
    "        [-1, -1,  1, -1, -1],\n",
    "        [-1,  1,  1, -1, -1],\n",
    "        [ 1, -1,  1, -1, -1],\n",
    "        [ 1,  1,  1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1],\n",
    "        [-1,  1, -1,  1, -1],\n",
    "        [ 1, -1, -1,  1, -1],\n",
    "        [ 1,  1, -1,  1, -1],\n",
    "        [-1, -1,  1,  1, -1],\n",
    "        [-1,  1,  1,  1, -1],\n",
    "        [ 1, -1,  1,  1, -1],\n",
    "        [ 1,  1,  1,  1, -1],\n",
    "        [-1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1,  1],\n",
    "        [ 1,  1, -1, -1,  1],\n",
    "        [-1, -1,  1, -1,  1],\n",
    "        [-1,  1,  1, -1,  1],\n",
    "        [ 1, -1,  1, -1,  1],\n",
    "        [ 1,  1,  1, -1,  1],\n",
    "        [-1, -1, -1,  1,  1],\n",
    "        [-1,  1, -1,  1,  1],\n",
    "        [ 1, -1, -1,  1,  1],\n",
    "        [ 1,  1, -1,  1,  1],\n",
    "        [-1, -1,  1,  1,  1],\n",
    "        [-1,  1,  1,  1,  1],\n",
    "        [ 1, -1,  1,  1,  1],\n",
    "        [ 1,  1,  1,  1,  1]])\n",
    "\n",
    "feasible_y[feasible_y==-1] = 0\n",
    "feasible_y = feasible_y.tolist()\n",
    "possibilities = list(map(lambda l : ''.join(map(str,l)), feasible_y))\n",
    "\n",
    "class_balance = np.empty(2 ** T)\n",
    "#compute class balance from dev set and use laplace smoothing\n",
    "\n",
    "valid_target_copy = np.copy(valid_target)\n",
    "valid_target_copy[valid_target_copy == 2] = 0\n",
    "\n",
    "assert len(valid_target_copy) % T == 0\n",
    "num_windows = len(valid_target_copy) / T\n",
    "\n",
    "freq = {}\n",
    "for i in range(0, len(valid_target_copy), T):\n",
    "    s = ''.join(map(str,valid_target_copy[i:i+T]))\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "for i in range(len(class_balance)):\n",
    "    if possibilities[i] in freq and freq[possibilities[i]] > 5:\n",
    "        class_balance[i] = (freq[possibilities[i]] + 1) / (num_windows + len(possibilities))\n",
    "    else:\n",
    "        class_balance[i] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0001\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.036\n",
      "F1: 0.381\n",
      "Recall: 0.249\n",
      "Precision: 0.810\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.200\n",
      "Recall: 0.132\n",
      "Precision: 0.414\n",
      "Accuracy: 0.032\n",
      "F1: 0.203\n",
      "Recall: 0.220\n",
      "Precision: 0.189\n",
      "\n",
      "Accuracy: 0.050\n",
      "F1: 0.316\n",
      "Recall: 0.348\n",
      "Precision: 0.290\n",
      "Accuracy: 0.008\n",
      "F1: 0.080\n",
      "Recall: 0.055\n",
      "Precision: 0.150\n",
      "\n",
      "Accuracy: 0.062\n",
      "F1: 0.552\n",
      "Recall: 0.429\n",
      "Precision: 0.775\n",
      "Accuracy: 0.010\n",
      "F1: 0.043\n",
      "Recall: 0.066\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.024\n",
      "F1: 0.239\n",
      "Recall: 0.168\n",
      "Precision: 0.411\n",
      "Accuracy: 0.002\n",
      "F1: 0.022\n",
      "Recall: 0.011\n",
      "Precision: 0.600\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.017\n",
      "Recall: 0.018\n",
      "Precision: 0.017\n",
      "Accuracy: 0.097\n",
      "F1: 0.751\n",
      "Recall: 0.667\n",
      "Precision: 0.858\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.043\n",
      "Recall: 0.048\n",
      "Precision: 0.039\n",
      "Accuracy: 0.107\n",
      "F1: 0.736\n",
      "Recall: 0.740\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.023\n",
      "Recall: 0.037\n",
      "Precision: 0.017\n",
      "Accuracy: 0.038\n",
      "F1: 0.406\n",
      "Recall: 0.260\n",
      "Precision: 0.922\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.106\n",
      "F1: 0.687\n",
      "Recall: 0.733\n",
      "Precision: 0.647\n",
      "\n",
      "Accuracy: 0.065\n",
      "F1: 0.566\n",
      "Recall: 0.447\n",
      "Precision: 0.772\n",
      "Accuracy: 0.010\n",
      "F1: 0.058\n",
      "Recall: 0.070\n",
      "Precision: 0.050\n",
      "\n",
      "1 1e-05\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.036\n",
      "F1: 0.381\n",
      "Recall: 0.249\n",
      "Precision: 0.810\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.200\n",
      "Recall: 0.132\n",
      "Precision: 0.414\n",
      "Accuracy: 0.032\n",
      "F1: 0.203\n",
      "Recall: 0.220\n",
      "Precision: 0.189\n",
      "\n",
      "Accuracy: 0.050\n",
      "F1: 0.316\n",
      "Recall: 0.348\n",
      "Precision: 0.290\n",
      "Accuracy: 0.008\n",
      "F1: 0.080\n",
      "Recall: 0.055\n",
      "Precision: 0.150\n",
      "\n",
      "Accuracy: 0.062\n",
      "F1: 0.552\n",
      "Recall: 0.429\n",
      "Precision: 0.775\n",
      "Accuracy: 0.010\n",
      "F1: 0.043\n",
      "Recall: 0.066\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.024\n",
      "F1: 0.239\n",
      "Recall: 0.168\n",
      "Precision: 0.411\n",
      "Accuracy: 0.002\n",
      "F1: 0.022\n",
      "Recall: 0.011\n",
      "Precision: 0.600\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.017\n",
      "Recall: 0.018\n",
      "Precision: 0.017\n",
      "Accuracy: 0.097\n",
      "F1: 0.751\n",
      "Recall: 0.667\n",
      "Precision: 0.858\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.043\n",
      "Recall: 0.048\n",
      "Precision: 0.039\n",
      "Accuracy: 0.107\n",
      "F1: 0.736\n",
      "Recall: 0.740\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.023\n",
      "Recall: 0.037\n",
      "Precision: 0.017\n",
      "Accuracy: 0.038\n",
      "F1: 0.406\n",
      "Recall: 0.260\n",
      "Precision: 0.922\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.106\n",
      "F1: 0.687\n",
      "Recall: 0.733\n",
      "Precision: 0.647\n",
      "\n",
      "Accuracy: 0.065\n",
      "F1: 0.566\n",
      "Recall: 0.447\n",
      "Precision: 0.772\n",
      "Accuracy: 0.010\n",
      "F1: 0.058\n",
      "Recall: 0.070\n",
      "Precision: 0.050\n",
      "\n",
      "1 1e-06\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.036\n",
      "F1: 0.381\n",
      "Recall: 0.249\n",
      "Precision: 0.810\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.200\n",
      "Recall: 0.132\n",
      "Precision: 0.414\n",
      "Accuracy: 0.032\n",
      "F1: 0.203\n",
      "Recall: 0.220\n",
      "Precision: 0.189\n",
      "\n",
      "Accuracy: 0.050\n",
      "F1: 0.316\n",
      "Recall: 0.348\n",
      "Precision: 0.290\n",
      "Accuracy: 0.008\n",
      "F1: 0.080\n",
      "Recall: 0.055\n",
      "Precision: 0.150\n",
      "\n",
      "Accuracy: 0.062\n",
      "F1: 0.552\n",
      "Recall: 0.429\n",
      "Precision: 0.775\n",
      "Accuracy: 0.010\n",
      "F1: 0.043\n",
      "Recall: 0.066\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.024\n",
      "F1: 0.239\n",
      "Recall: 0.168\n",
      "Precision: 0.411\n",
      "Accuracy: 0.002\n",
      "F1: 0.022\n",
      "Recall: 0.011\n",
      "Precision: 0.600\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.017\n",
      "Recall: 0.018\n",
      "Precision: 0.017\n",
      "Accuracy: 0.097\n",
      "F1: 0.751\n",
      "Recall: 0.667\n",
      "Precision: 0.858\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.043\n",
      "Recall: 0.048\n",
      "Precision: 0.039\n",
      "Accuracy: 0.107\n",
      "F1: 0.736\n",
      "Recall: 0.740\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.023\n",
      "Recall: 0.037\n",
      "Precision: 0.017\n",
      "Accuracy: 0.038\n",
      "F1: 0.406\n",
      "Recall: 0.260\n",
      "Precision: 0.922\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.106\n",
      "F1: 0.687\n",
      "Recall: 0.733\n",
      "Precision: 0.647\n",
      "\n",
      "Accuracy: 0.065\n",
      "F1: 0.566\n",
      "Recall: 0.447\n",
      "Precision: 0.772\n",
      "Accuracy: 0.010\n",
      "F1: 0.058\n",
      "Recall: 0.070\n",
      "Precision: 0.050\n",
      "\n",
      "1 1e-07\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.036\n",
      "F1: 0.381\n",
      "Recall: 0.249\n",
      "Precision: 0.810\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.200\n",
      "Recall: 0.132\n",
      "Precision: 0.414\n",
      "Accuracy: 0.032\n",
      "F1: 0.203\n",
      "Recall: 0.220\n",
      "Precision: 0.189\n",
      "\n",
      "Accuracy: 0.050\n",
      "F1: 0.316\n",
      "Recall: 0.348\n",
      "Precision: 0.290\n",
      "Accuracy: 0.008\n",
      "F1: 0.080\n",
      "Recall: 0.055\n",
      "Precision: 0.150\n",
      "\n",
      "Accuracy: 0.062\n",
      "F1: 0.552\n",
      "Recall: 0.429\n",
      "Precision: 0.775\n",
      "Accuracy: 0.010\n",
      "F1: 0.043\n",
      "Recall: 0.066\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.024\n",
      "F1: 0.239\n",
      "Recall: 0.168\n",
      "Precision: 0.411\n",
      "Accuracy: 0.002\n",
      "F1: 0.022\n",
      "Recall: 0.011\n",
      "Precision: 0.600\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.017\n",
      "Recall: 0.018\n",
      "Precision: 0.017\n",
      "Accuracy: 0.097\n",
      "F1: 0.751\n",
      "Recall: 0.667\n",
      "Precision: 0.858\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.043\n",
      "Recall: 0.048\n",
      "Precision: 0.039\n",
      "Accuracy: 0.107\n",
      "F1: 0.736\n",
      "Recall: 0.740\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.023\n",
      "Recall: 0.037\n",
      "Precision: 0.017\n",
      "Accuracy: 0.038\n",
      "F1: 0.406\n",
      "Recall: 0.260\n",
      "Precision: 0.922\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.106\n",
      "F1: 0.687\n",
      "Recall: 0.733\n",
      "Precision: 0.647\n",
      "\n",
      "Accuracy: 0.065\n",
      "F1: 0.566\n",
      "Recall: 0.447\n",
      "Precision: 0.772\n",
      "Accuracy: 0.010\n",
      "F1: 0.058\n",
      "Recall: 0.070\n",
      "Precision: 0.050\n",
      "\n",
      "5 0.0001\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.055\n",
      "F1: 0.505\n",
      "Recall: 0.381\n",
      "Precision: 0.748\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.707\n",
      "Recall: 0.597\n",
      "Precision: 0.867\n",
      "Accuracy: 0.012\n",
      "F1: 0.074\n",
      "Recall: 0.084\n",
      "Precision: 0.066\n",
      "\n",
      "Accuracy: 0.055\n",
      "F1: 0.331\n",
      "Recall: 0.377\n",
      "Precision: 0.295\n",
      "Accuracy: 0.010\n",
      "F1: 0.086\n",
      "Recall: 0.070\n",
      "Precision: 0.111\n",
      "\n",
      "Accuracy: 0.078\n",
      "F1: 0.646\n",
      "Recall: 0.538\n",
      "Precision: 0.808\n",
      "Accuracy: 0.008\n",
      "F1: 0.035\n",
      "Recall: 0.055\n",
      "Precision: 0.026\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.536\n",
      "Recall: 0.396\n",
      "Precision: 0.831\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.010\n",
      "F1: 0.105\n",
      "Recall: 0.066\n",
      "Precision: 0.261\n",
      "Accuracy: 0.048\n",
      "F1: 0.449\n",
      "Recall: 0.330\n",
      "Precision: 0.703\n",
      "\n",
      "Accuracy: 0.011\n",
      "F1: 0.098\n",
      "Recall: 0.077\n",
      "Precision: 0.134\n",
      "Accuracy: 0.096\n",
      "F1: 0.559\n",
      "Recall: 0.663\n",
      "Precision: 0.483\n",
      "\n",
      "Accuracy: 0.083\n",
      "F1: 0.447\n",
      "Recall: 0.571\n",
      "Precision: 0.367\n",
      "Accuracy: 0.001\n",
      "F1: 0.013\n",
      "Recall: 0.007\n",
      "Precision: 0.074\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.020\n",
      "Recall: 0.015\n",
      "Precision: 0.029\n",
      "Accuracy: 0.096\n",
      "F1: 0.500\n",
      "Recall: 0.663\n",
      "Precision: 0.401\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.861\n",
      "Recall: 0.861\n",
      "Precision: 0.861\n",
      "Accuracy: 0.003\n",
      "F1: 0.015\n",
      "Recall: 0.022\n",
      "Precision: 0.011\n",
      "\n",
      "5 1e-05\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.055\n",
      "F1: 0.505\n",
      "Recall: 0.381\n",
      "Precision: 0.748\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.707\n",
      "Recall: 0.597\n",
      "Precision: 0.867\n",
      "Accuracy: 0.012\n",
      "F1: 0.074\n",
      "Recall: 0.084\n",
      "Precision: 0.066\n",
      "\n",
      "Accuracy: 0.055\n",
      "F1: 0.331\n",
      "Recall: 0.377\n",
      "Precision: 0.295\n",
      "Accuracy: 0.010\n",
      "F1: 0.086\n",
      "Recall: 0.070\n",
      "Precision: 0.111\n",
      "\n",
      "Accuracy: 0.078\n",
      "F1: 0.646\n",
      "Recall: 0.538\n",
      "Precision: 0.808\n",
      "Accuracy: 0.008\n",
      "F1: 0.035\n",
      "Recall: 0.055\n",
      "Precision: 0.026\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.536\n",
      "Recall: 0.396\n",
      "Precision: 0.831\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.010\n",
      "F1: 0.105\n",
      "Recall: 0.066\n",
      "Precision: 0.261\n",
      "Accuracy: 0.048\n",
      "F1: 0.449\n",
      "Recall: 0.330\n",
      "Precision: 0.703\n",
      "\n",
      "Accuracy: 0.011\n",
      "F1: 0.098\n",
      "Recall: 0.077\n",
      "Precision: 0.134\n",
      "Accuracy: 0.096\n",
      "F1: 0.559\n",
      "Recall: 0.663\n",
      "Precision: 0.483\n",
      "\n",
      "Accuracy: 0.083\n",
      "F1: 0.447\n",
      "Recall: 0.571\n",
      "Precision: 0.367\n",
      "Accuracy: 0.001\n",
      "F1: 0.013\n",
      "Recall: 0.007\n",
      "Precision: 0.074\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.020\n",
      "Recall: 0.015\n",
      "Precision: 0.029\n",
      "Accuracy: 0.096\n",
      "F1: 0.500\n",
      "Recall: 0.663\n",
      "Precision: 0.401\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.861\n",
      "Recall: 0.861\n",
      "Precision: 0.861\n",
      "Accuracy: 0.003\n",
      "F1: 0.015\n",
      "Recall: 0.022\n",
      "Precision: 0.011\n",
      "\n",
      "5 1e-06\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.055\n",
      "F1: 0.505\n",
      "Recall: 0.381\n",
      "Precision: 0.748\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.707\n",
      "Recall: 0.597\n",
      "Precision: 0.867\n",
      "Accuracy: 0.012\n",
      "F1: 0.074\n",
      "Recall: 0.084\n",
      "Precision: 0.066\n",
      "\n",
      "Accuracy: 0.055\n",
      "F1: 0.331\n",
      "Recall: 0.377\n",
      "Precision: 0.295\n",
      "Accuracy: 0.010\n",
      "F1: 0.086\n",
      "Recall: 0.070\n",
      "Precision: 0.111\n",
      "\n",
      "Accuracy: 0.078\n",
      "F1: 0.646\n",
      "Recall: 0.538\n",
      "Precision: 0.808\n",
      "Accuracy: 0.008\n",
      "F1: 0.035\n",
      "Recall: 0.055\n",
      "Precision: 0.026\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.536\n",
      "Recall: 0.396\n",
      "Precision: 0.831\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.010\n",
      "F1: 0.105\n",
      "Recall: 0.066\n",
      "Precision: 0.261\n",
      "Accuracy: 0.048\n",
      "F1: 0.449\n",
      "Recall: 0.330\n",
      "Precision: 0.703\n",
      "\n",
      "Accuracy: 0.011\n",
      "F1: 0.098\n",
      "Recall: 0.077\n",
      "Precision: 0.134\n",
      "Accuracy: 0.096\n",
      "F1: 0.559\n",
      "Recall: 0.663\n",
      "Precision: 0.483\n",
      "\n",
      "Accuracy: 0.083\n",
      "F1: 0.447\n",
      "Recall: 0.571\n",
      "Precision: 0.367\n",
      "Accuracy: 0.001\n",
      "F1: 0.013\n",
      "Recall: 0.007\n",
      "Precision: 0.074\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.020\n",
      "Recall: 0.015\n",
      "Precision: 0.029\n",
      "Accuracy: 0.096\n",
      "F1: 0.500\n",
      "Recall: 0.663\n",
      "Precision: 0.401\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.861\n",
      "Recall: 0.861\n",
      "Precision: 0.861\n",
      "Accuracy: 0.003\n",
      "F1: 0.015\n",
      "Recall: 0.022\n",
      "Precision: 0.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 1e-07\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.055\n",
      "F1: 0.505\n",
      "Recall: 0.381\n",
      "Precision: 0.748\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.707\n",
      "Recall: 0.597\n",
      "Precision: 0.867\n",
      "Accuracy: 0.012\n",
      "F1: 0.074\n",
      "Recall: 0.084\n",
      "Precision: 0.066\n",
      "\n",
      "Accuracy: 0.055\n",
      "F1: 0.331\n",
      "Recall: 0.377\n",
      "Precision: 0.295\n",
      "Accuracy: 0.010\n",
      "F1: 0.086\n",
      "Recall: 0.070\n",
      "Precision: 0.111\n",
      "\n",
      "Accuracy: 0.078\n",
      "F1: 0.646\n",
      "Recall: 0.538\n",
      "Precision: 0.808\n",
      "Accuracy: 0.008\n",
      "F1: 0.035\n",
      "Recall: 0.055\n",
      "Precision: 0.026\n",
      "\n",
      "Accuracy: 0.057\n",
      "F1: 0.536\n",
      "Recall: 0.396\n",
      "Precision: 0.831\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.010\n",
      "F1: 0.105\n",
      "Recall: 0.066\n",
      "Precision: 0.261\n",
      "Accuracy: 0.048\n",
      "F1: 0.449\n",
      "Recall: 0.330\n",
      "Precision: 0.703\n",
      "\n",
      "Accuracy: 0.011\n",
      "F1: 0.098\n",
      "Recall: 0.077\n",
      "Precision: 0.134\n",
      "Accuracy: 0.096\n",
      "F1: 0.559\n",
      "Recall: 0.663\n",
      "Precision: 0.483\n",
      "\n",
      "Accuracy: 0.083\n",
      "F1: 0.447\n",
      "Recall: 0.571\n",
      "Precision: 0.367\n",
      "Accuracy: 0.001\n",
      "F1: 0.013\n",
      "Recall: 0.007\n",
      "Precision: 0.074\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.020\n",
      "Recall: 0.015\n",
      "Precision: 0.029\n",
      "Accuracy: 0.096\n",
      "F1: 0.500\n",
      "Recall: 0.663\n",
      "Precision: 0.401\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.861\n",
      "Recall: 0.861\n",
      "Precision: 0.861\n",
      "Accuracy: 0.003\n",
      "F1: 0.015\n",
      "Recall: 0.022\n",
      "Precision: 0.011\n",
      "\n",
      "10 0.0001\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=1 loss=5075.7890625\n",
      "iteration=2 loss=4248.07373046875\n",
      "iteration=3 loss=3585.47607421875\n",
      "iteration=4 loss=3159.949951171875\n",
      "iteration=5 loss=2928.616943359375\n",
      "iteration=6 loss=2825.76708984375\n",
      "iteration=7 loss=2785.769775390625\n",
      "iteration=8 loss=2709.9521484375\n",
      "iteration=9 loss=2605.576416015625\n",
      "iteration=9 loss=2605.576416015625\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=1 loss=5253.2060546875\n",
      "iteration=2 loss=3984.01513671875\n",
      "iteration=3 loss=3178.08642578125\n",
      "iteration=4 loss=2772.7314453125\n",
      "iteration=5 loss=2617.2158203125\n",
      "iteration=6 loss=2589.6875\n",
      "iteration=7 loss=2567.008544921875\n",
      "iteration=8 loss=2475.24365234375\n",
      "iteration=9 loss=2350.2607421875\n",
      "iteration=9 loss=2350.2607421875\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=1 loss=5487.10498046875\n",
      "iteration=2 loss=4454.2978515625\n",
      "iteration=3 loss=3678.1845703125\n",
      "iteration=4 loss=3190.572265625\n",
      "iteration=5 loss=2928.121337890625\n",
      "iteration=6 loss=2821.7470703125\n",
      "iteration=7 loss=2798.736083984375\n",
      "iteration=8 loss=2782.960205078125\n",
      "iteration=9 loss=2716.3466796875\n",
      "iteration=9 loss=2716.3466796875\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=1 loss=4093.1259765625\n",
      "iteration=2 loss=3619.044189453125\n",
      "iteration=3 loss=3228.59423828125\n",
      "iteration=4 loss=2967.9345703125\n",
      "iteration=5 loss=2817.573974609375\n",
      "iteration=6 loss=2739.201904296875\n",
      "iteration=7 loss=2691.9873046875\n",
      "iteration=8 loss=2640.67041015625\n",
      "iteration=9 loss=2563.614990234375\n",
      "iteration=9 loss=2563.614990234375\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=1 loss=5145.59619140625\n",
      "iteration=2 loss=3987.45849609375\n",
      "iteration=3 loss=3271.37890625\n",
      "iteration=4 loss=2926.72509765625\n",
      "iteration=5 loss=2820.909912109375\n",
      "iteration=6 loss=2841.716552734375\n",
      "iteration=7 loss=2879.850830078125\n",
      "iteration=8 loss=2844.715576171875\n",
      "iteration=9 loss=2705.5771484375\n",
      "iteration=9 loss=2705.5771484375\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=1 loss=5264.20263671875\n",
      "iteration=2 loss=4096.70263671875\n",
      "iteration=3 loss=3346.449951171875\n",
      "iteration=4 loss=2954.39404296875\n",
      "iteration=5 loss=2786.7685546875\n",
      "iteration=6 loss=2737.154296875\n",
      "iteration=7 loss=2724.7958984375\n",
      "iteration=8 loss=2692.083984375\n",
      "iteration=9 loss=2597.989501953125\n",
      "iteration=9 loss=2597.989501953125\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=1 loss=6149.13671875\n",
      "iteration=2 loss=4107.5908203125\n",
      "iteration=3 loss=3203.435546875\n",
      "iteration=4 loss=2905.21337890625\n",
      "iteration=5 loss=2914.696044921875\n",
      "iteration=6 loss=3050.10400390625\n",
      "iteration=7 loss=3110.96044921875\n",
      "iteration=8 loss=3030.776611328125\n",
      "iteration=9 loss=2854.2998046875\n",
      "iteration=9 loss=2854.2998046875\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=1 loss=6451.8125\n",
      "iteration=2 loss=3951.136962890625\n",
      "iteration=3 loss=3086.358642578125\n",
      "iteration=4 loss=2908.28271484375\n",
      "iteration=5 loss=3048.087890625\n",
      "iteration=6 loss=3281.242919921875\n",
      "iteration=7 loss=3359.569580078125\n",
      "iteration=8 loss=3190.38134765625\n",
      "iteration=9 loss=2907.2734375\n",
      "iteration=9 loss=2907.2734375\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=1 loss=7083.20703125\n",
      "iteration=2 loss=4493.58544921875\n",
      "iteration=3 loss=3472.33056640625\n",
      "iteration=4 loss=3128.27880859375\n",
      "iteration=5 loss=3096.952392578125\n",
      "iteration=6 loss=3189.439453125\n",
      "iteration=7 loss=3152.835693359375\n",
      "iteration=8 loss=3003.33984375\n",
      "iteration=9 loss=2784.266845703125\n",
      "iteration=9 loss=2784.266845703125\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=1 loss=5075.22265625\n",
      "iteration=2 loss=3962.605224609375\n",
      "iteration=3 loss=3221.7353515625\n",
      "iteration=4 loss=2794.06689453125\n",
      "iteration=5 loss=2569.48291015625\n",
      "iteration=6 loss=2469.23876953125\n",
      "iteration=7 loss=2431.632080078125\n",
      "iteration=8 loss=2403.12255859375\n",
      "iteration=9 loss=2337.748779296875\n",
      "iteration=9 loss=2337.748779296875\n",
      "Accuracy: 0.091\n",
      "F1: 0.710\n",
      "Recall: 0.626\n",
      "Precision: 0.818\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.004\n",
      "Precision: 0.016\n",
      "\n",
      "Accuracy: 0.091\n",
      "F1: 0.695\n",
      "Recall: 0.626\n",
      "Precision: 0.781\n",
      "Accuracy: 0.002\n",
      "F1: 0.010\n",
      "Recall: 0.011\n",
      "Precision: 0.009\n",
      "\n",
      "Accuracy: 0.084\n",
      "F1: 0.472\n",
      "Recall: 0.579\n",
      "Precision: 0.398\n",
      "Accuracy: 0.002\n",
      "F1: 0.016\n",
      "Recall: 0.011\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.695\n",
      "Recall: 0.608\n",
      "Precision: 0.810\n",
      "Accuracy: 0.005\n",
      "F1: 0.023\n",
      "Recall: 0.033\n",
      "Precision: 0.018\n",
      "\n",
      "Accuracy: 0.090\n",
      "F1: 0.700\n",
      "Recall: 0.623\n",
      "Precision: 0.798\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.098\n",
      "F1: 0.736\n",
      "Recall: 0.678\n",
      "Precision: 0.804\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.011\n",
      "Precision: 0.037\n",
      "\n",
      "Accuracy: 0.087\n",
      "F1: 0.600\n",
      "Recall: 0.601\n",
      "Precision: 0.599\n",
      "Accuracy: 0.008\n",
      "F1: 0.053\n",
      "Recall: 0.059\n",
      "Precision: 0.049\n",
      "\n",
      "Accuracy: 0.120\n",
      "F1: 0.581\n",
      "Recall: 0.832\n",
      "Precision: 0.446\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.095\n",
      "F1: 0.616\n",
      "Recall: 0.659\n",
      "Precision: 0.579\n",
      "Accuracy: 0.015\n",
      "F1: 0.099\n",
      "Recall: 0.106\n",
      "Precision: 0.092\n",
      "\n",
      "Accuracy: 0.131\n",
      "F1: 0.792\n",
      "Recall: 0.905\n",
      "Precision: 0.704\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "10 1e-05\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=1 loss=5075.7890625\n",
      "iteration=2 loss=4248.07373046875\n",
      "iteration=3 loss=3585.47607421875\n",
      "iteration=4 loss=3159.949951171875\n",
      "iteration=5 loss=2928.616943359375\n",
      "iteration=6 loss=2825.76708984375\n",
      "iteration=7 loss=2785.769775390625\n",
      "iteration=8 loss=2709.9521484375\n",
      "iteration=9 loss=2605.576416015625\n",
      "iteration=9 loss=2605.576416015625\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=1 loss=5253.2060546875\n",
      "iteration=2 loss=3984.01513671875\n",
      "iteration=3 loss=3178.08642578125\n",
      "iteration=4 loss=2772.7314453125\n",
      "iteration=5 loss=2617.2158203125\n",
      "iteration=6 loss=2589.6875\n",
      "iteration=7 loss=2567.008544921875\n",
      "iteration=8 loss=2475.24365234375\n",
      "iteration=9 loss=2350.2607421875\n",
      "iteration=9 loss=2350.2607421875\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=1 loss=5487.10498046875\n",
      "iteration=2 loss=4454.2978515625\n",
      "iteration=3 loss=3678.1845703125\n",
      "iteration=4 loss=3190.572265625\n",
      "iteration=5 loss=2928.121337890625\n",
      "iteration=6 loss=2821.7470703125\n",
      "iteration=7 loss=2798.736083984375\n",
      "iteration=8 loss=2782.960205078125\n",
      "iteration=9 loss=2716.3466796875\n",
      "iteration=9 loss=2716.3466796875\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=1 loss=4093.1259765625\n",
      "iteration=2 loss=3619.044189453125\n",
      "iteration=3 loss=3228.59423828125\n",
      "iteration=4 loss=2967.9345703125\n",
      "iteration=5 loss=2817.573974609375\n",
      "iteration=6 loss=2739.201904296875\n",
      "iteration=7 loss=2691.9873046875\n",
      "iteration=8 loss=2640.67041015625\n",
      "iteration=9 loss=2563.614990234375\n",
      "iteration=9 loss=2563.614990234375\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=1 loss=5145.59619140625\n",
      "iteration=2 loss=3987.45849609375\n",
      "iteration=3 loss=3271.37890625\n",
      "iteration=4 loss=2926.72509765625\n",
      "iteration=5 loss=2820.909912109375\n",
      "iteration=6 loss=2841.716552734375\n",
      "iteration=7 loss=2879.850830078125\n",
      "iteration=8 loss=2844.715576171875\n",
      "iteration=9 loss=2705.5771484375\n",
      "iteration=9 loss=2705.5771484375\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=1 loss=5264.20263671875\n",
      "iteration=2 loss=4096.70263671875\n",
      "iteration=3 loss=3346.449951171875\n",
      "iteration=4 loss=2954.39404296875\n",
      "iteration=5 loss=2786.7685546875\n",
      "iteration=6 loss=2737.154296875\n",
      "iteration=7 loss=2724.7958984375\n",
      "iteration=8 loss=2692.083984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=9 loss=2597.989501953125\n",
      "iteration=9 loss=2597.989501953125\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=1 loss=6149.13671875\n",
      "iteration=2 loss=4107.5908203125\n",
      "iteration=3 loss=3203.435546875\n",
      "iteration=4 loss=2905.21337890625\n",
      "iteration=5 loss=2914.696044921875\n",
      "iteration=6 loss=3050.10400390625\n",
      "iteration=7 loss=3110.96044921875\n",
      "iteration=8 loss=3030.776611328125\n",
      "iteration=9 loss=2854.2998046875\n",
      "iteration=9 loss=2854.2998046875\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=1 loss=6451.8125\n",
      "iteration=2 loss=3951.136962890625\n",
      "iteration=3 loss=3086.358642578125\n",
      "iteration=4 loss=2908.28271484375\n",
      "iteration=5 loss=3048.087890625\n",
      "iteration=6 loss=3281.242919921875\n",
      "iteration=7 loss=3359.569580078125\n",
      "iteration=8 loss=3190.38134765625\n",
      "iteration=9 loss=2907.2734375\n",
      "iteration=9 loss=2907.2734375\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=1 loss=7083.20703125\n",
      "iteration=2 loss=4493.58544921875\n",
      "iteration=3 loss=3472.33056640625\n",
      "iteration=4 loss=3128.27880859375\n",
      "iteration=5 loss=3096.952392578125\n",
      "iteration=6 loss=3189.439453125\n",
      "iteration=7 loss=3152.835693359375\n",
      "iteration=8 loss=3003.33984375\n",
      "iteration=9 loss=2784.266845703125\n",
      "iteration=9 loss=2784.266845703125\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=1 loss=5075.22265625\n",
      "iteration=2 loss=3962.605224609375\n",
      "iteration=3 loss=3221.7353515625\n",
      "iteration=4 loss=2794.06689453125\n",
      "iteration=5 loss=2569.48291015625\n",
      "iteration=6 loss=2469.23876953125\n",
      "iteration=7 loss=2431.632080078125\n",
      "iteration=8 loss=2403.12255859375\n",
      "iteration=9 loss=2337.748779296875\n",
      "iteration=9 loss=2337.748779296875\n",
      "Accuracy: 0.091\n",
      "F1: 0.710\n",
      "Recall: 0.626\n",
      "Precision: 0.818\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.004\n",
      "Precision: 0.016\n",
      "\n",
      "Accuracy: 0.091\n",
      "F1: 0.695\n",
      "Recall: 0.626\n",
      "Precision: 0.781\n",
      "Accuracy: 0.002\n",
      "F1: 0.010\n",
      "Recall: 0.011\n",
      "Precision: 0.009\n",
      "\n",
      "Accuracy: 0.084\n",
      "F1: 0.472\n",
      "Recall: 0.579\n",
      "Precision: 0.398\n",
      "Accuracy: 0.002\n",
      "F1: 0.016\n",
      "Recall: 0.011\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.695\n",
      "Recall: 0.608\n",
      "Precision: 0.810\n",
      "Accuracy: 0.005\n",
      "F1: 0.023\n",
      "Recall: 0.033\n",
      "Precision: 0.018\n",
      "\n",
      "Accuracy: 0.090\n",
      "F1: 0.700\n",
      "Recall: 0.623\n",
      "Precision: 0.798\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.098\n",
      "F1: 0.736\n",
      "Recall: 0.678\n",
      "Precision: 0.804\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.011\n",
      "Precision: 0.037\n",
      "\n",
      "Accuracy: 0.087\n",
      "F1: 0.600\n",
      "Recall: 0.601\n",
      "Precision: 0.599\n",
      "Accuracy: 0.008\n",
      "F1: 0.053\n",
      "Recall: 0.059\n",
      "Precision: 0.049\n",
      "\n",
      "Accuracy: 0.120\n",
      "F1: 0.581\n",
      "Recall: 0.832\n",
      "Precision: 0.446\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.095\n",
      "F1: 0.616\n",
      "Recall: 0.659\n",
      "Precision: 0.579\n",
      "Accuracy: 0.015\n",
      "F1: 0.099\n",
      "Recall: 0.106\n",
      "Precision: 0.092\n",
      "\n",
      "Accuracy: 0.131\n",
      "F1: 0.792\n",
      "Recall: 0.905\n",
      "Precision: 0.704\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "10 1e-06\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=1 loss=5075.7890625\n",
      "iteration=2 loss=4248.07373046875\n",
      "iteration=3 loss=3585.47607421875\n",
      "iteration=4 loss=3159.949951171875\n",
      "iteration=5 loss=2928.616943359375\n",
      "iteration=6 loss=2825.76708984375\n",
      "iteration=7 loss=2785.769775390625\n",
      "iteration=8 loss=2709.9521484375\n",
      "iteration=9 loss=2605.576416015625\n",
      "iteration=9 loss=2605.576416015625\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=1 loss=5253.2060546875\n",
      "iteration=2 loss=3984.01513671875\n",
      "iteration=3 loss=3178.08642578125\n",
      "iteration=4 loss=2772.7314453125\n",
      "iteration=5 loss=2617.2158203125\n",
      "iteration=6 loss=2589.6875\n",
      "iteration=7 loss=2567.008544921875\n",
      "iteration=8 loss=2475.24365234375\n",
      "iteration=9 loss=2350.2607421875\n",
      "iteration=9 loss=2350.2607421875\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=1 loss=5487.10498046875\n",
      "iteration=2 loss=4454.2978515625\n",
      "iteration=3 loss=3678.1845703125\n",
      "iteration=4 loss=3190.572265625\n",
      "iteration=5 loss=2928.121337890625\n",
      "iteration=6 loss=2821.7470703125\n",
      "iteration=7 loss=2798.736083984375\n",
      "iteration=8 loss=2782.960205078125\n",
      "iteration=9 loss=2716.3466796875\n",
      "iteration=9 loss=2716.3466796875\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=1 loss=4093.1259765625\n",
      "iteration=2 loss=3619.044189453125\n",
      "iteration=3 loss=3228.59423828125\n",
      "iteration=4 loss=2967.9345703125\n",
      "iteration=5 loss=2817.573974609375\n",
      "iteration=6 loss=2739.201904296875\n",
      "iteration=7 loss=2691.9873046875\n",
      "iteration=8 loss=2640.67041015625\n",
      "iteration=9 loss=2563.614990234375\n",
      "iteration=9 loss=2563.614990234375\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=1 loss=5145.59619140625\n",
      "iteration=2 loss=3987.45849609375\n",
      "iteration=3 loss=3271.37890625\n",
      "iteration=4 loss=2926.72509765625\n",
      "iteration=5 loss=2820.909912109375\n",
      "iteration=6 loss=2841.716552734375\n",
      "iteration=7 loss=2879.850830078125\n",
      "iteration=8 loss=2844.715576171875\n",
      "iteration=9 loss=2705.5771484375\n",
      "iteration=9 loss=2705.5771484375\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=1 loss=5264.20263671875\n",
      "iteration=2 loss=4096.70263671875\n",
      "iteration=3 loss=3346.449951171875\n",
      "iteration=4 loss=2954.39404296875\n",
      "iteration=5 loss=2786.7685546875\n",
      "iteration=6 loss=2737.154296875\n",
      "iteration=7 loss=2724.7958984375\n",
      "iteration=8 loss=2692.083984375\n",
      "iteration=9 loss=2597.989501953125\n",
      "iteration=9 loss=2597.989501953125\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=1 loss=6149.13671875\n",
      "iteration=2 loss=4107.5908203125\n",
      "iteration=3 loss=3203.435546875\n",
      "iteration=4 loss=2905.21337890625\n",
      "iteration=5 loss=2914.696044921875\n",
      "iteration=6 loss=3050.10400390625\n",
      "iteration=7 loss=3110.96044921875\n",
      "iteration=8 loss=3030.776611328125\n",
      "iteration=9 loss=2854.2998046875\n",
      "iteration=9 loss=2854.2998046875\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=1 loss=6451.8125\n",
      "iteration=2 loss=3951.136962890625\n",
      "iteration=3 loss=3086.358642578125\n",
      "iteration=4 loss=2908.28271484375\n",
      "iteration=5 loss=3048.087890625\n",
      "iteration=6 loss=3281.242919921875\n",
      "iteration=7 loss=3359.569580078125\n",
      "iteration=8 loss=3190.38134765625\n",
      "iteration=9 loss=2907.2734375\n",
      "iteration=9 loss=2907.2734375\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=1 loss=7083.20703125\n",
      "iteration=2 loss=4493.58544921875\n",
      "iteration=3 loss=3472.33056640625\n",
      "iteration=4 loss=3128.27880859375\n",
      "iteration=5 loss=3096.952392578125\n",
      "iteration=6 loss=3189.439453125\n",
      "iteration=7 loss=3152.835693359375\n",
      "iteration=8 loss=3003.33984375\n",
      "iteration=9 loss=2784.266845703125\n",
      "iteration=9 loss=2784.266845703125\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=1 loss=5075.22265625\n",
      "iteration=2 loss=3962.605224609375\n",
      "iteration=3 loss=3221.7353515625\n",
      "iteration=4 loss=2794.06689453125\n",
      "iteration=5 loss=2569.48291015625\n",
      "iteration=6 loss=2469.23876953125\n",
      "iteration=7 loss=2431.632080078125\n",
      "iteration=8 loss=2403.12255859375\n",
      "iteration=9 loss=2337.748779296875\n",
      "iteration=9 loss=2337.748779296875\n",
      "Accuracy: 0.091\n",
      "F1: 0.710\n",
      "Recall: 0.626\n",
      "Precision: 0.818\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.004\n",
      "Precision: 0.016\n",
      "\n",
      "Accuracy: 0.091\n",
      "F1: 0.695\n",
      "Recall: 0.626\n",
      "Precision: 0.781\n",
      "Accuracy: 0.002\n",
      "F1: 0.010\n",
      "Recall: 0.011\n",
      "Precision: 0.009\n",
      "\n",
      "Accuracy: 0.084\n",
      "F1: 0.472\n",
      "Recall: 0.579\n",
      "Precision: 0.398\n",
      "Accuracy: 0.002\n",
      "F1: 0.016\n",
      "Recall: 0.011\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.695\n",
      "Recall: 0.608\n",
      "Precision: 0.810\n",
      "Accuracy: 0.005\n",
      "F1: 0.023\n",
      "Recall: 0.033\n",
      "Precision: 0.018\n",
      "\n",
      "Accuracy: 0.090\n",
      "F1: 0.700\n",
      "Recall: 0.623\n",
      "Precision: 0.798\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.098\n",
      "F1: 0.736\n",
      "Recall: 0.678\n",
      "Precision: 0.804\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.011\n",
      "Precision: 0.037\n",
      "\n",
      "Accuracy: 0.087\n",
      "F1: 0.600\n",
      "Recall: 0.601\n",
      "Precision: 0.599\n",
      "Accuracy: 0.008\n",
      "F1: 0.053\n",
      "Recall: 0.059\n",
      "Precision: 0.049\n",
      "\n",
      "Accuracy: 0.120\n",
      "F1: 0.581\n",
      "Recall: 0.832\n",
      "Precision: 0.446\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.095\n",
      "F1: 0.616\n",
      "Recall: 0.659\n",
      "Precision: 0.579\n",
      "Accuracy: 0.015\n",
      "F1: 0.099\n",
      "Recall: 0.106\n",
      "Precision: 0.092\n",
      "\n",
      "Accuracy: 0.131\n",
      "F1: 0.792\n",
      "Recall: 0.905\n",
      "Precision: 0.704\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "10 1e-07\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=1 loss=5075.7890625\n",
      "iteration=2 loss=4248.07373046875\n",
      "iteration=3 loss=3585.47607421875\n",
      "iteration=4 loss=3159.949951171875\n",
      "iteration=5 loss=2928.616943359375\n",
      "iteration=6 loss=2825.76708984375\n",
      "iteration=7 loss=2785.769775390625\n",
      "iteration=8 loss=2709.9521484375\n",
      "iteration=9 loss=2605.576416015625\n",
      "iteration=9 loss=2605.576416015625\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=1 loss=5253.2060546875\n",
      "iteration=2 loss=3984.01513671875\n",
      "iteration=3 loss=3178.08642578125\n",
      "iteration=4 loss=2772.7314453125\n",
      "iteration=5 loss=2617.2158203125\n",
      "iteration=6 loss=2589.6875\n",
      "iteration=7 loss=2567.008544921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=8 loss=2475.24365234375\n",
      "iteration=9 loss=2350.2607421875\n",
      "iteration=9 loss=2350.2607421875\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=1 loss=5487.10498046875\n",
      "iteration=2 loss=4454.2978515625\n",
      "iteration=3 loss=3678.1845703125\n",
      "iteration=4 loss=3190.572265625\n",
      "iteration=5 loss=2928.121337890625\n",
      "iteration=6 loss=2821.7470703125\n",
      "iteration=7 loss=2798.736083984375\n",
      "iteration=8 loss=2782.960205078125\n",
      "iteration=9 loss=2716.3466796875\n",
      "iteration=9 loss=2716.3466796875\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=1 loss=4093.1259765625\n",
      "iteration=2 loss=3619.044189453125\n",
      "iteration=3 loss=3228.59423828125\n",
      "iteration=4 loss=2967.9345703125\n",
      "iteration=5 loss=2817.573974609375\n",
      "iteration=6 loss=2739.201904296875\n",
      "iteration=7 loss=2691.9873046875\n",
      "iteration=8 loss=2640.67041015625\n",
      "iteration=9 loss=2563.614990234375\n",
      "iteration=9 loss=2563.614990234375\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=1 loss=5145.59619140625\n",
      "iteration=2 loss=3987.45849609375\n",
      "iteration=3 loss=3271.37890625\n",
      "iteration=4 loss=2926.72509765625\n",
      "iteration=5 loss=2820.909912109375\n",
      "iteration=6 loss=2841.716552734375\n",
      "iteration=7 loss=2879.850830078125\n",
      "iteration=8 loss=2844.715576171875\n",
      "iteration=9 loss=2705.5771484375\n",
      "iteration=9 loss=2705.5771484375\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=1 loss=5264.20263671875\n",
      "iteration=2 loss=4096.70263671875\n",
      "iteration=3 loss=3346.449951171875\n",
      "iteration=4 loss=2954.39404296875\n",
      "iteration=5 loss=2786.7685546875\n",
      "iteration=6 loss=2737.154296875\n",
      "iteration=7 loss=2724.7958984375\n",
      "iteration=8 loss=2692.083984375\n",
      "iteration=9 loss=2597.989501953125\n",
      "iteration=9 loss=2597.989501953125\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=1 loss=6149.13671875\n",
      "iteration=2 loss=4107.5908203125\n",
      "iteration=3 loss=3203.435546875\n",
      "iteration=4 loss=2905.21337890625\n",
      "iteration=5 loss=2914.696044921875\n",
      "iteration=6 loss=3050.10400390625\n",
      "iteration=7 loss=3110.96044921875\n",
      "iteration=8 loss=3030.776611328125\n",
      "iteration=9 loss=2854.2998046875\n",
      "iteration=9 loss=2854.2998046875\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=1 loss=6451.8125\n",
      "iteration=2 loss=3951.136962890625\n",
      "iteration=3 loss=3086.358642578125\n",
      "iteration=4 loss=2908.28271484375\n",
      "iteration=5 loss=3048.087890625\n",
      "iteration=6 loss=3281.242919921875\n",
      "iteration=7 loss=3359.569580078125\n",
      "iteration=8 loss=3190.38134765625\n",
      "iteration=9 loss=2907.2734375\n",
      "iteration=9 loss=2907.2734375\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=1 loss=7083.20703125\n",
      "iteration=2 loss=4493.58544921875\n",
      "iteration=3 loss=3472.33056640625\n",
      "iteration=4 loss=3128.27880859375\n",
      "iteration=5 loss=3096.952392578125\n",
      "iteration=6 loss=3189.439453125\n",
      "iteration=7 loss=3152.835693359375\n",
      "iteration=8 loss=3003.33984375\n",
      "iteration=9 loss=2784.266845703125\n",
      "iteration=9 loss=2784.266845703125\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=1 loss=5075.22265625\n",
      "iteration=2 loss=3962.605224609375\n",
      "iteration=3 loss=3221.7353515625\n",
      "iteration=4 loss=2794.06689453125\n",
      "iteration=5 loss=2569.48291015625\n",
      "iteration=6 loss=2469.23876953125\n",
      "iteration=7 loss=2431.632080078125\n",
      "iteration=8 loss=2403.12255859375\n",
      "iteration=9 loss=2337.748779296875\n",
      "iteration=9 loss=2337.748779296875\n",
      "Accuracy: 0.091\n",
      "F1: 0.710\n",
      "Recall: 0.626\n",
      "Precision: 0.818\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.004\n",
      "Precision: 0.016\n",
      "\n",
      "Accuracy: 0.091\n",
      "F1: 0.695\n",
      "Recall: 0.626\n",
      "Precision: 0.781\n",
      "Accuracy: 0.002\n",
      "F1: 0.010\n",
      "Recall: 0.011\n",
      "Precision: 0.009\n",
      "\n",
      "Accuracy: 0.084\n",
      "F1: 0.472\n",
      "Recall: 0.579\n",
      "Precision: 0.398\n",
      "Accuracy: 0.002\n",
      "F1: 0.016\n",
      "Recall: 0.011\n",
      "Precision: 0.032\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.695\n",
      "Recall: 0.608\n",
      "Precision: 0.810\n",
      "Accuracy: 0.005\n",
      "F1: 0.023\n",
      "Recall: 0.033\n",
      "Precision: 0.018\n",
      "\n",
      "Accuracy: 0.090\n",
      "F1: 0.700\n",
      "Recall: 0.623\n",
      "Precision: 0.798\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.098\n",
      "F1: 0.736\n",
      "Recall: 0.678\n",
      "Precision: 0.804\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.011\n",
      "Precision: 0.037\n",
      "\n",
      "Accuracy: 0.087\n",
      "F1: 0.600\n",
      "Recall: 0.601\n",
      "Precision: 0.599\n",
      "Accuracy: 0.008\n",
      "F1: 0.053\n",
      "Recall: 0.059\n",
      "Precision: 0.049\n",
      "\n",
      "Accuracy: 0.120\n",
      "F1: 0.581\n",
      "Recall: 0.832\n",
      "Precision: 0.446\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.095\n",
      "F1: 0.616\n",
      "Recall: 0.659\n",
      "Precision: 0.579\n",
      "Accuracy: 0.015\n",
      "F1: 0.099\n",
      "Recall: 0.106\n",
      "Precision: 0.092\n",
      "\n",
      "Accuracy: 0.131\n",
      "F1: 0.792\n",
      "Recall: 0.905\n",
      "Precision: 0.704\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "25 0.0001\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=2 loss=4248.07373046875\n",
      "iteration=4 loss=3159.949951171875\n",
      "iteration=6 loss=2825.76708984375\n",
      "iteration=8 loss=2709.9521484375\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=12 loss=2278.9375\n",
      "iteration=14 loss=2053.9521484375\n",
      "iteration=16 loss=1874.3697509765625\n",
      "iteration=18 loss=1743.015380859375\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=22 loss=1569.89306640625\n",
      "iteration=24 loss=1514.773193359375\n",
      "iteration=24 loss=1514.773193359375\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=2 loss=3984.01513671875\n",
      "iteration=4 loss=2772.7314453125\n",
      "iteration=6 loss=2589.6875\n",
      "iteration=8 loss=2475.24365234375\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=12 loss=1891.5181884765625\n",
      "iteration=14 loss=1638.3114013671875\n",
      "iteration=16 loss=1474.6983642578125\n",
      "iteration=18 loss=1377.6141357421875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=22 loss=1243.11865234375\n",
      "iteration=24 loss=1164.747802734375\n",
      "iteration=24 loss=1164.747802734375\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=2 loss=4454.2978515625\n",
      "iteration=4 loss=3190.572265625\n",
      "iteration=6 loss=2821.7470703125\n",
      "iteration=8 loss=2782.960205078125\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=12 loss=2222.886474609375\n",
      "iteration=14 loss=1931.8404541015625\n",
      "iteration=16 loss=1748.189453125\n",
      "iteration=18 loss=1615.0277099609375\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=22 loss=1416.271484375\n",
      "iteration=24 loss=1364.3084716796875\n",
      "iteration=24 loss=1364.3084716796875\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=2 loss=3619.044189453125\n",
      "iteration=4 loss=2967.9345703125\n",
      "iteration=6 loss=2739.201904296875\n",
      "iteration=8 loss=2640.67041015625\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=12 loss=2200.281005859375\n",
      "iteration=14 loss=1968.9268798828125\n",
      "iteration=16 loss=1795.0462646484375\n",
      "iteration=18 loss=1663.4398193359375\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=22 loss=1491.810546875\n",
      "iteration=24 loss=1434.5902099609375\n",
      "iteration=24 loss=1434.5902099609375\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=2 loss=3987.45849609375\n",
      "iteration=4 loss=2926.72509765625\n",
      "iteration=6 loss=2841.716552734375\n",
      "iteration=8 loss=2844.715576171875\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=12 loss=2119.041748046875\n",
      "iteration=14 loss=1918.472412109375\n",
      "iteration=16 loss=1820.6737060546875\n",
      "iteration=18 loss=1731.5496826171875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=22 loss=1527.944091796875\n",
      "iteration=24 loss=1441.45166015625\n",
      "iteration=24 loss=1441.45166015625\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=2 loss=4096.70263671875\n",
      "iteration=4 loss=2954.39404296875\n",
      "iteration=6 loss=2737.154296875\n",
      "iteration=8 loss=2692.083984375\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=12 loss=2162.767333984375\n",
      "iteration=14 loss=1915.7628173828125\n",
      "iteration=16 loss=1758.9959716796875\n",
      "iteration=18 loss=1661.2874755859375\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=22 loss=1527.831787109375\n",
      "iteration=24 loss=1456.135009765625\n",
      "iteration=24 loss=1456.135009765625\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=2 loss=4107.5908203125\n",
      "iteration=4 loss=2905.21337890625\n",
      "iteration=6 loss=3050.10400390625\n",
      "iteration=8 loss=3030.776611328125\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=12 loss=2213.3349609375\n",
      "iteration=14 loss=1994.278076171875\n",
      "iteration=16 loss=1906.4403076171875\n",
      "iteration=18 loss=1848.3292236328125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=22 loss=1669.8643798828125\n",
      "iteration=24 loss=1579.254150390625\n",
      "iteration=24 loss=1579.254150390625\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=2 loss=3951.136962890625\n",
      "iteration=4 loss=2908.28271484375\n",
      "iteration=6 loss=3281.242919921875\n",
      "iteration=8 loss=3190.38134765625\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=12 loss=2100.10302734375\n",
      "iteration=14 loss=1882.32421875\n",
      "iteration=16 loss=1800.235107421875\n",
      "iteration=18 loss=1730.430419921875\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=22 loss=1520.0118408203125\n",
      "iteration=24 loss=1428.2437744140625\n",
      "iteration=24 loss=1428.2437744140625\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=2 loss=4493.58544921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4 loss=3128.27880859375\n",
      "iteration=6 loss=3189.439453125\n",
      "iteration=8 loss=3003.33984375\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=12 loss=2111.530517578125\n",
      "iteration=14 loss=1842.0155029296875\n",
      "iteration=16 loss=1696.781982421875\n",
      "iteration=18 loss=1602.4034423828125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=22 loss=1445.839111328125\n",
      "iteration=24 loss=1357.8831787109375\n",
      "iteration=24 loss=1357.8831787109375\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=2 loss=3962.605224609375\n",
      "iteration=4 loss=2794.06689453125\n",
      "iteration=6 loss=2469.23876953125\n",
      "iteration=8 loss=2403.12255859375\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=12 loss=1883.8006591796875\n",
      "iteration=14 loss=1589.754638671875\n",
      "iteration=16 loss=1397.10205078125\n",
      "iteration=18 loss=1269.0604248046875\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=22 loss=1100.8773193359375\n",
      "iteration=24 loss=1059.69580078125\n",
      "iteration=24 loss=1059.69580078125\n",
      "Accuracy: 0.112\n",
      "F1: 0.709\n",
      "Recall: 0.773\n",
      "Precision: 0.655\n",
      "Accuracy: 0.001\n",
      "F1: 0.013\n",
      "Recall: 0.007\n",
      "Precision: 0.047\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.062\n",
      "Accuracy: 0.046\n",
      "F1: 0.427\n",
      "Recall: 0.315\n",
      "Precision: 0.662\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.345\n",
      "Recall: 0.462\n",
      "Precision: 0.276\n",
      "Accuracy: 0.013\n",
      "F1: 0.146\n",
      "Recall: 0.088\n",
      "Precision: 0.429\n",
      "\n",
      "Accuracy: 0.096\n",
      "F1: 0.742\n",
      "Recall: 0.663\n",
      "Precision: 0.842\n",
      "Accuracy: 0.012\n",
      "F1: 0.065\n",
      "Recall: 0.084\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.632\n",
      "Recall: 0.593\n",
      "Precision: 0.675\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.035\n",
      "F1: 0.256\n",
      "Recall: 0.242\n",
      "Precision: 0.272\n",
      "Accuracy: 0.092\n",
      "F1: 0.674\n",
      "Recall: 0.634\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.038\n",
      "F1: 0.282\n",
      "Recall: 0.264\n",
      "Precision: 0.303\n",
      "Accuracy: 0.048\n",
      "F1: 0.421\n",
      "Recall: 0.330\n",
      "Precision: 0.581\n",
      "\n",
      "Accuracy: 0.066\n",
      "F1: 0.304\n",
      "Recall: 0.454\n",
      "Precision: 0.228\n",
      "Accuracy: 0.030\n",
      "F1: 0.332\n",
      "Recall: 0.209\n",
      "Precision: 0.814\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.102\n",
      "F1: 0.790\n",
      "Recall: 0.703\n",
      "Precision: 0.901\n",
      "\n",
      "Accuracy: 0.129\n",
      "F1: 0.714\n",
      "Recall: 0.890\n",
      "Precision: 0.596\n",
      "Accuracy: 0.008\n",
      "F1: 0.073\n",
      "Recall: 0.059\n",
      "Precision: 0.096\n",
      "\n",
      "25 1e-05\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=2 loss=4248.07373046875\n",
      "iteration=4 loss=3159.949951171875\n",
      "iteration=6 loss=2825.76708984375\n",
      "iteration=8 loss=2709.9521484375\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=12 loss=2278.9375\n",
      "iteration=14 loss=2053.9521484375\n",
      "iteration=16 loss=1874.3697509765625\n",
      "iteration=18 loss=1743.015380859375\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=22 loss=1569.89306640625\n",
      "iteration=24 loss=1514.773193359375\n",
      "iteration=24 loss=1514.773193359375\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=2 loss=3984.01513671875\n",
      "iteration=4 loss=2772.7314453125\n",
      "iteration=6 loss=2589.6875\n",
      "iteration=8 loss=2475.24365234375\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=12 loss=1891.5181884765625\n",
      "iteration=14 loss=1638.3114013671875\n",
      "iteration=16 loss=1474.6983642578125\n",
      "iteration=18 loss=1377.6141357421875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=22 loss=1243.11865234375\n",
      "iteration=24 loss=1164.747802734375\n",
      "iteration=24 loss=1164.747802734375\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=2 loss=4454.2978515625\n",
      "iteration=4 loss=3190.572265625\n",
      "iteration=6 loss=2821.7470703125\n",
      "iteration=8 loss=2782.960205078125\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=12 loss=2222.886474609375\n",
      "iteration=14 loss=1931.8404541015625\n",
      "iteration=16 loss=1748.189453125\n",
      "iteration=18 loss=1615.0277099609375\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=22 loss=1416.271484375\n",
      "iteration=24 loss=1364.3084716796875\n",
      "iteration=24 loss=1364.3084716796875\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=2 loss=3619.044189453125\n",
      "iteration=4 loss=2967.9345703125\n",
      "iteration=6 loss=2739.201904296875\n",
      "iteration=8 loss=2640.67041015625\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=12 loss=2200.281005859375\n",
      "iteration=14 loss=1968.9268798828125\n",
      "iteration=16 loss=1795.0462646484375\n",
      "iteration=18 loss=1663.4398193359375\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=22 loss=1491.810546875\n",
      "iteration=24 loss=1434.5902099609375\n",
      "iteration=24 loss=1434.5902099609375\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=2 loss=3987.45849609375\n",
      "iteration=4 loss=2926.72509765625\n",
      "iteration=6 loss=2841.716552734375\n",
      "iteration=8 loss=2844.715576171875\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=12 loss=2119.041748046875\n",
      "iteration=14 loss=1918.472412109375\n",
      "iteration=16 loss=1820.6737060546875\n",
      "iteration=18 loss=1731.5496826171875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=22 loss=1527.944091796875\n",
      "iteration=24 loss=1441.45166015625\n",
      "iteration=24 loss=1441.45166015625\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=2 loss=4096.70263671875\n",
      "iteration=4 loss=2954.39404296875\n",
      "iteration=6 loss=2737.154296875\n",
      "iteration=8 loss=2692.083984375\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=12 loss=2162.767333984375\n",
      "iteration=14 loss=1915.7628173828125\n",
      "iteration=16 loss=1758.9959716796875\n",
      "iteration=18 loss=1661.2874755859375\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=22 loss=1527.831787109375\n",
      "iteration=24 loss=1456.135009765625\n",
      "iteration=24 loss=1456.135009765625\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=2 loss=4107.5908203125\n",
      "iteration=4 loss=2905.21337890625\n",
      "iteration=6 loss=3050.10400390625\n",
      "iteration=8 loss=3030.776611328125\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=12 loss=2213.3349609375\n",
      "iteration=14 loss=1994.278076171875\n",
      "iteration=16 loss=1906.4403076171875\n",
      "iteration=18 loss=1848.3292236328125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=22 loss=1669.8643798828125\n",
      "iteration=24 loss=1579.254150390625\n",
      "iteration=24 loss=1579.254150390625\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=2 loss=3951.136962890625\n",
      "iteration=4 loss=2908.28271484375\n",
      "iteration=6 loss=3281.242919921875\n",
      "iteration=8 loss=3190.38134765625\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=12 loss=2100.10302734375\n",
      "iteration=14 loss=1882.32421875\n",
      "iteration=16 loss=1800.235107421875\n",
      "iteration=18 loss=1730.430419921875\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=22 loss=1520.0118408203125\n",
      "iteration=24 loss=1428.2437744140625\n",
      "iteration=24 loss=1428.2437744140625\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=2 loss=4493.58544921875\n",
      "iteration=4 loss=3128.27880859375\n",
      "iteration=6 loss=3189.439453125\n",
      "iteration=8 loss=3003.33984375\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=12 loss=2111.530517578125\n",
      "iteration=14 loss=1842.0155029296875\n",
      "iteration=16 loss=1696.781982421875\n",
      "iteration=18 loss=1602.4034423828125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=22 loss=1445.839111328125\n",
      "iteration=24 loss=1357.8831787109375\n",
      "iteration=24 loss=1357.8831787109375\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=2 loss=3962.605224609375\n",
      "iteration=4 loss=2794.06689453125\n",
      "iteration=6 loss=2469.23876953125\n",
      "iteration=8 loss=2403.12255859375\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=12 loss=1883.8006591796875\n",
      "iteration=14 loss=1589.754638671875\n",
      "iteration=16 loss=1397.10205078125\n",
      "iteration=18 loss=1269.0604248046875\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=22 loss=1100.8773193359375\n",
      "iteration=24 loss=1059.69580078125\n",
      "iteration=24 loss=1059.69580078125\n",
      "Accuracy: 0.112\n",
      "F1: 0.709\n",
      "Recall: 0.773\n",
      "Precision: 0.655\n",
      "Accuracy: 0.001\n",
      "F1: 0.013\n",
      "Recall: 0.007\n",
      "Precision: 0.047\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.062\n",
      "Accuracy: 0.046\n",
      "F1: 0.427\n",
      "Recall: 0.315\n",
      "Precision: 0.662\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.345\n",
      "Recall: 0.462\n",
      "Precision: 0.276\n",
      "Accuracy: 0.013\n",
      "F1: 0.146\n",
      "Recall: 0.088\n",
      "Precision: 0.429\n",
      "\n",
      "Accuracy: 0.096\n",
      "F1: 0.742\n",
      "Recall: 0.663\n",
      "Precision: 0.842\n",
      "Accuracy: 0.012\n",
      "F1: 0.065\n",
      "Recall: 0.084\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.632\n",
      "Recall: 0.593\n",
      "Precision: 0.675\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.035\n",
      "F1: 0.256\n",
      "Recall: 0.242\n",
      "Precision: 0.272\n",
      "Accuracy: 0.092\n",
      "F1: 0.674\n",
      "Recall: 0.634\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.038\n",
      "F1: 0.282\n",
      "Recall: 0.264\n",
      "Precision: 0.303\n",
      "Accuracy: 0.048\n",
      "F1: 0.421\n",
      "Recall: 0.330\n",
      "Precision: 0.581\n",
      "\n",
      "Accuracy: 0.066\n",
      "F1: 0.304\n",
      "Recall: 0.454\n",
      "Precision: 0.228\n",
      "Accuracy: 0.030\n",
      "F1: 0.332\n",
      "Recall: 0.209\n",
      "Precision: 0.814\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.102\n",
      "F1: 0.790\n",
      "Recall: 0.703\n",
      "Precision: 0.901\n",
      "\n",
      "Accuracy: 0.129\n",
      "F1: 0.714\n",
      "Recall: 0.890\n",
      "Precision: 0.596\n",
      "Accuracy: 0.008\n",
      "F1: 0.073\n",
      "Recall: 0.059\n",
      "Precision: 0.096\n",
      "\n",
      "25 1e-06\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=2 loss=4248.07373046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4 loss=3159.949951171875\n",
      "iteration=6 loss=2825.76708984375\n",
      "iteration=8 loss=2709.9521484375\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=12 loss=2278.9375\n",
      "iteration=14 loss=2053.9521484375\n",
      "iteration=16 loss=1874.3697509765625\n",
      "iteration=18 loss=1743.015380859375\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=22 loss=1569.89306640625\n",
      "iteration=24 loss=1514.773193359375\n",
      "iteration=24 loss=1514.773193359375\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=2 loss=3984.01513671875\n",
      "iteration=4 loss=2772.7314453125\n",
      "iteration=6 loss=2589.6875\n",
      "iteration=8 loss=2475.24365234375\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=12 loss=1891.5181884765625\n",
      "iteration=14 loss=1638.3114013671875\n",
      "iteration=16 loss=1474.6983642578125\n",
      "iteration=18 loss=1377.6141357421875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=22 loss=1243.11865234375\n",
      "iteration=24 loss=1164.747802734375\n",
      "iteration=24 loss=1164.747802734375\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=2 loss=4454.2978515625\n",
      "iteration=4 loss=3190.572265625\n",
      "iteration=6 loss=2821.7470703125\n",
      "iteration=8 loss=2782.960205078125\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=12 loss=2222.886474609375\n",
      "iteration=14 loss=1931.8404541015625\n",
      "iteration=16 loss=1748.189453125\n",
      "iteration=18 loss=1615.0277099609375\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=22 loss=1416.271484375\n",
      "iteration=24 loss=1364.3084716796875\n",
      "iteration=24 loss=1364.3084716796875\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=2 loss=3619.044189453125\n",
      "iteration=4 loss=2967.9345703125\n",
      "iteration=6 loss=2739.201904296875\n",
      "iteration=8 loss=2640.67041015625\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=12 loss=2200.281005859375\n",
      "iteration=14 loss=1968.9268798828125\n",
      "iteration=16 loss=1795.0462646484375\n",
      "iteration=18 loss=1663.4398193359375\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=22 loss=1491.810546875\n",
      "iteration=24 loss=1434.5902099609375\n",
      "iteration=24 loss=1434.5902099609375\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=2 loss=3987.45849609375\n",
      "iteration=4 loss=2926.72509765625\n",
      "iteration=6 loss=2841.716552734375\n",
      "iteration=8 loss=2844.715576171875\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=12 loss=2119.041748046875\n",
      "iteration=14 loss=1918.472412109375\n",
      "iteration=16 loss=1820.6737060546875\n",
      "iteration=18 loss=1731.5496826171875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=22 loss=1527.944091796875\n",
      "iteration=24 loss=1441.45166015625\n",
      "iteration=24 loss=1441.45166015625\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=2 loss=4096.70263671875\n",
      "iteration=4 loss=2954.39404296875\n",
      "iteration=6 loss=2737.154296875\n",
      "iteration=8 loss=2692.083984375\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=12 loss=2162.767333984375\n",
      "iteration=14 loss=1915.7628173828125\n",
      "iteration=16 loss=1758.9959716796875\n",
      "iteration=18 loss=1661.2874755859375\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=22 loss=1527.831787109375\n",
      "iteration=24 loss=1456.135009765625\n",
      "iteration=24 loss=1456.135009765625\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=2 loss=4107.5908203125\n",
      "iteration=4 loss=2905.21337890625\n",
      "iteration=6 loss=3050.10400390625\n",
      "iteration=8 loss=3030.776611328125\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=12 loss=2213.3349609375\n",
      "iteration=14 loss=1994.278076171875\n",
      "iteration=16 loss=1906.4403076171875\n",
      "iteration=18 loss=1848.3292236328125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=22 loss=1669.8643798828125\n",
      "iteration=24 loss=1579.254150390625\n",
      "iteration=24 loss=1579.254150390625\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=2 loss=3951.136962890625\n",
      "iteration=4 loss=2908.28271484375\n",
      "iteration=6 loss=3281.242919921875\n",
      "iteration=8 loss=3190.38134765625\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=12 loss=2100.10302734375\n",
      "iteration=14 loss=1882.32421875\n",
      "iteration=16 loss=1800.235107421875\n",
      "iteration=18 loss=1730.430419921875\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=22 loss=1520.0118408203125\n",
      "iteration=24 loss=1428.2437744140625\n",
      "iteration=24 loss=1428.2437744140625\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=2 loss=4493.58544921875\n",
      "iteration=4 loss=3128.27880859375\n",
      "iteration=6 loss=3189.439453125\n",
      "iteration=8 loss=3003.33984375\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=12 loss=2111.530517578125\n",
      "iteration=14 loss=1842.0155029296875\n",
      "iteration=16 loss=1696.781982421875\n",
      "iteration=18 loss=1602.4034423828125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=22 loss=1445.839111328125\n",
      "iteration=24 loss=1357.8831787109375\n",
      "iteration=24 loss=1357.8831787109375\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=2 loss=3962.605224609375\n",
      "iteration=4 loss=2794.06689453125\n",
      "iteration=6 loss=2469.23876953125\n",
      "iteration=8 loss=2403.12255859375\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=12 loss=1883.8006591796875\n",
      "iteration=14 loss=1589.754638671875\n",
      "iteration=16 loss=1397.10205078125\n",
      "iteration=18 loss=1269.0604248046875\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=22 loss=1100.8773193359375\n",
      "iteration=24 loss=1059.69580078125\n",
      "iteration=24 loss=1059.69580078125\n",
      "Accuracy: 0.112\n",
      "F1: 0.709\n",
      "Recall: 0.773\n",
      "Precision: 0.655\n",
      "Accuracy: 0.001\n",
      "F1: 0.013\n",
      "Recall: 0.007\n",
      "Precision: 0.047\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.062\n",
      "Accuracy: 0.046\n",
      "F1: 0.427\n",
      "Recall: 0.315\n",
      "Precision: 0.662\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.345\n",
      "Recall: 0.462\n",
      "Precision: 0.276\n",
      "Accuracy: 0.013\n",
      "F1: 0.146\n",
      "Recall: 0.088\n",
      "Precision: 0.429\n",
      "\n",
      "Accuracy: 0.096\n",
      "F1: 0.742\n",
      "Recall: 0.663\n",
      "Precision: 0.842\n",
      "Accuracy: 0.012\n",
      "F1: 0.065\n",
      "Recall: 0.084\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.632\n",
      "Recall: 0.593\n",
      "Precision: 0.675\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.035\n",
      "F1: 0.256\n",
      "Recall: 0.242\n",
      "Precision: 0.272\n",
      "Accuracy: 0.092\n",
      "F1: 0.674\n",
      "Recall: 0.634\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.038\n",
      "F1: 0.282\n",
      "Recall: 0.264\n",
      "Precision: 0.303\n",
      "Accuracy: 0.048\n",
      "F1: 0.421\n",
      "Recall: 0.330\n",
      "Precision: 0.581\n",
      "\n",
      "Accuracy: 0.066\n",
      "F1: 0.304\n",
      "Recall: 0.454\n",
      "Precision: 0.228\n",
      "Accuracy: 0.030\n",
      "F1: 0.332\n",
      "Recall: 0.209\n",
      "Precision: 0.814\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.102\n",
      "F1: 0.790\n",
      "Recall: 0.703\n",
      "Precision: 0.901\n",
      "\n",
      "Accuracy: 0.129\n",
      "F1: 0.714\n",
      "Recall: 0.890\n",
      "Precision: 0.596\n",
      "Accuracy: 0.008\n",
      "F1: 0.073\n",
      "Recall: 0.059\n",
      "Precision: 0.096\n",
      "\n",
      "25 1e-07\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=2 loss=4248.07373046875\n",
      "iteration=4 loss=3159.949951171875\n",
      "iteration=6 loss=2825.76708984375\n",
      "iteration=8 loss=2709.9521484375\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=12 loss=2278.9375\n",
      "iteration=14 loss=2053.9521484375\n",
      "iteration=16 loss=1874.3697509765625\n",
      "iteration=18 loss=1743.015380859375\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=22 loss=1569.89306640625\n",
      "iteration=24 loss=1514.773193359375\n",
      "iteration=24 loss=1514.773193359375\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=2 loss=3984.01513671875\n",
      "iteration=4 loss=2772.7314453125\n",
      "iteration=6 loss=2589.6875\n",
      "iteration=8 loss=2475.24365234375\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=12 loss=1891.5181884765625\n",
      "iteration=14 loss=1638.3114013671875\n",
      "iteration=16 loss=1474.6983642578125\n",
      "iteration=18 loss=1377.6141357421875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=22 loss=1243.11865234375\n",
      "iteration=24 loss=1164.747802734375\n",
      "iteration=24 loss=1164.747802734375\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=2 loss=4454.2978515625\n",
      "iteration=4 loss=3190.572265625\n",
      "iteration=6 loss=2821.7470703125\n",
      "iteration=8 loss=2782.960205078125\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=12 loss=2222.886474609375\n",
      "iteration=14 loss=1931.8404541015625\n",
      "iteration=16 loss=1748.189453125\n",
      "iteration=18 loss=1615.0277099609375\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=22 loss=1416.271484375\n",
      "iteration=24 loss=1364.3084716796875\n",
      "iteration=24 loss=1364.3084716796875\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=2 loss=3619.044189453125\n",
      "iteration=4 loss=2967.9345703125\n",
      "iteration=6 loss=2739.201904296875\n",
      "iteration=8 loss=2640.67041015625\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=12 loss=2200.281005859375\n",
      "iteration=14 loss=1968.9268798828125\n",
      "iteration=16 loss=1795.0462646484375\n",
      "iteration=18 loss=1663.4398193359375\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=22 loss=1491.810546875\n",
      "iteration=24 loss=1434.5902099609375\n",
      "iteration=24 loss=1434.5902099609375\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=2 loss=3987.45849609375\n",
      "iteration=4 loss=2926.72509765625\n",
      "iteration=6 loss=2841.716552734375\n",
      "iteration=8 loss=2844.715576171875\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=12 loss=2119.041748046875\n",
      "iteration=14 loss=1918.472412109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=16 loss=1820.6737060546875\n",
      "iteration=18 loss=1731.5496826171875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=22 loss=1527.944091796875\n",
      "iteration=24 loss=1441.45166015625\n",
      "iteration=24 loss=1441.45166015625\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=2 loss=4096.70263671875\n",
      "iteration=4 loss=2954.39404296875\n",
      "iteration=6 loss=2737.154296875\n",
      "iteration=8 loss=2692.083984375\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=12 loss=2162.767333984375\n",
      "iteration=14 loss=1915.7628173828125\n",
      "iteration=16 loss=1758.9959716796875\n",
      "iteration=18 loss=1661.2874755859375\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=22 loss=1527.831787109375\n",
      "iteration=24 loss=1456.135009765625\n",
      "iteration=24 loss=1456.135009765625\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=2 loss=4107.5908203125\n",
      "iteration=4 loss=2905.21337890625\n",
      "iteration=6 loss=3050.10400390625\n",
      "iteration=8 loss=3030.776611328125\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=12 loss=2213.3349609375\n",
      "iteration=14 loss=1994.278076171875\n",
      "iteration=16 loss=1906.4403076171875\n",
      "iteration=18 loss=1848.3292236328125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=22 loss=1669.8643798828125\n",
      "iteration=24 loss=1579.254150390625\n",
      "iteration=24 loss=1579.254150390625\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=2 loss=3951.136962890625\n",
      "iteration=4 loss=2908.28271484375\n",
      "iteration=6 loss=3281.242919921875\n",
      "iteration=8 loss=3190.38134765625\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=12 loss=2100.10302734375\n",
      "iteration=14 loss=1882.32421875\n",
      "iteration=16 loss=1800.235107421875\n",
      "iteration=18 loss=1730.430419921875\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=22 loss=1520.0118408203125\n",
      "iteration=24 loss=1428.2437744140625\n",
      "iteration=24 loss=1428.2437744140625\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=2 loss=4493.58544921875\n",
      "iteration=4 loss=3128.27880859375\n",
      "iteration=6 loss=3189.439453125\n",
      "iteration=8 loss=3003.33984375\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=12 loss=2111.530517578125\n",
      "iteration=14 loss=1842.0155029296875\n",
      "iteration=16 loss=1696.781982421875\n",
      "iteration=18 loss=1602.4034423828125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=22 loss=1445.839111328125\n",
      "iteration=24 loss=1357.8831787109375\n",
      "iteration=24 loss=1357.8831787109375\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=2 loss=3962.605224609375\n",
      "iteration=4 loss=2794.06689453125\n",
      "iteration=6 loss=2469.23876953125\n",
      "iteration=8 loss=2403.12255859375\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=12 loss=1883.8006591796875\n",
      "iteration=14 loss=1589.754638671875\n",
      "iteration=16 loss=1397.10205078125\n",
      "iteration=18 loss=1269.0604248046875\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=22 loss=1100.8773193359375\n",
      "iteration=24 loss=1059.69580078125\n",
      "iteration=24 loss=1059.69580078125\n",
      "Accuracy: 0.112\n",
      "F1: 0.709\n",
      "Recall: 0.773\n",
      "Precision: 0.655\n",
      "Accuracy: 0.001\n",
      "F1: 0.013\n",
      "Recall: 0.007\n",
      "Precision: 0.047\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.062\n",
      "Accuracy: 0.046\n",
      "F1: 0.427\n",
      "Recall: 0.315\n",
      "Precision: 0.662\n",
      "\n",
      "Accuracy: 0.067\n",
      "F1: 0.345\n",
      "Recall: 0.462\n",
      "Precision: 0.276\n",
      "Accuracy: 0.013\n",
      "F1: 0.146\n",
      "Recall: 0.088\n",
      "Precision: 0.429\n",
      "\n",
      "Accuracy: 0.096\n",
      "F1: 0.742\n",
      "Recall: 0.663\n",
      "Precision: 0.842\n",
      "Accuracy: 0.012\n",
      "F1: 0.065\n",
      "Recall: 0.084\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.086\n",
      "F1: 0.632\n",
      "Recall: 0.593\n",
      "Precision: 0.675\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.035\n",
      "F1: 0.256\n",
      "Recall: 0.242\n",
      "Precision: 0.272\n",
      "Accuracy: 0.092\n",
      "F1: 0.674\n",
      "Recall: 0.634\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.038\n",
      "F1: 0.282\n",
      "Recall: 0.264\n",
      "Precision: 0.303\n",
      "Accuracy: 0.048\n",
      "F1: 0.421\n",
      "Recall: 0.330\n",
      "Precision: 0.581\n",
      "\n",
      "Accuracy: 0.066\n",
      "F1: 0.304\n",
      "Recall: 0.454\n",
      "Precision: 0.228\n",
      "Accuracy: 0.030\n",
      "F1: 0.332\n",
      "Recall: 0.209\n",
      "Precision: 0.814\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.102\n",
      "F1: 0.790\n",
      "Recall: 0.703\n",
      "Precision: 0.901\n",
      "\n",
      "Accuracy: 0.129\n",
      "F1: 0.714\n",
      "Recall: 0.890\n",
      "Precision: 0.596\n",
      "Accuracy: 0.008\n",
      "F1: 0.073\n",
      "Recall: 0.059\n",
      "Precision: 0.096\n",
      "\n",
      "50 0.0001\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=5 loss=2928.616943359375\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=15 loss=1957.38134765625\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=25 loss=1484.6624755859375\n",
      "iteration=30 loss=1325.8538818359375\n",
      "iteration=35 loss=1202.9091796875\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=45 loss=1001.5836181640625\n",
      "iteration=49 loss=941.2645263671875\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=5 loss=2617.2158203125\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=15 loss=1546.4384765625\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=25 loss=1133.6246337890625\n",
      "iteration=30 loss=1002.1429443359375\n",
      "iteration=35 loss=898.266845703125\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=45 loss=759.4443969726562\n",
      "iteration=49 loss=719.7236938476562\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=5 loss=2928.121337890625\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=15 loss=1830.0472412109375\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=25 loss=1328.890380859375\n",
      "iteration=30 loss=1173.0308837890625\n",
      "iteration=35 loss=1057.8218994140625\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=45 loss=890.91845703125\n",
      "iteration=49 loss=840.9227905273438\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=5 loss=2817.573974609375\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=15 loss=1875.39990234375\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=25 loss=1398.288818359375\n",
      "iteration=30 loss=1236.588134765625\n",
      "iteration=35 loss=1109.8526611328125\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=45 loss=919.6339111328125\n",
      "iteration=49 loss=865.28369140625\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=5 loss=2820.909912109375\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=15 loss=1864.2725830078125\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=25 loss=1404.606689453125\n",
      "iteration=30 loss=1241.51708984375\n",
      "iteration=35 loss=1099.86083984375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=45 loss=901.4501953125\n",
      "iteration=49 loss=846.7899169921875\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=5 loss=2786.7685546875\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=15 loss=1827.42138671875\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=25 loss=1419.458251953125\n",
      "iteration=30 loss=1282.2803955078125\n",
      "iteration=35 loss=1167.8455810546875\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=45 loss=981.1607666015625\n",
      "iteration=49 loss=917.7510986328125\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=5 loss=2914.696044921875\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=15 loss=1941.13720703125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=25 loss=1541.0853271484375\n",
      "iteration=30 loss=1387.263916015625\n",
      "iteration=35 loss=1246.197265625\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=45 loss=1045.013671875\n",
      "iteration=49 loss=984.5531616210938\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=5 loss=3048.087890625\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=15 loss=1833.6861572265625\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=25 loss=1390.999267578125\n",
      "iteration=30 loss=1237.87841796875\n",
      "iteration=35 loss=1106.4827880859375\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=45 loss=929.9840087890625\n",
      "iteration=49 loss=880.483642578125\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=5 loss=3096.952392578125\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=15 loss=1759.2147216796875\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=25 loss=1314.61865234375\n",
      "iteration=30 loss=1152.52978515625\n",
      "iteration=35 loss=1030.5587158203125\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=45 loss=856.2857666015625\n",
      "iteration=49 loss=807.417236328125\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=5 loss=2569.48291015625\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=15 loss=1482.3905029296875\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=25 loss=1037.6781005859375\n",
      "iteration=30 loss=925.128173828125\n",
      "iteration=35 loss=856.0450439453125\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=45 loss=754.322021484375\n",
      "iteration=49 loss=722.9154052734375\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.071\n",
      "Accuracy: 0.015\n",
      "F1: 0.164\n",
      "Recall: 0.103\n",
      "Precision: 0.406\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.020\n",
      "Recall: 0.011\n",
      "Precision: 0.150\n",
      "Accuracy: 0.063\n",
      "F1: 0.539\n",
      "Recall: 0.432\n",
      "Precision: 0.715\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.105\n",
      "Accuracy: 0.018\n",
      "F1: 0.192\n",
      "Recall: 0.121\n",
      "Precision: 0.465\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.014\n",
      "F1: 0.167\n",
      "Recall: 0.095\n",
      "Precision: 0.684\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.008\n",
      "F1: 0.100\n",
      "Recall: 0.059\n",
      "Precision: 0.348\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.097\n",
      "Recall: 0.128\n",
      "Precision: 0.078\n",
      "Accuracy: 0.095\n",
      "F1: 0.637\n",
      "Recall: 0.659\n",
      "Precision: 0.616\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.021\n",
      "Recall: 0.011\n",
      "Precision: 0.188\n",
      "Accuracy: 0.048\n",
      "F1: 0.434\n",
      "Recall: 0.330\n",
      "Precision: 0.634\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.111\n",
      "Accuracy: 0.047\n",
      "F1: 0.454\n",
      "Recall: 0.322\n",
      "Precision: 0.765\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.083\n",
      "F1: 0.690\n",
      "Recall: 0.575\n",
      "Precision: 0.863\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.029\n",
      "F1: 0.304\n",
      "Recall: 0.201\n",
      "Precision: 0.618\n",
      "\n",
      "50 1e-05\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=5 loss=2928.616943359375\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=15 loss=1957.38134765625\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=25 loss=1484.6624755859375\n",
      "iteration=30 loss=1325.8538818359375\n",
      "iteration=35 loss=1202.9091796875\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=45 loss=1001.5836181640625\n",
      "iteration=49 loss=941.2645263671875\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=5 loss=2617.2158203125\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=15 loss=1546.4384765625\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=25 loss=1133.6246337890625\n",
      "iteration=30 loss=1002.1429443359375\n",
      "iteration=35 loss=898.266845703125\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=45 loss=759.4443969726562\n",
      "iteration=49 loss=719.7236938476562\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=5 loss=2928.121337890625\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=15 loss=1830.0472412109375\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=25 loss=1328.890380859375\n",
      "iteration=30 loss=1173.0308837890625\n",
      "iteration=35 loss=1057.8218994140625\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=45 loss=890.91845703125\n",
      "iteration=49 loss=840.9227905273438\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=5 loss=2817.573974609375\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=15 loss=1875.39990234375\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=25 loss=1398.288818359375\n",
      "iteration=30 loss=1236.588134765625\n",
      "iteration=35 loss=1109.8526611328125\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=45 loss=919.6339111328125\n",
      "iteration=49 loss=865.28369140625\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=5 loss=2820.909912109375\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=15 loss=1864.2725830078125\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=25 loss=1404.606689453125\n",
      "iteration=30 loss=1241.51708984375\n",
      "iteration=35 loss=1099.86083984375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=45 loss=901.4501953125\n",
      "iteration=49 loss=846.7899169921875\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=5 loss=2786.7685546875\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=15 loss=1827.42138671875\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=25 loss=1419.458251953125\n",
      "iteration=30 loss=1282.2803955078125\n",
      "iteration=35 loss=1167.8455810546875\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=45 loss=981.1607666015625\n",
      "iteration=49 loss=917.7510986328125\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=5 loss=2914.696044921875\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=15 loss=1941.13720703125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=25 loss=1541.0853271484375\n",
      "iteration=30 loss=1387.263916015625\n",
      "iteration=35 loss=1246.197265625\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=45 loss=1045.013671875\n",
      "iteration=49 loss=984.5531616210938\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=5 loss=3048.087890625\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=15 loss=1833.6861572265625\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=25 loss=1390.999267578125\n",
      "iteration=30 loss=1237.87841796875\n",
      "iteration=35 loss=1106.4827880859375\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=45 loss=929.9840087890625\n",
      "iteration=49 loss=880.483642578125\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=5 loss=3096.952392578125\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=15 loss=1759.2147216796875\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=25 loss=1314.61865234375\n",
      "iteration=30 loss=1152.52978515625\n",
      "iteration=35 loss=1030.5587158203125\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=45 loss=856.2857666015625\n",
      "iteration=49 loss=807.417236328125\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=5 loss=2569.48291015625\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=15 loss=1482.3905029296875\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=25 loss=1037.6781005859375\n",
      "iteration=30 loss=925.128173828125\n",
      "iteration=35 loss=856.0450439453125\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=45 loss=754.322021484375\n",
      "iteration=49 loss=722.9154052734375\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.071\n",
      "Accuracy: 0.015\n",
      "F1: 0.164\n",
      "Recall: 0.103\n",
      "Precision: 0.406\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.020\n",
      "Recall: 0.011\n",
      "Precision: 0.150\n",
      "Accuracy: 0.063\n",
      "F1: 0.539\n",
      "Recall: 0.432\n",
      "Precision: 0.715\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.105\n",
      "Accuracy: 0.018\n",
      "F1: 0.192\n",
      "Recall: 0.121\n",
      "Precision: 0.465\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.014\n",
      "F1: 0.167\n",
      "Recall: 0.095\n",
      "Precision: 0.684\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.067\n",
      "Accuracy: 0.008\n",
      "F1: 0.100\n",
      "Recall: 0.059\n",
      "Precision: 0.348\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.097\n",
      "Recall: 0.128\n",
      "Precision: 0.078\n",
      "Accuracy: 0.095\n",
      "F1: 0.637\n",
      "Recall: 0.659\n",
      "Precision: 0.616\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.021\n",
      "Recall: 0.011\n",
      "Precision: 0.188\n",
      "Accuracy: 0.048\n",
      "F1: 0.434\n",
      "Recall: 0.330\n",
      "Precision: 0.634\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.111\n",
      "Accuracy: 0.047\n",
      "F1: 0.454\n",
      "Recall: 0.322\n",
      "Precision: 0.765\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.083\n",
      "F1: 0.690\n",
      "Recall: 0.575\n",
      "Precision: 0.863\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.029\n",
      "F1: 0.304\n",
      "Recall: 0.201\n",
      "Precision: 0.618\n",
      "\n",
      "50 1e-06\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=5 loss=2928.616943359375\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=15 loss=1957.38134765625\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=25 loss=1484.6624755859375\n",
      "iteration=30 loss=1325.8538818359375\n",
      "iteration=35 loss=1202.9091796875\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=45 loss=1001.5836181640625\n",
      "iteration=49 loss=941.2645263671875\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=5 loss=2617.2158203125\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=15 loss=1546.4384765625\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=25 loss=1133.6246337890625\n",
      "iteration=30 loss=1002.1429443359375\n",
      "iteration=35 loss=898.266845703125\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=45 loss=759.4443969726562\n",
      "iteration=49 loss=719.7236938476562\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=5 loss=2928.121337890625\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=15 loss=1830.0472412109375\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=25 loss=1328.890380859375\n",
      "iteration=30 loss=1173.0308837890625\n",
      "iteration=35 loss=1057.8218994140625\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=45 loss=890.91845703125\n",
      "iteration=49 loss=840.9227905273438\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=5 loss=2817.573974609375\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=15 loss=1875.39990234375\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=25 loss=1398.288818359375\n",
      "iteration=30 loss=1236.588134765625\n",
      "iteration=35 loss=1109.8526611328125\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=45 loss=919.6339111328125\n",
      "iteration=49 loss=865.28369140625\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=5 loss=2820.909912109375\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=15 loss=1864.2725830078125\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=25 loss=1404.606689453125\n",
      "iteration=30 loss=1241.51708984375\n",
      "iteration=35 loss=1099.86083984375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=45 loss=901.4501953125\n",
      "iteration=49 loss=846.7899169921875\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=5 loss=2786.7685546875\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=15 loss=1827.42138671875\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=25 loss=1419.458251953125\n",
      "iteration=30 loss=1282.2803955078125\n",
      "iteration=35 loss=1167.8455810546875\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=45 loss=981.1607666015625\n",
      "iteration=49 loss=917.7510986328125\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=5 loss=2914.696044921875\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=15 loss=1941.13720703125\n",
      "iteration=20 loss=1768.67138671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=25 loss=1541.0853271484375\n",
      "iteration=30 loss=1387.263916015625\n",
      "iteration=35 loss=1246.197265625\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=45 loss=1045.013671875\n",
      "iteration=49 loss=984.5531616210938\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=5 loss=3048.087890625\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=15 loss=1833.6861572265625\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=25 loss=1390.999267578125\n",
      "iteration=30 loss=1237.87841796875\n",
      "iteration=35 loss=1106.4827880859375\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=45 loss=929.9840087890625\n",
      "iteration=49 loss=880.483642578125\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=5 loss=3096.952392578125\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=15 loss=1759.2147216796875\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=25 loss=1314.61865234375\n",
      "iteration=30 loss=1152.52978515625\n",
      "iteration=35 loss=1030.5587158203125\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=45 loss=856.2857666015625\n",
      "iteration=49 loss=807.417236328125\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=5 loss=2569.48291015625\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=15 loss=1482.3905029296875\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=25 loss=1037.6781005859375\n",
      "iteration=30 loss=925.128173828125\n",
      "iteration=35 loss=856.0450439453125\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=45 loss=754.322021484375\n",
      "iteration=49 loss=722.9154052734375\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.071\n",
      "Accuracy: 0.015\n",
      "F1: 0.164\n",
      "Recall: 0.103\n",
      "Precision: 0.406\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.020\n",
      "Recall: 0.011\n",
      "Precision: 0.150\n",
      "Accuracy: 0.063\n",
      "F1: 0.539\n",
      "Recall: 0.432\n",
      "Precision: 0.715\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.105\n",
      "Accuracy: 0.018\n",
      "F1: 0.192\n",
      "Recall: 0.121\n",
      "Precision: 0.465\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.014\n",
      "F1: 0.167\n",
      "Recall: 0.095\n",
      "Precision: 0.684\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.067\n",
      "Accuracy: 0.008\n",
      "F1: 0.100\n",
      "Recall: 0.059\n",
      "Precision: 0.348\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.097\n",
      "Recall: 0.128\n",
      "Precision: 0.078\n",
      "Accuracy: 0.095\n",
      "F1: 0.637\n",
      "Recall: 0.659\n",
      "Precision: 0.616\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.021\n",
      "Recall: 0.011\n",
      "Precision: 0.188\n",
      "Accuracy: 0.048\n",
      "F1: 0.434\n",
      "Recall: 0.330\n",
      "Precision: 0.634\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.111\n",
      "Accuracy: 0.047\n",
      "F1: 0.454\n",
      "Recall: 0.322\n",
      "Precision: 0.765\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.083\n",
      "F1: 0.690\n",
      "Recall: 0.575\n",
      "Precision: 0.863\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.029\n",
      "F1: 0.304\n",
      "Recall: 0.201\n",
      "Precision: 0.618\n",
      "\n",
      "50 1e-07\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=5 loss=2928.616943359375\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=15 loss=1957.38134765625\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=25 loss=1484.6624755859375\n",
      "iteration=30 loss=1325.8538818359375\n",
      "iteration=35 loss=1202.9091796875\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=45 loss=1001.5836181640625\n",
      "iteration=49 loss=941.2645263671875\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=5 loss=2617.2158203125\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=15 loss=1546.4384765625\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=25 loss=1133.6246337890625\n",
      "iteration=30 loss=1002.1429443359375\n",
      "iteration=35 loss=898.266845703125\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=45 loss=759.4443969726562\n",
      "iteration=49 loss=719.7236938476562\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=5 loss=2928.121337890625\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=15 loss=1830.0472412109375\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=25 loss=1328.890380859375\n",
      "iteration=30 loss=1173.0308837890625\n",
      "iteration=35 loss=1057.8218994140625\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=45 loss=890.91845703125\n",
      "iteration=49 loss=840.9227905273438\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=5 loss=2817.573974609375\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=15 loss=1875.39990234375\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=25 loss=1398.288818359375\n",
      "iteration=30 loss=1236.588134765625\n",
      "iteration=35 loss=1109.8526611328125\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=45 loss=919.6339111328125\n",
      "iteration=49 loss=865.28369140625\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=5 loss=2820.909912109375\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=15 loss=1864.2725830078125\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=25 loss=1404.606689453125\n",
      "iteration=30 loss=1241.51708984375\n",
      "iteration=35 loss=1099.86083984375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=45 loss=901.4501953125\n",
      "iteration=49 loss=846.7899169921875\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=5 loss=2786.7685546875\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=15 loss=1827.42138671875\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=25 loss=1419.458251953125\n",
      "iteration=30 loss=1282.2803955078125\n",
      "iteration=35 loss=1167.8455810546875\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=45 loss=981.1607666015625\n",
      "iteration=49 loss=917.7510986328125\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=5 loss=2914.696044921875\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=15 loss=1941.13720703125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=25 loss=1541.0853271484375\n",
      "iteration=30 loss=1387.263916015625\n",
      "iteration=35 loss=1246.197265625\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=45 loss=1045.013671875\n",
      "iteration=49 loss=984.5531616210938\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=5 loss=3048.087890625\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=15 loss=1833.6861572265625\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=25 loss=1390.999267578125\n",
      "iteration=30 loss=1237.87841796875\n",
      "iteration=35 loss=1106.4827880859375\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=45 loss=929.9840087890625\n",
      "iteration=49 loss=880.483642578125\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=5 loss=3096.952392578125\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=15 loss=1759.2147216796875\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=25 loss=1314.61865234375\n",
      "iteration=30 loss=1152.52978515625\n",
      "iteration=35 loss=1030.5587158203125\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=45 loss=856.2857666015625\n",
      "iteration=49 loss=807.417236328125\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=5 loss=2569.48291015625\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=15 loss=1482.3905029296875\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=25 loss=1037.6781005859375\n",
      "iteration=30 loss=925.128173828125\n",
      "iteration=35 loss=856.0450439453125\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=45 loss=754.322021484375\n",
      "iteration=49 loss=722.9154052734375\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.071\n",
      "Accuracy: 0.015\n",
      "F1: 0.164\n",
      "Recall: 0.103\n",
      "Precision: 0.406\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.020\n",
      "Recall: 0.011\n",
      "Precision: 0.150\n",
      "Accuracy: 0.063\n",
      "F1: 0.539\n",
      "Recall: 0.432\n",
      "Precision: 0.715\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.105\n",
      "Accuracy: 0.018\n",
      "F1: 0.192\n",
      "Recall: 0.121\n",
      "Precision: 0.465\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.014\n",
      "F1: 0.167\n",
      "Recall: 0.095\n",
      "Precision: 0.684\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.007\n",
      "Recall: 0.004\n",
      "Precision: 0.067\n",
      "Accuracy: 0.008\n",
      "F1: 0.100\n",
      "Recall: 0.059\n",
      "Precision: 0.348\n",
      "\n",
      "Accuracy: 0.019\n",
      "F1: 0.097\n",
      "Recall: 0.128\n",
      "Precision: 0.078\n",
      "Accuracy: 0.095\n",
      "F1: 0.637\n",
      "Recall: 0.659\n",
      "Precision: 0.616\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.021\n",
      "Recall: 0.011\n",
      "Precision: 0.188\n",
      "Accuracy: 0.048\n",
      "F1: 0.434\n",
      "Recall: 0.330\n",
      "Precision: 0.634\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.014\n",
      "Recall: 0.007\n",
      "Precision: 0.111\n",
      "Accuracy: 0.047\n",
      "F1: 0.454\n",
      "Recall: 0.322\n",
      "Precision: 0.765\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.083\n",
      "F1: 0.690\n",
      "Recall: 0.575\n",
      "Precision: 0.863\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.029\n",
      "F1: 0.304\n",
      "Recall: 0.201\n",
      "Precision: 0.618\n",
      "\n",
      "100 0.0001\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=30 loss=1325.8538818359375\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=50 loss=927.9925537109375\n",
      "iteration=60 loss=818.8817138671875\n",
      "iteration=70 loss=742.384521484375\n",
      "iteration=80 loss=682.3972778320312\n",
      "iteration=90 loss=633.2258911132812\n",
      "iteration=99 loss=597.0362548828125\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=30 loss=1002.1429443359375\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=50 loss=710.8357543945312\n",
      "iteration=60 loss=638.4151000976562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=70 loss=584.7223510742188\n",
      "iteration=80 loss=543.3680419921875\n",
      "iteration=90 loss=514.9365234375\n",
      "iteration=99 loss=493.5545349121094\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=30 loss=1173.0308837890625\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=50 loss=829.5867919921875\n",
      "iteration=60 loss=735.806396484375\n",
      "iteration=70 loss=666.4803466796875\n",
      "iteration=80 loss=614.7596435546875\n",
      "iteration=90 loss=573.1862182617188\n",
      "iteration=99 loss=542.1043090820312\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=30 loss=1236.588134765625\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=50 loss=853.099853515625\n",
      "iteration=60 loss=753.7174072265625\n",
      "iteration=70 loss=682.725830078125\n",
      "iteration=80 loss=633.201416015625\n",
      "iteration=90 loss=595.4388427734375\n",
      "iteration=99 loss=566.69189453125\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=30 loss=1241.51708984375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=50 loss=834.6402587890625\n",
      "iteration=60 loss=737.2369384765625\n",
      "iteration=70 loss=670.2481689453125\n",
      "iteration=80 loss=623.0763549804688\n",
      "iteration=90 loss=585.9044189453125\n",
      "iteration=99 loss=558.6183471679688\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=30 loss=1282.2803955078125\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=50 loss=902.7169189453125\n",
      "iteration=60 loss=772.7919921875\n",
      "iteration=70 loss=678.1187744140625\n",
      "iteration=80 loss=608.4813232421875\n",
      "iteration=90 loss=556.1527099609375\n",
      "iteration=99 loss=519.7158813476562\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=30 loss=1387.263916015625\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=50 loss=970.8511962890625\n",
      "iteration=60 loss=857.174560546875\n",
      "iteration=70 loss=774.1044921875\n",
      "iteration=80 loss=712.970703125\n",
      "iteration=90 loss=663.9308471679688\n",
      "iteration=99 loss=627.3656005859375\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=30 loss=1237.87841796875\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=50 loss=869.3905029296875\n",
      "iteration=60 loss=778.2174682617188\n",
      "iteration=70 loss=715.2422485351562\n",
      "iteration=80 loss=668.2877807617188\n",
      "iteration=90 loss=629.900146484375\n",
      "iteration=99 loss=600.486572265625\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=30 loss=1152.52978515625\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=50 loss=796.5899658203125\n",
      "iteration=60 loss=708.612548828125\n",
      "iteration=70 loss=644.5256958007812\n",
      "iteration=80 loss=594.8436889648438\n",
      "iteration=90 loss=556.1832885742188\n",
      "iteration=99 loss=527.6089477539062\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=30 loss=925.128173828125\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=50 loss=715.582275390625\n",
      "iteration=60 loss=653.4389038085938\n",
      "iteration=70 loss=605.4336547851562\n",
      "iteration=80 loss=567.042236328125\n",
      "iteration=90 loss=535.753173828125\n",
      "iteration=99 loss=512.1762084960938\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.054\n",
      "F1: 0.485\n",
      "Recall: 0.374\n",
      "Precision: 0.689\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.048\n",
      "F1: 0.455\n",
      "Recall: 0.330\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.050\n",
      "F1: 0.461\n",
      "Recall: 0.348\n",
      "Precision: 0.683\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.032\n",
      "F1: 0.324\n",
      "Recall: 0.220\n",
      "Precision: 0.619\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.019\n",
      "F1: 0.201\n",
      "Recall: 0.128\n",
      "Precision: 0.467\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.029\n",
      "F1: 0.325\n",
      "Recall: 0.201\n",
      "Precision: 0.846\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.472\n",
      "Recall: 0.366\n",
      "Precision: 0.662\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.056\n",
      "F1: 0.510\n",
      "Recall: 0.388\n",
      "Precision: 0.741\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.045\n",
      "F1: 0.444\n",
      "Recall: 0.308\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.057\n",
      "F1: 0.521\n",
      "Recall: 0.392\n",
      "Precision: 0.775\n",
      "\n",
      "100 1e-05\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=30 loss=1325.8538818359375\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=50 loss=927.9925537109375\n",
      "iteration=60 loss=818.8817138671875\n",
      "iteration=70 loss=742.384521484375\n",
      "iteration=80 loss=682.3972778320312\n",
      "iteration=90 loss=633.2258911132812\n",
      "iteration=99 loss=597.0362548828125\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=30 loss=1002.1429443359375\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=50 loss=710.8357543945312\n",
      "iteration=60 loss=638.4151000976562\n",
      "iteration=70 loss=584.7223510742188\n",
      "iteration=80 loss=543.3680419921875\n",
      "iteration=90 loss=514.9365234375\n",
      "iteration=99 loss=493.5545349121094\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=30 loss=1173.0308837890625\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=50 loss=829.5867919921875\n",
      "iteration=60 loss=735.806396484375\n",
      "iteration=70 loss=666.4803466796875\n",
      "iteration=80 loss=614.7596435546875\n",
      "iteration=90 loss=573.1862182617188\n",
      "iteration=99 loss=542.1043090820312\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=30 loss=1236.588134765625\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=50 loss=853.099853515625\n",
      "iteration=60 loss=753.7174072265625\n",
      "iteration=70 loss=682.725830078125\n",
      "iteration=80 loss=633.201416015625\n",
      "iteration=90 loss=595.4388427734375\n",
      "iteration=99 loss=566.69189453125\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=30 loss=1241.51708984375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=50 loss=834.6402587890625\n",
      "iteration=60 loss=737.2369384765625\n",
      "iteration=70 loss=670.2481689453125\n",
      "iteration=80 loss=623.0763549804688\n",
      "iteration=90 loss=585.9044189453125\n",
      "iteration=99 loss=558.6183471679688\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=30 loss=1282.2803955078125\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=50 loss=902.7169189453125\n",
      "iteration=60 loss=772.7919921875\n",
      "iteration=70 loss=678.1187744140625\n",
      "iteration=80 loss=608.4813232421875\n",
      "iteration=90 loss=556.1527099609375\n",
      "iteration=99 loss=519.7158813476562\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=30 loss=1387.263916015625\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=50 loss=970.8511962890625\n",
      "iteration=60 loss=857.174560546875\n",
      "iteration=70 loss=774.1044921875\n",
      "iteration=80 loss=712.970703125\n",
      "iteration=90 loss=663.9308471679688\n",
      "iteration=99 loss=627.3656005859375\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=30 loss=1237.87841796875\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=50 loss=869.3905029296875\n",
      "iteration=60 loss=778.2174682617188\n",
      "iteration=70 loss=715.2422485351562\n",
      "iteration=80 loss=668.2877807617188\n",
      "iteration=90 loss=629.900146484375\n",
      "iteration=99 loss=600.486572265625\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=30 loss=1152.52978515625\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=50 loss=796.5899658203125\n",
      "iteration=60 loss=708.612548828125\n",
      "iteration=70 loss=644.5256958007812\n",
      "iteration=80 loss=594.8436889648438\n",
      "iteration=90 loss=556.1832885742188\n",
      "iteration=99 loss=527.6089477539062\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=30 loss=925.128173828125\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=50 loss=715.582275390625\n",
      "iteration=60 loss=653.4389038085938\n",
      "iteration=70 loss=605.4336547851562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=80 loss=567.042236328125\n",
      "iteration=90 loss=535.753173828125\n",
      "iteration=99 loss=512.1762084960938\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.054\n",
      "F1: 0.485\n",
      "Recall: 0.374\n",
      "Precision: 0.689\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.048\n",
      "F1: 0.455\n",
      "Recall: 0.330\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.050\n",
      "F1: 0.461\n",
      "Recall: 0.348\n",
      "Precision: 0.683\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.032\n",
      "F1: 0.324\n",
      "Recall: 0.220\n",
      "Precision: 0.619\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.019\n",
      "F1: 0.201\n",
      "Recall: 0.128\n",
      "Precision: 0.467\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.029\n",
      "F1: 0.325\n",
      "Recall: 0.201\n",
      "Precision: 0.846\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.472\n",
      "Recall: 0.366\n",
      "Precision: 0.662\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.056\n",
      "F1: 0.510\n",
      "Recall: 0.388\n",
      "Precision: 0.741\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.045\n",
      "F1: 0.444\n",
      "Recall: 0.308\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.057\n",
      "F1: 0.521\n",
      "Recall: 0.392\n",
      "Precision: 0.775\n",
      "\n",
      "100 1e-06\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=30 loss=1325.8538818359375\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=50 loss=927.9925537109375\n",
      "iteration=60 loss=818.8817138671875\n",
      "iteration=70 loss=742.384521484375\n",
      "iteration=80 loss=682.3972778320312\n",
      "iteration=90 loss=633.2258911132812\n",
      "iteration=99 loss=597.0362548828125\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=30 loss=1002.1429443359375\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=50 loss=710.8357543945312\n",
      "iteration=60 loss=638.4151000976562\n",
      "iteration=70 loss=584.7223510742188\n",
      "iteration=80 loss=543.3680419921875\n",
      "iteration=90 loss=514.9365234375\n",
      "iteration=99 loss=493.5545349121094\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=30 loss=1173.0308837890625\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=50 loss=829.5867919921875\n",
      "iteration=60 loss=735.806396484375\n",
      "iteration=70 loss=666.4803466796875\n",
      "iteration=80 loss=614.7596435546875\n",
      "iteration=90 loss=573.1862182617188\n",
      "iteration=99 loss=542.1043090820312\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=30 loss=1236.588134765625\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=50 loss=853.099853515625\n",
      "iteration=60 loss=753.7174072265625\n",
      "iteration=70 loss=682.725830078125\n",
      "iteration=80 loss=633.201416015625\n",
      "iteration=90 loss=595.4388427734375\n",
      "iteration=99 loss=566.69189453125\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=30 loss=1241.51708984375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=50 loss=834.6402587890625\n",
      "iteration=60 loss=737.2369384765625\n",
      "iteration=70 loss=670.2481689453125\n",
      "iteration=80 loss=623.0763549804688\n",
      "iteration=90 loss=585.9044189453125\n",
      "iteration=99 loss=558.6183471679688\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=30 loss=1282.2803955078125\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=50 loss=902.7169189453125\n",
      "iteration=60 loss=772.7919921875\n",
      "iteration=70 loss=678.1187744140625\n",
      "iteration=80 loss=608.4813232421875\n",
      "iteration=90 loss=556.1527099609375\n",
      "iteration=99 loss=519.7158813476562\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=30 loss=1387.263916015625\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=50 loss=970.8511962890625\n",
      "iteration=60 loss=857.174560546875\n",
      "iteration=70 loss=774.1044921875\n",
      "iteration=80 loss=712.970703125\n",
      "iteration=90 loss=663.9308471679688\n",
      "iteration=99 loss=627.3656005859375\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=30 loss=1237.87841796875\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=50 loss=869.3905029296875\n",
      "iteration=60 loss=778.2174682617188\n",
      "iteration=70 loss=715.2422485351562\n",
      "iteration=80 loss=668.2877807617188\n",
      "iteration=90 loss=629.900146484375\n",
      "iteration=99 loss=600.486572265625\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=30 loss=1152.52978515625\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=50 loss=796.5899658203125\n",
      "iteration=60 loss=708.612548828125\n",
      "iteration=70 loss=644.5256958007812\n",
      "iteration=80 loss=594.8436889648438\n",
      "iteration=90 loss=556.1832885742188\n",
      "iteration=99 loss=527.6089477539062\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=30 loss=925.128173828125\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=50 loss=715.582275390625\n",
      "iteration=60 loss=653.4389038085938\n",
      "iteration=70 loss=605.4336547851562\n",
      "iteration=80 loss=567.042236328125\n",
      "iteration=90 loss=535.753173828125\n",
      "iteration=99 loss=512.1762084960938\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.054\n",
      "F1: 0.485\n",
      "Recall: 0.374\n",
      "Precision: 0.689\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.048\n",
      "F1: 0.455\n",
      "Recall: 0.330\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.050\n",
      "F1: 0.461\n",
      "Recall: 0.348\n",
      "Precision: 0.683\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.032\n",
      "F1: 0.324\n",
      "Recall: 0.220\n",
      "Precision: 0.619\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.019\n",
      "F1: 0.201\n",
      "Recall: 0.128\n",
      "Precision: 0.467\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.029\n",
      "F1: 0.325\n",
      "Recall: 0.201\n",
      "Precision: 0.846\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.472\n",
      "Recall: 0.366\n",
      "Precision: 0.662\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.056\n",
      "F1: 0.510\n",
      "Recall: 0.388\n",
      "Precision: 0.741\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.045\n",
      "F1: 0.444\n",
      "Recall: 0.308\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.057\n",
      "F1: 0.521\n",
      "Recall: 0.392\n",
      "Precision: 0.775\n",
      "\n",
      "100 1e-07\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=10 loss=2499.630859375\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=30 loss=1325.8538818359375\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=50 loss=927.9925537109375\n",
      "iteration=60 loss=818.8817138671875\n",
      "iteration=70 loss=742.384521484375\n",
      "iteration=80 loss=682.3972778320312\n",
      "iteration=90 loss=633.2258911132812\n",
      "iteration=99 loss=597.0362548828125\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=10 loss=2202.6171875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=30 loss=1002.1429443359375\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=50 loss=710.8357543945312\n",
      "iteration=60 loss=638.4151000976562\n",
      "iteration=70 loss=584.7223510742188\n",
      "iteration=80 loss=543.3680419921875\n",
      "iteration=90 loss=514.9365234375\n",
      "iteration=99 loss=493.5545349121094\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=10 loss=2582.6650390625\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=30 loss=1173.0308837890625\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=50 loss=829.5867919921875\n",
      "iteration=60 loss=735.806396484375\n",
      "iteration=70 loss=666.4803466796875\n",
      "iteration=80 loss=614.7596435546875\n",
      "iteration=90 loss=573.1862182617188\n",
      "iteration=99 loss=542.1043090820312\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=10 loss=2456.96875\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=30 loss=1236.588134765625\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=50 loss=853.099853515625\n",
      "iteration=60 loss=753.7174072265625\n",
      "iteration=70 loss=682.725830078125\n",
      "iteration=80 loss=633.201416015625\n",
      "iteration=90 loss=595.4388427734375\n",
      "iteration=99 loss=566.69189453125\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=10 loss=2498.903076171875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=30 loss=1241.51708984375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=50 loss=834.6402587890625\n",
      "iteration=60 loss=737.2369384765625\n",
      "iteration=70 loss=670.2481689453125\n",
      "iteration=80 loss=623.0763549804688\n",
      "iteration=90 loss=585.9044189453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=99 loss=558.6183471679688\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=10 loss=2460.31884765625\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=30 loss=1282.2803955078125\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=50 loss=902.7169189453125\n",
      "iteration=60 loss=772.7919921875\n",
      "iteration=70 loss=678.1187744140625\n",
      "iteration=80 loss=608.4813232421875\n",
      "iteration=90 loss=556.1527099609375\n",
      "iteration=99 loss=519.7158813476562\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=10 loss=2626.576904296875\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=30 loss=1387.263916015625\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=50 loss=970.8511962890625\n",
      "iteration=60 loss=857.174560546875\n",
      "iteration=70 loss=774.1044921875\n",
      "iteration=80 loss=712.970703125\n",
      "iteration=90 loss=663.9308471679688\n",
      "iteration=99 loss=627.3656005859375\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=10 loss=2588.40283203125\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=30 loss=1237.87841796875\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=50 loss=869.3905029296875\n",
      "iteration=60 loss=778.2174682617188\n",
      "iteration=70 loss=715.2422485351562\n",
      "iteration=80 loss=668.2877807617188\n",
      "iteration=90 loss=629.900146484375\n",
      "iteration=99 loss=600.486572265625\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=10 loss=2540.114501953125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=30 loss=1152.52978515625\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=50 loss=796.5899658203125\n",
      "iteration=60 loss=708.612548828125\n",
      "iteration=70 loss=644.5256958007812\n",
      "iteration=80 loss=594.8436889648438\n",
      "iteration=90 loss=556.1832885742188\n",
      "iteration=99 loss=527.6089477539062\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=10 loss=2217.6494140625\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=30 loss=925.128173828125\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=50 loss=715.582275390625\n",
      "iteration=60 loss=653.4389038085938\n",
      "iteration=70 loss=605.4336547851562\n",
      "iteration=80 loss=567.042236328125\n",
      "iteration=90 loss=535.753173828125\n",
      "iteration=99 loss=512.1762084960938\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.054\n",
      "F1: 0.485\n",
      "Recall: 0.374\n",
      "Precision: 0.689\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.048\n",
      "F1: 0.455\n",
      "Recall: 0.330\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.050\n",
      "F1: 0.461\n",
      "Recall: 0.348\n",
      "Precision: 0.683\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.032\n",
      "F1: 0.324\n",
      "Recall: 0.220\n",
      "Precision: 0.619\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.019\n",
      "F1: 0.201\n",
      "Recall: 0.128\n",
      "Precision: 0.467\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.029\n",
      "F1: 0.325\n",
      "Recall: 0.201\n",
      "Precision: 0.846\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.472\n",
      "Recall: 0.366\n",
      "Precision: 0.662\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.056\n",
      "F1: 0.510\n",
      "Recall: 0.388\n",
      "Precision: 0.741\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.045\n",
      "F1: 0.444\n",
      "Recall: 0.308\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.057\n",
      "F1: 0.521\n",
      "Recall: 0.392\n",
      "Precision: 0.775\n",
      "\n",
      "200 0.0001\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=60 loss=818.8817138671875\n",
      "iteration=80 loss=682.3972778320312\n",
      "iteration=100 loss=593.3499145507812\n",
      "iteration=120 loss=530.0678100585938\n",
      "iteration=140 loss=482.8487548828125\n",
      "iteration=160 loss=447.22332763671875\n",
      "iteration=180 loss=419.9387512207031\n",
      "iteration=199 loss=399.61285400390625\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=60 loss=638.4151000976562\n",
      "iteration=80 loss=543.3680419921875\n",
      "iteration=100 loss=491.376953125\n",
      "iteration=120 loss=454.53289794921875\n",
      "iteration=140 loss=427.4257507324219\n",
      "iteration=160 loss=406.80682373046875\n",
      "iteration=180 loss=390.46124267578125\n",
      "iteration=199 loss=377.96722412109375\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=60 loss=735.806396484375\n",
      "iteration=80 loss=614.7596435546875\n",
      "iteration=100 loss=538.9586181640625\n",
      "iteration=120 loss=486.5690612792969\n",
      "iteration=140 loss=449.49371337890625\n",
      "iteration=160 loss=422.6506042480469\n",
      "iteration=180 loss=402.98199462890625\n",
      "iteration=199 loss=388.8955993652344\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=60 loss=753.7174072265625\n",
      "iteration=80 loss=633.201416015625\n",
      "iteration=100 loss=563.75830078125\n",
      "iteration=120 loss=514.02587890625\n",
      "iteration=140 loss=477.3843994140625\n",
      "iteration=160 loss=449.5940246582031\n",
      "iteration=180 loss=428.1976623535156\n",
      "iteration=199 loss=412.176513671875\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=60 loss=737.2369384765625\n",
      "iteration=80 loss=623.0763549804688\n",
      "iteration=100 loss=555.902099609375\n",
      "iteration=120 loss=510.733154296875\n",
      "iteration=140 loss=478.40142822265625\n",
      "iteration=160 loss=453.90093994140625\n",
      "iteration=180 loss=434.6214294433594\n",
      "iteration=199 loss=419.68499755859375\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=60 loss=772.7919921875\n",
      "iteration=80 loss=608.4813232421875\n",
      "iteration=100 loss=516.1558227539062\n",
      "iteration=120 loss=460.824951171875\n",
      "iteration=140 loss=429.1441650390625\n",
      "iteration=160 loss=405.0798645019531\n",
      "iteration=180 loss=386.188232421875\n",
      "iteration=199 loss=371.68280029296875\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=60 loss=857.174560546875\n",
      "iteration=80 loss=712.970703125\n",
      "iteration=100 loss=623.7107543945312\n",
      "iteration=120 loss=561.8369140625\n",
      "iteration=140 loss=515.4503784179688\n",
      "iteration=160 loss=479.36346435546875\n",
      "iteration=180 loss=450.7696533203125\n",
      "iteration=199 loss=428.9556579589844\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=60 loss=778.2174682617188\n",
      "iteration=80 loss=668.2877807617188\n",
      "iteration=100 loss=597.4677124023438\n",
      "iteration=120 loss=545.367431640625\n",
      "iteration=140 loss=505.5953369140625\n",
      "iteration=160 loss=473.9143371582031\n",
      "iteration=180 loss=447.72857666015625\n",
      "iteration=199 loss=426.8200988769531\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=60 loss=708.612548828125\n",
      "iteration=80 loss=594.8436889648438\n",
      "iteration=100 loss=524.70263671875\n",
      "iteration=120 loss=475.69744873046875\n",
      "iteration=140 loss=440.26666259765625\n",
      "iteration=160 loss=415.766357421875\n",
      "iteration=180 loss=396.949462890625\n",
      "iteration=199 loss=382.78045654296875\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=60 loss=653.4389038085938\n",
      "iteration=80 loss=567.042236328125\n",
      "iteration=100 loss=509.7613220214844\n",
      "iteration=120 loss=469.48785400390625\n",
      "iteration=140 loss=439.32049560546875\n",
      "iteration=160 loss=416.13641357421875\n",
      "iteration=180 loss=397.9179992675781\n",
      "iteration=199 loss=383.9586181640625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.702\n",
      "Recall: 0.630\n",
      "Precision: 0.793\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.485\n",
      "Recall: 0.363\n",
      "Precision: 0.733\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.488\n",
      "Recall: 0.363\n",
      "Precision: 0.744\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.698\n",
      "Recall: 0.626\n",
      "Precision: 0.788\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.067\n",
      "F1: 0.564\n",
      "Recall: 0.462\n",
      "Precision: 0.724\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.032\n",
      "F1: 0.349\n",
      "Recall: 0.220\n",
      "Precision: 0.845\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.074\n",
      "F1: 0.600\n",
      "Recall: 0.509\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.089\n",
      "F1: 0.703\n",
      "Recall: 0.615\n",
      "Precision: 0.820\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.045\n",
      "F1: 0.455\n",
      "Recall: 0.311\n",
      "Precision: 0.842\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.091\n",
      "F1: 0.713\n",
      "Recall: 0.626\n",
      "Precision: 0.826\n",
      "\n",
      "200 1e-05\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=60 loss=818.8817138671875\n",
      "iteration=80 loss=682.3972778320312\n",
      "iteration=100 loss=593.3499145507812\n",
      "iteration=120 loss=530.0678100585938\n",
      "iteration=140 loss=482.8487548828125\n",
      "iteration=160 loss=447.22332763671875\n",
      "iteration=180 loss=419.9387512207031\n",
      "iteration=199 loss=399.61285400390625\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=60 loss=638.4151000976562\n",
      "iteration=80 loss=543.3680419921875\n",
      "iteration=100 loss=491.376953125\n",
      "iteration=120 loss=454.53289794921875\n",
      "iteration=140 loss=427.4257507324219\n",
      "iteration=160 loss=406.80682373046875\n",
      "iteration=180 loss=390.46124267578125\n",
      "iteration=199 loss=377.96722412109375\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=60 loss=735.806396484375\n",
      "iteration=80 loss=614.7596435546875\n",
      "iteration=100 loss=538.9586181640625\n",
      "iteration=120 loss=486.5690612792969\n",
      "iteration=140 loss=449.49371337890625\n",
      "iteration=160 loss=422.6506042480469\n",
      "iteration=180 loss=402.98199462890625\n",
      "iteration=199 loss=388.8955993652344\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=60 loss=753.7174072265625\n",
      "iteration=80 loss=633.201416015625\n",
      "iteration=100 loss=563.75830078125\n",
      "iteration=120 loss=514.02587890625\n",
      "iteration=140 loss=477.3843994140625\n",
      "iteration=160 loss=449.5940246582031\n",
      "iteration=180 loss=428.1976623535156\n",
      "iteration=199 loss=412.176513671875\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=60 loss=737.2369384765625\n",
      "iteration=80 loss=623.0763549804688\n",
      "iteration=100 loss=555.902099609375\n",
      "iteration=120 loss=510.733154296875\n",
      "iteration=140 loss=478.40142822265625\n",
      "iteration=160 loss=453.90093994140625\n",
      "iteration=180 loss=434.6214294433594\n",
      "iteration=199 loss=419.68499755859375\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=60 loss=772.7919921875\n",
      "iteration=80 loss=608.4813232421875\n",
      "iteration=100 loss=516.1558227539062\n",
      "iteration=120 loss=460.824951171875\n",
      "iteration=140 loss=429.1441650390625\n",
      "iteration=160 loss=405.0798645019531\n",
      "iteration=180 loss=386.188232421875\n",
      "iteration=199 loss=371.68280029296875\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=60 loss=857.174560546875\n",
      "iteration=80 loss=712.970703125\n",
      "iteration=100 loss=623.7107543945312\n",
      "iteration=120 loss=561.8369140625\n",
      "iteration=140 loss=515.4503784179688\n",
      "iteration=160 loss=479.36346435546875\n",
      "iteration=180 loss=450.7696533203125\n",
      "iteration=199 loss=428.9556579589844\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=60 loss=778.2174682617188\n",
      "iteration=80 loss=668.2877807617188\n",
      "iteration=100 loss=597.4677124023438\n",
      "iteration=120 loss=545.367431640625\n",
      "iteration=140 loss=505.5953369140625\n",
      "iteration=160 loss=473.9143371582031\n",
      "iteration=180 loss=447.72857666015625\n",
      "iteration=199 loss=426.8200988769531\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=60 loss=708.612548828125\n",
      "iteration=80 loss=594.8436889648438\n",
      "iteration=100 loss=524.70263671875\n",
      "iteration=120 loss=475.69744873046875\n",
      "iteration=140 loss=440.26666259765625\n",
      "iteration=160 loss=415.766357421875\n",
      "iteration=180 loss=396.949462890625\n",
      "iteration=199 loss=382.78045654296875\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=60 loss=653.4389038085938\n",
      "iteration=80 loss=567.042236328125\n",
      "iteration=100 loss=509.7613220214844\n",
      "iteration=120 loss=469.48785400390625\n",
      "iteration=140 loss=439.32049560546875\n",
      "iteration=160 loss=416.13641357421875\n",
      "iteration=180 loss=397.9179992675781\n",
      "iteration=199 loss=383.9586181640625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.702\n",
      "Recall: 0.630\n",
      "Precision: 0.793\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.485\n",
      "Recall: 0.363\n",
      "Precision: 0.733\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.488\n",
      "Recall: 0.363\n",
      "Precision: 0.744\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.698\n",
      "Recall: 0.626\n",
      "Precision: 0.788\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.067\n",
      "F1: 0.564\n",
      "Recall: 0.462\n",
      "Precision: 0.724\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.032\n",
      "F1: 0.349\n",
      "Recall: 0.220\n",
      "Precision: 0.845\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.074\n",
      "F1: 0.600\n",
      "Recall: 0.509\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.089\n",
      "F1: 0.703\n",
      "Recall: 0.615\n",
      "Precision: 0.820\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.045\n",
      "F1: 0.455\n",
      "Recall: 0.311\n",
      "Precision: 0.842\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.713\n",
      "Recall: 0.626\n",
      "Precision: 0.826\n",
      "\n",
      "200 1e-06\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=60 loss=818.8817138671875\n",
      "iteration=80 loss=682.3972778320312\n",
      "iteration=100 loss=593.3499145507812\n",
      "iteration=120 loss=530.0678100585938\n",
      "iteration=140 loss=482.8487548828125\n",
      "iteration=160 loss=447.22332763671875\n",
      "iteration=180 loss=419.9387512207031\n",
      "iteration=199 loss=399.61285400390625\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=60 loss=638.4151000976562\n",
      "iteration=80 loss=543.3680419921875\n",
      "iteration=100 loss=491.376953125\n",
      "iteration=120 loss=454.53289794921875\n",
      "iteration=140 loss=427.4257507324219\n",
      "iteration=160 loss=406.80682373046875\n",
      "iteration=180 loss=390.46124267578125\n",
      "iteration=199 loss=377.96722412109375\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=60 loss=735.806396484375\n",
      "iteration=80 loss=614.7596435546875\n",
      "iteration=100 loss=538.9586181640625\n",
      "iteration=120 loss=486.5690612792969\n",
      "iteration=140 loss=449.49371337890625\n",
      "iteration=160 loss=422.6506042480469\n",
      "iteration=180 loss=402.98199462890625\n",
      "iteration=199 loss=388.8955993652344\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=60 loss=753.7174072265625\n",
      "iteration=80 loss=633.201416015625\n",
      "iteration=100 loss=563.75830078125\n",
      "iteration=120 loss=514.02587890625\n",
      "iteration=140 loss=477.3843994140625\n",
      "iteration=160 loss=449.5940246582031\n",
      "iteration=180 loss=428.1976623535156\n",
      "iteration=199 loss=412.176513671875\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=60 loss=737.2369384765625\n",
      "iteration=80 loss=623.0763549804688\n",
      "iteration=100 loss=555.902099609375\n",
      "iteration=120 loss=510.733154296875\n",
      "iteration=140 loss=478.40142822265625\n",
      "iteration=160 loss=453.90093994140625\n",
      "iteration=180 loss=434.6214294433594\n",
      "iteration=199 loss=419.68499755859375\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=60 loss=772.7919921875\n",
      "iteration=80 loss=608.4813232421875\n",
      "iteration=100 loss=516.1558227539062\n",
      "iteration=120 loss=460.824951171875\n",
      "iteration=140 loss=429.1441650390625\n",
      "iteration=160 loss=405.0798645019531\n",
      "iteration=180 loss=386.188232421875\n",
      "iteration=199 loss=371.68280029296875\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=60 loss=857.174560546875\n",
      "iteration=80 loss=712.970703125\n",
      "iteration=100 loss=623.7107543945312\n",
      "iteration=120 loss=561.8369140625\n",
      "iteration=140 loss=515.4503784179688\n",
      "iteration=160 loss=479.36346435546875\n",
      "iteration=180 loss=450.7696533203125\n",
      "iteration=199 loss=428.9556579589844\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=60 loss=778.2174682617188\n",
      "iteration=80 loss=668.2877807617188\n",
      "iteration=100 loss=597.4677124023438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=120 loss=545.367431640625\n",
      "iteration=140 loss=505.5953369140625\n",
      "iteration=160 loss=473.9143371582031\n",
      "iteration=180 loss=447.72857666015625\n",
      "iteration=199 loss=426.8200988769531\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=60 loss=708.612548828125\n",
      "iteration=80 loss=594.8436889648438\n",
      "iteration=100 loss=524.70263671875\n",
      "iteration=120 loss=475.69744873046875\n",
      "iteration=140 loss=440.26666259765625\n",
      "iteration=160 loss=415.766357421875\n",
      "iteration=180 loss=396.949462890625\n",
      "iteration=199 loss=382.78045654296875\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=60 loss=653.4389038085938\n",
      "iteration=80 loss=567.042236328125\n",
      "iteration=100 loss=509.7613220214844\n",
      "iteration=120 loss=469.48785400390625\n",
      "iteration=140 loss=439.32049560546875\n",
      "iteration=160 loss=416.13641357421875\n",
      "iteration=180 loss=397.9179992675781\n",
      "iteration=199 loss=383.9586181640625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.702\n",
      "Recall: 0.630\n",
      "Precision: 0.793\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.485\n",
      "Recall: 0.363\n",
      "Precision: 0.733\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.488\n",
      "Recall: 0.363\n",
      "Precision: 0.744\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.698\n",
      "Recall: 0.626\n",
      "Precision: 0.788\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.067\n",
      "F1: 0.564\n",
      "Recall: 0.462\n",
      "Precision: 0.724\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.032\n",
      "F1: 0.349\n",
      "Recall: 0.220\n",
      "Precision: 0.845\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.074\n",
      "F1: 0.600\n",
      "Recall: 0.509\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.089\n",
      "F1: 0.703\n",
      "Recall: 0.615\n",
      "Precision: 0.820\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.045\n",
      "F1: 0.455\n",
      "Recall: 0.311\n",
      "Precision: 0.842\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.713\n",
      "Recall: 0.626\n",
      "Precision: 0.826\n",
      "\n",
      "200 1e-07\n",
      "0\n",
      "iteration=0 loss=5742.75341796875\n",
      "iteration=20 loss=1643.98291015625\n",
      "iteration=40 loss=1092.6077880859375\n",
      "iteration=60 loss=818.8817138671875\n",
      "iteration=80 loss=682.3972778320312\n",
      "iteration=100 loss=593.3499145507812\n",
      "iteration=120 loss=530.0678100585938\n",
      "iteration=140 loss=482.8487548828125\n",
      "iteration=160 loss=447.22332763671875\n",
      "iteration=180 loss=419.9387512207031\n",
      "iteration=199 loss=399.61285400390625\n",
      "1\n",
      "iteration=0 loss=6498.72216796875\n",
      "iteration=20 loss=1318.623046875\n",
      "iteration=40 loss=820.349609375\n",
      "iteration=60 loss=638.4151000976562\n",
      "iteration=80 loss=543.3680419921875\n",
      "iteration=100 loss=491.376953125\n",
      "iteration=120 loss=454.53289794921875\n",
      "iteration=140 loss=427.4257507324219\n",
      "iteration=160 loss=406.80682373046875\n",
      "iteration=180 loss=390.46124267578125\n",
      "iteration=199 loss=377.96722412109375\n",
      "2\n",
      "iteration=0 loss=6387.94580078125\n",
      "iteration=20 loss=1501.7684326171875\n",
      "iteration=40 loss=964.8474731445312\n",
      "iteration=60 loss=735.806396484375\n",
      "iteration=80 loss=614.7596435546875\n",
      "iteration=100 loss=538.9586181640625\n",
      "iteration=120 loss=486.5690612792969\n",
      "iteration=140 loss=449.49371337890625\n",
      "iteration=160 loss=422.6506042480469\n",
      "iteration=180 loss=402.98199462890625\n",
      "iteration=199 loss=388.8955993652344\n",
      "3\n",
      "iteration=0 loss=4468.0078125\n",
      "iteration=20 loss=1562.515869140625\n",
      "iteration=40 loss=1002.7515869140625\n",
      "iteration=60 loss=753.7174072265625\n",
      "iteration=80 loss=633.201416015625\n",
      "iteration=100 loss=563.75830078125\n",
      "iteration=120 loss=514.02587890625\n",
      "iteration=140 loss=477.3843994140625\n",
      "iteration=160 loss=449.5940246582031\n",
      "iteration=180 loss=428.1976623535156\n",
      "iteration=199 loss=412.176513671875\n",
      "4\n",
      "iteration=0 loss=6314.5498046875\n",
      "iteration=20 loss=1629.310302734375\n",
      "iteration=40 loss=988.1215209960938\n",
      "iteration=60 loss=737.2369384765625\n",
      "iteration=80 loss=623.0763549804688\n",
      "iteration=100 loss=555.902099609375\n",
      "iteration=120 loss=510.733154296875\n",
      "iteration=140 loss=478.40142822265625\n",
      "iteration=160 loss=453.90093994140625\n",
      "iteration=180 loss=434.6214294433594\n",
      "iteration=199 loss=419.68499755859375\n",
      "5\n",
      "iteration=0 loss=6413.3173828125\n",
      "iteration=20 loss=1590.64208984375\n",
      "iteration=40 loss=1068.6170654296875\n",
      "iteration=60 loss=772.7919921875\n",
      "iteration=80 loss=608.4813232421875\n",
      "iteration=100 loss=516.1558227539062\n",
      "iteration=120 loss=460.824951171875\n",
      "iteration=140 loss=429.1441650390625\n",
      "iteration=160 loss=405.0798645019531\n",
      "iteration=180 loss=386.188232421875\n",
      "iteration=199 loss=371.68280029296875\n",
      "6\n",
      "iteration=0 loss=9038.86328125\n",
      "iteration=20 loss=1768.67138671875\n",
      "iteration=40 loss=1135.447265625\n",
      "iteration=60 loss=857.174560546875\n",
      "iteration=80 loss=712.970703125\n",
      "iteration=100 loss=623.7107543945312\n",
      "iteration=120 loss=561.8369140625\n",
      "iteration=140 loss=515.4503784179688\n",
      "iteration=160 loss=479.36346435546875\n",
      "iteration=180 loss=450.7696533203125\n",
      "iteration=199 loss=428.9556579589844\n",
      "7\n",
      "iteration=0 loss=11144.638671875\n",
      "iteration=20 loss=1631.2030029296875\n",
      "iteration=40 loss=1006.7494506835938\n",
      "iteration=60 loss=778.2174682617188\n",
      "iteration=80 loss=668.2877807617188\n",
      "iteration=100 loss=597.4677124023438\n",
      "iteration=120 loss=545.367431640625\n",
      "iteration=140 loss=505.5953369140625\n",
      "iteration=160 loss=473.9143371582031\n",
      "iteration=180 loss=447.72857666015625\n",
      "iteration=199 loss=426.8200988769531\n",
      "8\n",
      "iteration=0 loss=11542.0048828125\n",
      "iteration=20 loss=1523.418701171875\n",
      "iteration=40 loss=932.4149780273438\n",
      "iteration=60 loss=708.612548828125\n",
      "iteration=80 loss=594.8436889648438\n",
      "iteration=100 loss=524.70263671875\n",
      "iteration=120 loss=475.69744873046875\n",
      "iteration=140 loss=440.26666259765625\n",
      "iteration=160 loss=415.766357421875\n",
      "iteration=180 loss=396.949462890625\n",
      "iteration=199 loss=382.78045654296875\n",
      "9\n",
      "iteration=0 loss=6166.609375\n",
      "iteration=20 loss=1172.8343505859375\n",
      "iteration=40 loss=799.4739990234375\n",
      "iteration=60 loss=653.4389038085938\n",
      "iteration=80 loss=567.042236328125\n",
      "iteration=100 loss=509.7613220214844\n",
      "iteration=120 loss=469.48785400390625\n",
      "iteration=140 loss=439.32049560546875\n",
      "iteration=160 loss=416.13641357421875\n",
      "iteration=180 loss=397.9179992675781\n",
      "iteration=199 loss=383.9586181640625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.702\n",
      "Recall: 0.630\n",
      "Precision: 0.793\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.485\n",
      "Recall: 0.363\n",
      "Precision: 0.733\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.488\n",
      "Recall: 0.363\n",
      "Precision: 0.744\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.698\n",
      "Recall: 0.626\n",
      "Precision: 0.788\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.067\n",
      "F1: 0.564\n",
      "Recall: 0.462\n",
      "Precision: 0.724\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.032\n",
      "F1: 0.349\n",
      "Recall: 0.220\n",
      "Precision: 0.845\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.074\n",
      "F1: 0.600\n",
      "Recall: 0.509\n",
      "Precision: 0.732\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.089\n",
      "F1: 0.703\n",
      "Recall: 0.615\n",
      "Precision: 0.820\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.045\n",
      "F1: 0.455\n",
      "Recall: 0.311\n",
      "Precision: 0.842\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.091\n",
      "F1: 0.713\n",
      "Recall: 0.626\n",
      "Precision: 0.826\n",
      "\n",
      "[5, 9, DPLabelModel(), 0.1246684350132626, 0.8608058608058609, 0.8608058608058609, 0.8608058608058609, 0.003183023872679045, 0.014527845036319612, 0.02197802197802198, 0.0108499095840868]\n",
      "CPU times: user 2h 9min 53s, sys: 11min 52s, total: 2h 21min 45s\n",
      "Wall time: 2h 22min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best = None\n",
    "for iterations in [1, 5,10, 25, 50, 100, 200]:\n",
    "    for learning_rate in [1e-4, 1e-5, 1e-6, 1e-7]:\n",
    "        print(iterations, learning_rate)\n",
    "        max_seed = 10\n",
    "        temporal_models = [None,]*max_seed\n",
    "        for seed in range(max_seed):\n",
    "            print(seed)\n",
    "            markov_model = DPLabelModel(m=m_per_task*T, \n",
    "                                        T=T,\n",
    "                                        edges=[(i,i+m_per_task) for i in range((T-1)*m_per_task)],\n",
    "                                        coverage_sets=[[t,] for t in range(T) for _ in range(m_per_task)],\n",
    "                                        mu_sharing=[[t*m_per_task+i for t in range(T)] for i in range(m_per_task)],\n",
    "                                        phi_sharing=[[(t*m_per_task+i, (t+1)*m_per_task+i)\n",
    "                                                      for t in range(T-1)] for i in range(m_per_task)],\n",
    "                                        device=device,\n",
    "                                        class_balance=torch.tensor(class_balance).float().to(device),\n",
    "                                        seed=seed)\n",
    "            optimize(markov_model, L_hat=MRI_data_temporal['Li_train'], num_iter=iterations,\n",
    "                     lr=1e-5, momentum=0.8, clamp=True, \n",
    "                     verbose=iterations >= 10, seed=seed)\n",
    "            temporal_models[seed] = markov_model\n",
    "\n",
    "        for seed, model in enumerate(temporal_models):\n",
    "            Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            scores = [iterations, seed, model]\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "                \n",
    "            model.flip_params()\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "            \n",
    "            model.flip_params()\n",
    "\n",
    "            if best == None or scores[4] > max(best[4], best[8]) or scores[8] > max(best[4], best[8]):\n",
    "                best = scores\n",
    "            print()\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.125\n",
      "F1: 0.861\n",
      "Recall: 0.861\n",
      "Precision: 0.861\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.083\n",
      "F1: 0.690\n",
      "Recall: 0.575\n",
      "Precision: 0.863\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1436.,   92.,   49.,   14.,   21.,   19.,   29.,   23.,   65.,\n",
       "         137.]),\n",
       " array([7.98336298e-08, 9.99848064e-02, 1.99969533e-01, 2.99954259e-01,\n",
       "        3.99938986e-01, 4.99923713e-01, 5.99908439e-01, 6.99893166e-01,\n",
       "        7.99877892e-01, 8.99862619e-01, 9.99847345e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_frame_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, 'models/ts_labelmodel_best_tuning_downsampled.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best[2], 'models/ts_labelmodel_best_many_iterations.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_preds = best[2].predict_element_proba(Li_dev.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5450.,  253.,   82.,   44.,   41.,   22.,   15.,   33.,   52.,\n",
       "        1333.]),\n",
       " array([1.20012491e-05, 1.00009002e-01, 2.00006003e-01, 3.00003004e-01,\n",
       "        4.00000005e-01, 4.99997006e-01, 5.99994007e-01, 6.99991008e-01,\n",
       "        7.99988009e-01, 8.99985010e-01, 9.99982011e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(best_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.054\n",
      "Recall: 0.029\n",
      "Precision: 0.493\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.068\n",
      "Recall: 0.036\n",
      "Precision: 0.522\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed, model in enumerate(temporal_models):\n",
    "    model.flip_params()\n",
    "    Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "    R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "    scores = [iterations, seed, model]\n",
    "    for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "        score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "        print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    if best == None or scores[4] > best[4]:\n",
    "        best = scores\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500, 0, DPLabelModel(), 0.17597269624573378, 0.2994192799070848, 1.0, 0.17606884305422757]\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best, 'models/ts_labelmodel_best_2500.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_pred_frame_label = best[2].predict_element_proba(Li_dev.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 7.321e+03]),\n",
       " array([0.01257361, 0.11131625, 0.21005889, 0.30880153, 0.40754417,\n",
       "        0.50628681, 0.60502945, 0.70377209, 0.80251473, 0.90125737,\n",
       "        1.00000001]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_frame_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = markov_model.predict_element_proba(Li_dev.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7325,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.154\n",
      "F1: 0.377\n",
      "Recall: 0.876\n",
      "Precision: 0.241\n"
     ]
    }
   ],
   "source": [
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "        score = metric_score(Y_dev.cpu(), np.round(preds), metric)\n",
    "        print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99934805, 0.9738352 , 0.97474937, 0.93077044, 0.39215547,\n",
       "       0.06551916, 0.03445013, 0.03749803, 0.28004386, 0.77988954])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.516888  , 0.516888  , 0.516888  , 0.516888  , 0.48311198],\n",
       "       [0.53924423, 0.53924423, 0.53924423, 0.53924423, 0.46075577]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_probs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1638.,  276.,  236.,  135.,  346.,  731.,  854.,  962.,  534.,\n",
       "        1613.]),\n",
       " array([3.50714667e-04, 1.00315638e-01, 2.00280562e-01, 3.00245486e-01,\n",
       "        4.00210410e-01, 5.00175333e-01, 6.00140257e-01, 7.00105181e-01,\n",
       "        8.00070105e-01, 9.00035028e-01, 9.99999952e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.41327795e-05, 0.00000000e+00, 2.17160606e-03, ...,\n",
       "        4.27972118e-05, 0.00000000e+00, 3.89957458e-01],\n",
       "       [2.02080816e-01, 0.00000000e+00, 9.10843536e-03, ...,\n",
       "        7.04662083e-03, 0.00000000e+00, 5.14468737e-03],\n",
       "       [6.88242763e-02, 0.00000000e+00, 2.51607329e-01, ...,\n",
       "        7.77709298e-04, 0.00000000e+00, 4.60530259e-02],\n",
       "       ...,\n",
       "       [1.05558075e-02, 0.00000000e+00, 3.71003598e-02, ...,\n",
       "        7.99736604e-02, 0.00000000e+00, 2.26837486e-01],\n",
       "       [9.67053056e-05, 0.00000000e+00, 5.52507641e-04, ...,\n",
       "        4.20493714e-04, 0.00000000e+00, 2.81781822e-01],\n",
       "       [1.14595937e-03, 0.00000000e+00, 4.02768841e-03, ...,\n",
       "        7.53226504e-02, 0.00000000e+00, 2.13645339e-01]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(R_pred_frame.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1523.,  724., 1270.,  681.,  958.,  720.,  430.,  350.,  427.,\n",
       "         242.]),\n",
       " array([0.08463663, 0.17535003, 0.26606343, 0.35677683, 0.44749023,\n",
       "        0.53820363, 0.62891703, 0.71963043, 0.81034383, 0.90105723,\n",
       "        0.99177063]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_probs.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.3900],\n",
       "        [0.2021, 0.0000, 0.0091,  ..., 0.0070, 0.0000, 0.0051],\n",
       "        [0.0688, 0.0000, 0.2516,  ..., 0.0008, 0.0000, 0.0461],\n",
       "        ...,\n",
       "        [0.0106, 0.0000, 0.0371,  ..., 0.0800, 0.0000, 0.2268],\n",
       "        [0.0001, 0.0000, 0.0006,  ..., 0.0004, 0.0000, 0.2818],\n",
       "        [0.0011, 0.0000, 0.0040,  ..., 0.0753, 0.0000, 0.2136]],\n",
       "       device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0022, 0.0209, 0.0000, 0.0000, 0.0000, 0.0455, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0005, 0.0000, 0.0218, 0.5169, 0.0000, 0.0000,\n",
       "        0.0001, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0015,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.3900],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feasible_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -1, -1, -1, -1],\n",
       "        [-1,  1, -1, -1, -1],\n",
       "        [ 1, -1, -1, -1, -1],\n",
       "        [ 1,  1, -1, -1, -1],\n",
       "        [-1, -1,  1, -1, -1],\n",
       "        [-1,  1,  1, -1, -1],\n",
       "        [ 1, -1,  1, -1, -1],\n",
       "        [ 1,  1,  1, -1, -1],\n",
       "        [-1, -1, -1,  1, -1],\n",
       "        [-1,  1, -1,  1, -1],\n",
       "        [ 1, -1, -1,  1, -1],\n",
       "        [ 1,  1, -1,  1, -1],\n",
       "        [-1, -1,  1,  1, -1],\n",
       "        [-1,  1,  1,  1, -1],\n",
       "        [ 1, -1,  1,  1, -1],\n",
       "        [ 1,  1,  1,  1, -1],\n",
       "        [-1, -1, -1, -1,  1],\n",
       "        [-1,  1, -1, -1,  1],\n",
       "        [ 1, -1, -1, -1,  1],\n",
       "        [ 1,  1, -1, -1,  1],\n",
       "        [-1, -1,  1, -1,  1],\n",
       "        [-1,  1,  1, -1,  1],\n",
       "        [ 1, -1,  1, -1,  1],\n",
       "        [ 1,  1,  1, -1,  1],\n",
       "        [-1, -1, -1,  1,  1],\n",
       "        [-1,  1, -1,  1,  1],\n",
       "        [ 1, -1, -1,  1,  1],\n",
       "        [ 1,  1, -1,  1,  1],\n",
       "        [-1, -1,  1,  1,  1],\n",
       "        [-1,  1,  1,  1,  1],\n",
       "        [ 1, -1,  1,  1,  1],\n",
       "        [ 1,  1,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feasible_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1465, 32])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99934804, 0.9738352 , 0.97474945, 0.93077046, 0.39215547,\n",
       "       0.06551916, 0.03445013, 0.03749803, 0.28004387, 0.7798896 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(R_pred_frame, torch.where(\n",
    "    model.feasible_y==-1,torch.tensor(0).to(device),model.feasible_y).float()\n",
    ").detach().cpu().numpy().ravel()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = model.feasible_y.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[arr == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [1, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99934805, 0.9738352 , 0.97474937, ..., 0.58535333, 0.58560057,\n",
       "       0.89727236])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feasible_y_np = model.feasible_y.detach().cpu().numpy()\n",
    "feasible_y_np[feasible_y_np == -1] = 0\n",
    "np.matmul(\n",
    "    R_pred_frame.detach().cpu().numpy(),\n",
    "    feasible_y_np\n",
    ").ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0022, 0.0209, 0.0000, 0.0000, 0.0000, 0.0455, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0005, 0.0000, 0.0218, 0.5169, 0.0000, 0.0000,\n",
       "         0.0001, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0015,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.3900],\n",
       "        [0.2021, 0.0000, 0.0091, 0.0045, 0.0000, 0.0020, 0.0000, 0.0003, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0019, 0.0000, 0.0001, 0.0001, 0.4608, 0.0000,\n",
       "         0.0258, 0.0084, 0.0000, 0.0069, 0.0000, 0.0000, 0.2397, 0.0000, 0.0121,\n",
       "         0.0000, 0.0140, 0.0070, 0.0000, 0.0051]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -1, -1, -1, -1],\n",
       "        [-1,  1, -1, -1, -1],\n",
       "        [ 1, -1, -1, -1, -1],\n",
       "        [ 1,  1, -1, -1, -1],\n",
       "        [-1, -1,  1, -1, -1],\n",
       "        [-1,  1,  1, -1, -1],\n",
       "        [ 1, -1,  1, -1, -1],\n",
       "        [ 1,  1,  1, -1, -1],\n",
       "        [-1, -1, -1,  1, -1],\n",
       "        [-1,  1, -1,  1, -1],\n",
       "        [ 1, -1, -1,  1, -1],\n",
       "        [ 1,  1, -1,  1, -1],\n",
       "        [-1, -1,  1,  1, -1],\n",
       "        [-1,  1,  1,  1, -1],\n",
       "        [ 1, -1,  1,  1, -1],\n",
       "        [ 1,  1,  1,  1, -1],\n",
       "        [-1, -1, -1, -1,  1],\n",
       "        [-1,  1, -1, -1,  1],\n",
       "        [ 1, -1, -1, -1,  1],\n",
       "        [ 1,  1, -1, -1,  1],\n",
       "        [-1, -1,  1, -1,  1],\n",
       "        [-1,  1,  1, -1,  1],\n",
       "        [ 1, -1,  1, -1,  1],\n",
       "        [ 1,  1,  1, -1,  1],\n",
       "        [-1, -1, -1,  1,  1],\n",
       "        [-1,  1, -1,  1,  1],\n",
       "        [ 1, -1, -1,  1,  1],\n",
       "        [ 1,  1, -1,  1,  1],\n",
       "        [-1, -1,  1,  1,  1],\n",
       "        [-1,  1,  1,  1,  1],\n",
       "        [ 1, -1,  1,  1,  1],\n",
       "        [ 1,  1,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feasible_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.3900],\n",
       "        [0.2021, 0.0000, 0.0091,  ..., 0.0070, 0.0000, 0.0051],\n",
       "        [0.0688, 0.0000, 0.2516,  ..., 0.0008, 0.0000, 0.0461],\n",
       "        ...,\n",
       "        [0.0106, 0.0000, 0.0371,  ..., 0.0800, 0.0000, 0.2268],\n",
       "        [0.0001, 0.0000, 0.0006,  ..., 0.0004, 0.0000, 0.2818],\n",
       "        [0.0011, 0.0000, 0.0040,  ..., 0.0753, 0.0000, 0.2136]],\n",
       "       device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2021, 0.0688,  ..., 0.0106, 0.0001, 0.0011],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0022, 0.0091, 0.2516,  ..., 0.0371, 0.0006, 0.0040],\n",
       "        ...,\n",
       "        [0.0000, 0.0070, 0.0008,  ..., 0.0800, 0.0004, 0.0753],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3900, 0.0051, 0.0461,  ..., 0.2268, 0.2818, 0.2136]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9987,  0.9477,  0.9495,  0.8615, -0.2157],\n",
       "        [-0.8690, -0.9311, -0.9250, -0.4399,  0.5598],\n",
       "        [ 0.6989, -0.4822, -0.8752, -0.6246,  0.0817],\n",
       "        ...,\n",
       "        [ 0.5028,  0.2961,  0.2270,  0.2729,  0.1078],\n",
       "        [ 0.9919,  0.9809,  0.2497,  0.0447, -0.1122],\n",
       "        [ 0.2565,  0.3115,  0.1707,  0.1712,  0.7945]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(model.feasible_y.transpose(dim0=0, dim1=1).float(), R_pred_frame.transpose(0, 1)).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dot: Expected 1-D argument self, but got 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9896a5f7d120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeasible_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_pred_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: dot: Expected 1-D argument self, but got 2-D"
     ]
    }
   ],
   "source": [
    "torch.dot(model.feasible_y.T, R_pred_frame[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1465, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame[0].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.511\n",
      "F1: 0.304\n",
      "Recall: 0.605\n",
      "Precision: 0.203\n"
     ]
    }
   ],
   "source": [
    "Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "#find sequence label config. with highest prob.\n",
    "config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "R_pred_config = model.feasible_y[config_index]\n",
    "R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "#for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "for idx in range(R_pred_config.shape[0]):\n",
    "    R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "R_pred_probs = R_pred_probs.numpy()\n",
    "R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "\n",
    "Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "#find sequence label config. with highest prob.\n",
    "config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "R_pred_config = model.feasible_y[config_index]\n",
    "R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "#for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "for idx in range(R_pred_config.shape[0]):\n",
    "    R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "R_pred_probs = R_pred_probs.numpy()\n",
    "R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "R_pred_frame_label = np.round(R_pred_probs.ravel())\n",
    "R_pred_frame_label[R_pred_frame_label == 0.] = 2.\n",
    "\n",
    "scores = [iterations, seed, model]\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), R_pred_frame_label, metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.939\n",
      "F1: 0.827\n",
      "Recall: 0.836\n",
      "Precision: 0.819\n"
     ]
    }
   ],
   "source": [
    "model = best_model\n",
    "Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "#find sequence label config. with highest prob.\n",
    "config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "R_pred_config = model.feasible_y[config_index]\n",
    "R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "#for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "for idx in range(R_pred_config.shape[0]):\n",
    "    R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "R_pred_probs = R_pred_probs.numpy()\n",
    "R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "\n",
    "Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "#find sequence label config. with highest prob.\n",
    "config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "R_pred_config = model.feasible_y[config_index]\n",
    "R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "#for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "for idx in range(R_pred_config.shape[0]):\n",
    "    R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "R_pred_probs = R_pred_probs.numpy()\n",
    "R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "R_pred_frame_label = np.round(R_pred_probs.ravel())\n",
    "R_pred_frame_label[R_pred_frame_label == 0.] = 2.\n",
    "\n",
    "scores = [iterations]\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), R_pred_frame_label, metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/ts_labelmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel_best_many_iterations.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel_best_tuning.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel_best_tuning_downsampled.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.125\n",
      "F1: 0.861\n",
      "Recall: 0.861\n",
      "Precision: 0.861\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = model.eval().predict_element_proba(\n",
    "    MRI_data_temporal['Li_dev'].to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.143\n",
      "F1: 0.768\n",
      "Recall: 0.682\n",
      "Precision: 0.878\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = model.eval().predict_element_proba(\n",
    "    MRI_data_temporal['Li_test'].to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_test.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions for everything and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sparse\n",
    "import pickle\n",
    "import rekall\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "from rekall.interval_list import IntervalList\n",
    "from rekall.temporal_predicates import *\n",
    "from metal.label_model.baselines import MajorityLabelVoter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load manually annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 4686.00it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 44250.38it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/manually_annotated_shots.pkl', 'rb') as f:\n",
    "    shots = VideoIntervalCollection(pickle.load(f))\n",
    "with open('../../data/shot_detection_folds.pkl', 'rb') as f:\n",
    "    shot_detection_folds = pickle.load(f)\n",
    "clips = shots.dilate(1).coalesce().dilate(-1)\n",
    "shot_boundaries = shots.map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.start, intrvl.payload)\n",
    ").set_union(\n",
    "    shots.map(lambda intrvl: (intrvl.end + 1, intrvl.end + 1, intrvl.payload))\n",
    ").coalesce()\n",
    "boundary_frames = {\n",
    "    video_id: [\n",
    "        intrvl.start\n",
    "        for intrvl in shot_boundaries.get_intervallist(video_id).get_intervals()\n",
    "    ]\n",
    "    for video_id in shot_boundaries.get_allintervals()\n",
    "}\n",
    "video_ids = sorted(list(clips.get_allintervals().keys()))\n",
    "frames_per_video = {\n",
    "    video_id: sorted([\n",
    "        f\n",
    "        for interval in clips.get_intervallist(video_id).get_intervals()\n",
    "        for f in range(interval.start, interval.end + 2)\n",
    "    ])\n",
    "    for video_id in video_ids\n",
    "}\n",
    "ground_truth = {\n",
    "    video_id: [\n",
    "        1 if f in boundary_frames[video_id] else 2\n",
    "        for f in frames_per_video[video_id]\n",
    "    ] \n",
    "    for video_id in video_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load label matrix with all frames in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/all_labels.pkl', 'rb') as f:\n",
    "    weak_labels_all_movies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load videos and number of frames per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/frame_counts.pkl', 'rb') as f:\n",
    "    frame_counts = pickle.load(f)\n",
    "video_ids_all = sorted(list(frame_counts.keys()))\n",
    "video_ids_train = sorted(list(set(video_ids_all).difference(set(video_ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct windows for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, construct windows of 16 frames for each video\n",
    "windows = VideoIntervalCollection({\n",
    "    video_id: [\n",
    "        (f, f + 16, video_id)\n",
    "        for f in range(0, frame_counts[video_id] - 16, 16)\n",
    "    ]\n",
    "    for video_id in video_ids_all\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truth labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, intersect the windows with ground truth and get ground truth labels for the windows\n",
    "windows_intersecting_ground_truth = windows.filter_against(\n",
    "    clips,\n",
    "    predicate=overlaps()\n",
    ").map(lambda intrvl: (intrvl.start, intrvl.end, 2))\n",
    "windows_with_shot_boundaries = windows_intersecting_ground_truth.filter_against(\n",
    "    shot_boundaries,\n",
    "    predicate = lambda window, shot_boundary:\n",
    "        shot_boundary.start >= window.start and shot_boundary.start < window.end\n",
    ").map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.end, 1)\n",
    ")\n",
    "windows_with_labels = windows_with_shot_boundaries.set_union(\n",
    "    windows_intersecting_ground_truth\n",
    ").coalesce(\n",
    "    predicate = equal(),\n",
    "    payload_merge_op = lambda p1, p2: min(p1, p2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weak labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label windows with the weak labels in our labeling functions\n",
    "def label_window(per_frame_weak_labels):\n",
    "    if 1 in per_frame_weak_labels:\n",
    "        return 1\n",
    "    if len([l for l in per_frame_weak_labels if l == 2]) >= len(per_frame_weak_labels) / 2:\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "windows_with_weak_labels = windows.map(\n",
    "    lambda window: (\n",
    "        window.start,\n",
    "        window.end,\n",
    "        [\n",
    "            label_window([\n",
    "                lf[window.payload][f-1]\n",
    "                for f in range(window.start, window.end)\n",
    "            ])\n",
    "            for lf in weak_labels_all_movies\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_everything_windows = csr_matrix([\n",
    "    intrvl.payload\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows_downsampled.npy', 'wb') as f:\n",
    "    np.save(f, L_everything_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows_downsampled.npy', 'rb') as f:\n",
    "    L_everything_windows = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert L matrix to timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5\n",
    "m_per_task = L_everything_windows.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled = torch.FloatTensor(L_everything_windows[:L_everything_windows.shape[0] -\n",
    "                                                      (L_everything_windows.shape[0] % T)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_per_task_unlabelled = L_unlabelled.size(1)\n",
    "n_frames_unlabelled = L_unlabelled.size(0)\n",
    "n_patients_unlabelled = n_frames_unlabelled//T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled_ts = torch.LongTensor(\n",
    "    L_unlabelled.view(n_patients_unlabelled, (m_per_task*T)).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235081"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled_ts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val = model.eval().predict_element_proba(MRI_data_temporal['Li_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1436.,   92.,   49.,   14.,   21.,   19.,   29.,   23.,   65.,\n",
       "         137.]),\n",
       " array([7.98336298e-08, 9.99848064e-02, 1.99969533e-01, 2.99954259e-01,\n",
       "        3.99938986e-01, 4.99923713e-01, 5.99908439e-01, 6.99893166e-01,\n",
       "        7.99877892e-01, 8.99862619e-01, 9.99847345e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.960\n",
      "F1: 0.861\n",
      "Recall: 0.861\n",
      "Precision: 0.861\n"
     ]
    }
   ],
   "source": [
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu().where(Y_dev.cpu() == torch.tensor(1.), torch.tensor(0.)),\n",
    "                         np.round(predictions_val), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n"
     ]
    }
   ],
   "source": [
    "predictions_everything = []\n",
    "for i in range(0, L_unlabelled_ts.shape[0], 100000):\n",
    "    print(i)\n",
    "    start = i\n",
    "    end = i + 100000\n",
    "    labels = L_unlabelled_ts[start:end] if end < L_unlabelled_ts.shape[0] else L_unlabelled_ts[start:]\n",
    "    predictions_for_labels = model.eval().predict_element_proba(labels.to(device))\n",
    "    predictions_everything.append(predictions_for_labels)\n",
    "    del predictions_for_labels\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6175405,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(predictions_everything).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_pred_probs_per_frame = np.concatenate(predictions_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_pred_frame = predictions_everything_together\n",
    "\n",
    "# #find sequence label config. with highest prob.\n",
    "# config_index = np.argmax(R_pred_frame, axis=1)\n",
    "# R_pred_config = model.feasible_y[config_index].detach().cpu()\n",
    "# R_pred_max = torch.FloatTensor(np.max(R_pred_frame.numpy(), axis=1))\n",
    "\n",
    "# #for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "# R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "# for idx in range(R_pred_config.shape[0]):\n",
    "#     R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:]).float()*R_pred_max[idx]\n",
    "\n",
    "# R_pred_probs = R_pred_probs.numpy()\n",
    "# R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "# R_pred_frame_label = np.round(R_pred_probs.ravel())\n",
    "# R_pred_frame_label[R_pred_frame_label == 0.] = 2.\n",
    "\n",
    "# R_pred_probs_per_frame = R_pred_probs.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3974824.,  895914.,  365832.,  102541.,  102136.,  119002.,\n",
       "         114341.,  184300.,  126978.,  189537.]),\n",
       " array([5.18732044e-08, 9.99846214e-02, 1.99969191e-01, 2.99953760e-01,\n",
       "        3.99938330e-01, 4.99922899e-01, 5.99907469e-01, 6.99892038e-01,\n",
       "        7.99876608e-01, 8.99861177e-01, 9.99845747e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_probs_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "         0],\n",
       "        [1, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "         0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled_ts[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_nums = [\n",
    "    (video_id, intrvl.start, intrvl.end)\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows = [\n",
    "    (window_info, np.array([prediction, 1. - prediction]))\n",
    "    for window_info, prediction in zip(window_nums, R_pred_probs_per_frame)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we needed to cut the predictions to a multiple of T\n",
    "last_preds = []\n",
    "for window_info in window_nums[len(predictions_to_save_windows):]:\n",
    "    last_preds.append((window_info, np.array([0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows += last_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np_windows = np.array(predictions_to_save_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to disk\n",
    "with open('../../data/shot_detection_weak_labels/ts_weak_labels_all_windows_tuned_downsampled.npy', 'wb') as f:\n",
    "    np.save(f, preds_np_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution compared to Metal LabelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(1, 0, 16), array([0., 1.])],\n",
       "       [(1, 8, 24), array([0., 1.])],\n",
       "       [(1, 16, 32), array([0., 1.])],\n",
       "       [(1, 24, 40), array([0., 1.])],\n",
       "       [(1, 32, 48), array([0., 1.])],\n",
       "       [(1, 40, 56), array([0.01515144, 0.98484856])],\n",
       "       [(1, 48, 64), array([0.98484856, 0.01515144])],\n",
       "       [(1, 56, 72), array([0.98484856, 0.01515144])],\n",
       "       [(1, 64, 80), array([0.01515144, 0.98484856])],\n",
       "       [(1, 72, 88), array([0.01515144, 0.98484856])]], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_np_windows[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12350523, 2)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_np_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/noisy_labels_all_windows.npy', 'rb') as f:\n",
    "    preds_np_windows_metal = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(1, 0, 16), array([0.0032828, 0.9967172])],\n",
       "       [(1, 8, 24), array([0.0032828, 0.9967172])],\n",
       "       [(1, 16, 32), array([0.0032828, 0.9967172])],\n",
       "       [(1, 24, 40), array([0.0032828, 0.9967172])],\n",
       "       [(1, 32, 48), array([0.0032828, 0.9967172])],\n",
       "       [(1, 40, 56), array([0.24933879, 0.75066121])],\n",
       "       [(1, 48, 64), array([0.98051349, 0.01948651])],\n",
       "       [(1, 56, 72), array([0.36412127, 0.63587873])],\n",
       "       [(1, 64, 80), array([0.0032828, 0.9967172])],\n",
       "       [(1, 72, 88), array([0.0032828, 0.9967172])]], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_np_windows_metal[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12350523, 2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_np_windows_metal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Distribution across all data, timeseries LabelModel')\n",
    "ax.hist([\n",
    "    i[1][0]\n",
    "    for i in preds_np_windows\n",
    "])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Distribution across all data, Metal LabelModel')\n",
    "ax.hist([\n",
    "    i[1][0]\n",
    "    for i in preds_np_windows_metal\n",
    "])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/majority_vote_labels_all_windows.npy', 'rb') as f:\n",
    "    preds_np_windows_mv = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Distribution across all data, Majority Vote')\n",
    "ax.hist([\n",
    "    i[1][0]\n",
    "    for i in preds_np_windows_mv\n",
    "])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff (bad class balance, wrong R_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.8 s, sys: 1.6 s, total: 43.4 s\n",
      "Wall time: 43.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "max_seed = 10\n",
    "temporal_models = [None,]*max_seed\n",
    "for seed in range(max_seed):\n",
    "    markov_model = DPLabelModel(m=m_per_task*T, \n",
    "                                T=T,\n",
    "                                edges=[(i,i+m_per_task) for i in range((T-1)*m_per_task)],\n",
    "                                coverage_sets=[[t,] for t in range(T) for _ in range(m_per_task)],\n",
    "                                mu_sharing=[[t*m_per_task+i for t in range(T)] for i in range(m_per_task)],\n",
    "                                phi_sharing=[[(t*m_per_task+i, (t+1)*m_per_task+i) for t in range(T-1)] for i in range(m_per_task)],\n",
    "                                device=device,\n",
    "                                # class_balance=MRI_data_temporal['class_balance'],\n",
    "                                seed=seed)\n",
    "    optimize(markov_model, L_hat=MRI_data_temporal['Li_train'], num_iter=10, lr=1e-5, momentum=0.8, clamp=True, \n",
    "             verbose=False, seed=seed)\n",
    "    temporal_models[seed] = markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7325,)\n",
      "Accuracy: 0.357\n",
      "F1: 0.090\n",
      "Recall: 0.182\n",
      "Precision: 0.060\n",
      "(7325,)\n",
      "Accuracy: 0.369\n",
      "F1: 0.184\n",
      "Recall: 0.404\n",
      "Precision: 0.119\n",
      "(7325,)\n",
      "Accuracy: 0.523\n",
      "F1: 0.218\n",
      "Recall: 0.377\n",
      "Precision: 0.153\n",
      "(7325,)\n",
      "Accuracy: 0.388\n",
      "F1: 0.225\n",
      "Recall: 0.506\n",
      "Precision: 0.145\n",
      "(7325,)\n",
      "Accuracy: 0.375\n",
      "F1: 0.234\n",
      "Recall: 0.542\n",
      "Precision: 0.149\n",
      "(7325,)\n",
      "Accuracy: 0.668\n",
      "F1: 0.344\n",
      "Recall: 0.494\n",
      "Precision: 0.263\n",
      "(7325,)\n",
      "Accuracy: 0.580\n",
      "F1: 0.086\n",
      "Recall: 0.112\n",
      "Precision: 0.069\n",
      "(7325,)\n",
      "Accuracy: 0.509\n",
      "F1: 0.154\n",
      "Recall: 0.254\n",
      "Precision: 0.111\n",
      "(7325,)\n",
      "Accuracy: 0.277\n",
      "F1: 0.024\n",
      "Recall: 0.050\n",
      "Precision: 0.015\n",
      "(7325,)\n",
      "Accuracy: 0.544\n",
      "F1: 0.279\n",
      "Recall: 0.500\n",
      "Precision: 0.193\n"
     ]
    }
   ],
   "source": [
    "for seed, model in enumerate(temporal_models):\n",
    "    Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "    R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "    #find sequence label config. with highest prob.\n",
    "    config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "    R_pred_config = model.feasible_y[config_index]\n",
    "    R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "    #for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "    R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "    for idx in range(R_pred_config.shape[0]):\n",
    "        R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "    R_pred_probs = R_pred_probs.numpy()\n",
    "    R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "\n",
    "    Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "    R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "    #find sequence label config. with highest prob.\n",
    "    config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "    R_pred_config = model.feasible_y[config_index]\n",
    "    R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "    #for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "    R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "    for idx in range(R_pred_config.shape[0]):\n",
    "        R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "    R_pred_probs = R_pred_probs.numpy()\n",
    "    R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "    R_pred_frame_label = np.round(R_pred_probs.ravel())\n",
    "    R_pred_frame_label[R_pred_frame_label == 0.] = 2.\n",
    "\n",
    "    for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "        score = metric_score(Y_dev.cpu(), R_pred_frame_label, metric)\n",
    "        print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.207  F1=0.294  precision=0.175 recall=0.931 \n",
      "seed=1  accuracy=0.208  F1=0.270  precision=0.161 recall=0.823 \n",
      "seed=2  accuracy=0.730  F1=0.168  precision=0.185 recall=0.154 \n",
      "seed=3  accuracy=0.182  F1=0.300  precision=0.177 recall=0.988 \n",
      "seed=4  accuracy=0.196  F1=0.283  precision=0.168 recall=0.892 \n",
      "seed=5  accuracy=0.674  F1=0.143  precision=0.134 recall=0.154 \n",
      "seed=6  accuracy=0.812  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=7  accuracy=0.499  F1=0.193  precision=0.135 recall=0.338 \n",
      "seed=8  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=9  accuracy=0.639  F1=0.470  precision=0.318 recall=0.904 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=1 (20s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=2  accuracy=0.616  F1=0.228  precision=0.177 recall=0.319 \n",
      "seed=3  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=4  accuracy=0.174  F1=0.293  precision=0.173 recall=0.965 \n",
      "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=8  accuracy=0.739  F1=0.073  precision=0.098 recall=0.058 \n",
      "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=10 (40s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=2  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=3  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=4  accuracy=0.661  F1=0.347  precision=0.263 recall=0.508 \n",
      "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=8  accuracy=0.386  F1=0.334  precision=0.207 recall=0.865 \n",
      "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=50 (2 min 25s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=2  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=3  accuracy=0.621  F1=0.224  precision=0.176 recall=0.308 \n",
      "seed=4  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=8  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=100 (4 min 28s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=1  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=2  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=3  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=4  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=5  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=6  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=7  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=8  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=9  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=500 (22 min 2s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on runs\n",
    "\n",
    "```\n",
    "1 iteration:\n",
    "seed=0  accuracy=0.207  F1=0.294  precision=0.175 recall=0.931 \n",
    "seed=1  accuracy=0.208  F1=0.270  precision=0.161 recall=0.823 \n",
    "seed=2  accuracy=0.730  F1=0.168  precision=0.185 recall=0.154 \n",
    "seed=3  accuracy=0.182  F1=0.300  precision=0.177 recall=0.988 \n",
    "seed=4  accuracy=0.196  F1=0.283  precision=0.168 recall=0.892 \n",
    "seed=5  accuracy=0.674  F1=0.143  precision=0.134 recall=0.154 \n",
    "seed=6  accuracy=0.812  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=7  accuracy=0.499  F1=0.193  precision=0.135 recall=0.338 \n",
    "seed=8  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=9  accuracy=0.639  F1=0.470  precision=0.318 recall=0.904\n",
    "\n",
    "10 iterations:\n",
    "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=2  accuracy=0.616  F1=0.228  precision=0.177 recall=0.319 \n",
    "seed=3  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=4  accuracy=0.174  F1=0.293  precision=0.173 recall=0.965 \n",
    "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=8  accuracy=0.739  F1=0.073  precision=0.098 recall=0.058 \n",
    "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000\n",
    "\n",
    "50 iterations:\n",
    "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=2  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=3  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=4  accuracy=0.661  F1=0.347  precision=0.263 recall=0.508 \n",
    "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=8  accuracy=0.386  F1=0.334  precision=0.207 recall=0.865 \n",
    "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "\n",
    "100 iterations:\n",
    "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=2  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=3  accuracy=0.621  F1=0.224  precision=0.176 recall=0.308 \n",
    "seed=4  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=8  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "\n",
    "500 iterations:\n",
    "seed=0  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=1  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=2  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=3  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=4  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=5  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=6  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=7  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=8  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=9  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
