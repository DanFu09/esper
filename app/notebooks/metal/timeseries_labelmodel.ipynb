{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, pickle, csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "sys.path.append('/lfs/1/danfu/metal')\n",
    "sys.path.append('/lfs/1/danfu/sequential_ws')\n",
    "from metal.metrics import metric_score\n",
    "from torch.nn.functional import normalize\n",
    "from DP.label_model import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_train_100_windows.npz'\n",
    "L_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_val_windows.npz'\n",
    "Y_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_val_windows.npy'\n",
    "L_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_test_windows.npz'\n",
    "Y_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_test_windows.npy'\n",
    "\n",
    "stride = 1\n",
    "L_train_raw = sp.sparse.load_npz(L_train_path).todense()[::stride]\n",
    "L_dev_raw = sp.sparse.load_npz(L_dev_path).todense()\n",
    "Y_dev_raw = np.load(Y_dev_path)\n",
    "L_test_raw = sp.sparse.load_npz(L_test_path).todense()\n",
    "Y_test_raw = np.load(Y_test_path)\n",
    "\n",
    "T = 5\n",
    "\n",
    "L_train = torch.FloatTensor(L_train_raw[:L_train_raw.shape[0] - (L_train_raw.shape[0] % T)]).to(device)\n",
    "L_dev = torch.FloatTensor(L_dev_raw[:L_dev_raw.shape[0] - (L_dev_raw.shape[0] % T)]).to(device)\n",
    "Y_dev = torch.FloatTensor(Y_dev_raw[:Y_dev_raw.shape[0] - (Y_dev_raw.shape[0] % T)]).to(device)\n",
    "L_test = torch.FloatTensor(L_test_raw[:L_test_raw.shape[0] - (L_test_raw.shape[0] % T)]).to(device)\n",
    "Y_test = torch.FloatTensor(Y_test_raw[:Y_test_raw.shape[0] - (Y_test_raw.shape[0] % T)]).to(device)\n",
    "m_per_task = L_train.size(1)\n",
    "n_frames_train = L_train.size(0)\n",
    "n_patients_train = n_frames_train//T\n",
    "n_frames_dev = L_dev.size(0)\n",
    "n_patients_dev = n_frames_dev//T\n",
    "n_frames_test = L_test.size(0)\n",
    "n_patients_test = n_frames_test//T\n",
    "\n",
    "# MRI_data_naive = {'Li_train': (L_train.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'Li_dev': (L_dev.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'R_dev': (Y_dev.unsqueeze(1) == torch.FloatTensor([-1,1]).to(device).unsqueeze(0)).argmax(1),\n",
    "#                   'm':m_per_task, 'T':1,\n",
    "#                  }\n",
    "\n",
    "# don't need to transform the raw data\n",
    "MRI_data_naive = {'Li_train': L_train.long().to(device),\n",
    "                  'Li_dev': L_dev.long().to(device),\n",
    "                  'R_dev': Y_dev.long().to(device),\n",
    "                  'Li_test': L_test.long().to(device),\n",
    "                  'R_test': Y_test.long().to(device),\n",
    "                  'm':m_per_task, 'T':1,\n",
    "                 }\n",
    "MRI_data_naive['class_balance'] = normalize((MRI_data_naive['R_dev'].unsqueeze(1)==torch.arange(2, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                            dim=0, p=1)\n",
    "MRI_data_temporal = {'Li_train': MRI_data_naive['Li_train'].view(n_patients_train, (m_per_task*T)),\n",
    "                     'Li_dev': MRI_data_naive['Li_dev'].view(n_patients_dev, (m_per_task*T)),\n",
    "                     'R_dev': MRI_data_naive['R_dev']*(2**T-1),\n",
    "                     'Li_test': MRI_data_naive['Li_test'].view(n_patients_test, (m_per_task*T)),\n",
    "                     'R_test': MRI_data_naive['R_test']*(2**T-1),\n",
    "                     'm': m_per_task * T, 'T': T,\n",
    "                    } \n",
    "MRI_data_temporal['class_balance'] = normalize((MRI_data_temporal['R_dev'].unsqueeze(1)==torch.arange(2**T, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                                dim=0, p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MRI_data_naive['class_balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=50.6702766418457\n",
      "iteration=300 loss=8.80715274810791\n",
      "iteration=600 loss=3.66485595703125\n",
      "iteration=900 loss=1.8281009197235107\n",
      "iteration=1200 loss=0.9653668999671936\n",
      "iteration=1500 loss=0.6120548844337463\n",
      "iteration=1800 loss=0.4767417907714844\n",
      "iteration=2100 loss=0.40450260043144226\n",
      "iteration=2400 loss=0.3607012629508972\n",
      "iteration=2700 loss=0.3310929536819458\n",
      "iteration=2999 loss=0.30959445238113403\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           # class_balance=MRI_data_naive['class_balance'], \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=3000, lr=4.087885261759692e-05,\n",
    "         momentum=0.9, clamp=True, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.125\n",
      "F1: 0.389\n",
      "Recall: 0.712\n",
      "Precision: 0.268\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping Parameters\n",
      "Accuracy: 0.051\n",
      "F1: 0.143\n",
      "Recall: 0.288\n",
      "Precision: 0.095\n"
     ]
    }
   ],
   "source": [
    "# Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           class_balance=torch.tensor([.4, .6]).to(device), \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=5000, lr=4.087885261759692e-04,\n",
    "         momentum=0.9, clamp=False, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.171\n",
      "F1: 0.488\n",
      "Recall: 0.971\n",
      "Precision: 0.326\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping Parameters\n",
      "Accuracy: 0.069\n",
      "F1: 0.185\n",
      "Recall: 0.389\n",
      "Precision: 0.121\n"
     ]
    }
   ],
   "source": [
    "# Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=53.89699172973633\n",
      "iteration=100 loss=0.3030668795108795\n",
      "iteration=200 loss=0.23939409852027893\n",
      "iteration=300 loss=0.22569435834884644\n",
      "iteration=400 loss=0.22112223505973816\n",
      "iteration=500 loss=0.21906107664108276\n",
      "iteration=600 loss=0.2178979068994522\n",
      "iteration=700 loss=0.2171371728181839\n",
      "iteration=800 loss=0.21659354865550995\n",
      "iteration=900 loss=0.21618370711803436\n",
      "iteration=999 loss=0.21586711704730988\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           class_balance=torch.tensor([.3, .7]).to(device), \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=1000, lr=2.087885261759692e-02,\n",
    "         momentum=0., clamp=False, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.172\n",
      "F1: 0.489\n",
      "Recall: 0.975\n",
      "Precision: 0.326\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping Parameters\n",
      "Accuracy: 0.137\n",
      "F1: 0.245\n",
      "Recall: 0.777\n",
      "Precision: 0.145\n"
     ]
    }
   ],
   "source": [
    "# Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=66.03443145751953\n",
      "iteration=500 loss=2.6357004642486572\n",
      "iteration=1000 loss=0.3299635946750641\n",
      "iteration=1500 loss=0.29519757628440857\n",
      "iteration=2000 loss=0.2911560535430908\n",
      "iteration=2500 loss=0.28930333256721497\n",
      "iteration=3000 loss=0.2881740927696228\n",
      "iteration=3500 loss=0.28736594319343567\n",
      "iteration=4000 loss=0.28671225905418396\n",
      "iteration=4500 loss=0.2861360013484955\n",
      "iteration=4999 loss=0.2856000065803528\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           class_balance=torch.tensor([-.1, 1.1]).to(device), \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=5000, lr=1.087885261759692e-02,\n",
    "         momentum=0.3, clamp=False, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.154\n",
      "F1: 0.452\n",
      "Recall: 0.874\n",
      "Precision: 0.304\n",
      "Flipping Parameters\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "    # Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=52.20237731933594\n",
      "iteration=200 loss=0.23215782642364502\n",
      "iteration=400 loss=0.19091296195983887\n",
      "iteration=600 loss=0.183028444647789\n",
      "iteration=800 loss=0.18011479079723358\n",
      "iteration=1000 loss=0.17864780128002167\n",
      "iteration=1200 loss=0.17776866257190704\n",
      "iteration=1400 loss=0.17718084156513214\n",
      "iteration=1600 loss=0.17675688862800598\n",
      "iteration=1800 loss=0.1764335036277771\n",
      "iteration=1999 loss=0.176177516579628\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           class_balance=torch.tensor([.4, .6]).to(device), \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'][::4000], num_iter=2000, lr=1.087885261759692e-03,\n",
    "         momentum=.9, clamp=False, seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.171\n",
      "F1: 0.488\n",
      "Recall: 0.971\n",
      "Precision: 0.326\n",
      "Flipping Parameters\n",
      "Accuracy: 0.069\n",
      "F1: 0.185\n",
      "Recall: 0.389\n",
      "Precision: 0.121\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "    # Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_target = Y_dev.long()\n",
    "T = 5\n",
    "\n",
    "feasible_y = np.array([[-1, -1, -1, -1, -1],\n",
    "        [-1,  1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1],\n",
    "        [ 1,  1, -1, -1, -1],\n",
    "        [-1, -1,  1, -1, -1],\n",
    "        [-1,  1,  1, -1, -1],\n",
    "        [ 1, -1,  1, -1, -1],\n",
    "        [ 1,  1,  1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1],\n",
    "        [-1,  1, -1,  1, -1],\n",
    "        [ 1, -1, -1,  1, -1],\n",
    "        [ 1,  1, -1,  1, -1],\n",
    "        [-1, -1,  1,  1, -1],\n",
    "        [-1,  1,  1,  1, -1],\n",
    "        [ 1, -1,  1,  1, -1],\n",
    "        [ 1,  1,  1,  1, -1],\n",
    "        [-1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1,  1],\n",
    "        [ 1,  1, -1, -1,  1],\n",
    "        [-1, -1,  1, -1,  1],\n",
    "        [-1,  1,  1, -1,  1],\n",
    "        [ 1, -1,  1, -1,  1],\n",
    "        [ 1,  1,  1, -1,  1],\n",
    "        [-1, -1, -1,  1,  1],\n",
    "        [-1,  1, -1,  1,  1],\n",
    "        [ 1, -1, -1,  1,  1],\n",
    "        [ 1,  1, -1,  1,  1],\n",
    "        [-1, -1,  1,  1,  1],\n",
    "        [-1,  1,  1,  1,  1],\n",
    "        [ 1, -1,  1,  1,  1],\n",
    "        [ 1,  1,  1,  1,  1]])\n",
    "\n",
    "feasible_y[feasible_y==-1] = 0\n",
    "feasible_y = feasible_y.tolist()\n",
    "possibilities = list(map(lambda l : ''.join(map(str,l)), feasible_y))\n",
    "\n",
    "class_balance = np.empty(2 ** T)\n",
    "#compute class balance from dev set and use laplace smoothing\n",
    "\n",
    "valid_target_copy = np.copy(valid_target)\n",
    "valid_target_copy[valid_target_copy == 2] = 0\n",
    "\n",
    "assert len(valid_target_copy) % T == 0\n",
    "num_windows = len(valid_target_copy) / T\n",
    "\n",
    "freq = {}\n",
    "for i in range(0, len(valid_target_copy), T):\n",
    "    s = ''.join(map(str,valid_target_copy[i:i+T]))\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "for i in range(len(class_balance)):\n",
    "    if possibilities[i] in freq and freq[possibilities[i]] > 5:\n",
    "        class_balance[i] = (freq[possibilities[i]] + 1) / (num_windows + len(possibilities))\n",
    "    else:\n",
    "        class_balance[i] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0001\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.123\n",
      "F1: 0.742\n",
      "Recall: 0.701\n",
      "Precision: 0.789\n",
      "Accuracy: 0.010\n",
      "F1: 0.031\n",
      "Recall: 0.057\n",
      "Precision: 0.021\n",
      "\n",
      "Accuracy: 0.052\n",
      "F1: 0.377\n",
      "Recall: 0.296\n",
      "Precision: 0.521\n",
      "Accuracy: 0.097\n",
      "F1: 0.363\n",
      "Recall: 0.550\n",
      "Precision: 0.271\n",
      "\n",
      "Accuracy: 0.139\n",
      "F1: 0.461\n",
      "Recall: 0.787\n",
      "Precision: 0.326\n",
      "Accuracy: 0.015\n",
      "F1: 0.074\n",
      "Recall: 0.084\n",
      "Precision: 0.067\n",
      "\n",
      "Accuracy: 0.095\n",
      "F1: 0.630\n",
      "Recall: 0.538\n",
      "Precision: 0.760\n",
      "Accuracy: 0.042\n",
      "F1: 0.085\n",
      "Recall: 0.237\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.046\n",
      "F1: 0.223\n",
      "Recall: 0.261\n",
      "Precision: 0.195\n",
      "Accuracy: 0.002\n",
      "F1: 0.021\n",
      "Recall: 0.011\n",
      "Precision: 0.250\n",
      "\n",
      "Accuracy: 0.025\n",
      "F1: 0.068\n",
      "Recall: 0.140\n",
      "Precision: 0.045\n",
      "Accuracy: 0.072\n",
      "F1: 0.540\n",
      "Recall: 0.412\n",
      "Precision: 0.782\n",
      "\n",
      "Accuracy: 0.046\n",
      "F1: 0.166\n",
      "Recall: 0.261\n",
      "Precision: 0.121\n",
      "Accuracy: 0.147\n",
      "F1: 0.687\n",
      "Recall: 0.834\n",
      "Precision: 0.584\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.103\n",
      "Recall: 0.292\n",
      "Precision: 0.063\n",
      "Accuracy: 0.079\n",
      "F1: 0.595\n",
      "Recall: 0.447\n",
      "Precision: 0.892\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.018\n",
      "Recall: 0.034\n",
      "Precision: 0.012\n",
      "Accuracy: 0.161\n",
      "F1: 0.595\n",
      "Recall: 0.914\n",
      "Precision: 0.441\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.505\n",
      "Recall: 0.499\n",
      "Precision: 0.511\n",
      "Accuracy: 0.060\n",
      "F1: 0.134\n",
      "Recall: 0.342\n",
      "Precision: 0.083\n",
      "\n",
      "1 1e-05\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.123\n",
      "F1: 0.742\n",
      "Recall: 0.701\n",
      "Precision: 0.789\n",
      "Accuracy: 0.010\n",
      "F1: 0.031\n",
      "Recall: 0.057\n",
      "Precision: 0.021\n",
      "\n",
      "Accuracy: 0.052\n",
      "F1: 0.377\n",
      "Recall: 0.296\n",
      "Precision: 0.521\n",
      "Accuracy: 0.097\n",
      "F1: 0.363\n",
      "Recall: 0.550\n",
      "Precision: 0.271\n",
      "\n",
      "Accuracy: 0.139\n",
      "F1: 0.461\n",
      "Recall: 0.787\n",
      "Precision: 0.326\n",
      "Accuracy: 0.015\n",
      "F1: 0.074\n",
      "Recall: 0.084\n",
      "Precision: 0.067\n",
      "\n",
      "Accuracy: 0.095\n",
      "F1: 0.630\n",
      "Recall: 0.538\n",
      "Precision: 0.760\n",
      "Accuracy: 0.042\n",
      "F1: 0.085\n",
      "Recall: 0.237\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.046\n",
      "F1: 0.223\n",
      "Recall: 0.261\n",
      "Precision: 0.195\n",
      "Accuracy: 0.002\n",
      "F1: 0.021\n",
      "Recall: 0.011\n",
      "Precision: 0.250\n",
      "\n",
      "Accuracy: 0.025\n",
      "F1: 0.068\n",
      "Recall: 0.140\n",
      "Precision: 0.045\n",
      "Accuracy: 0.072\n",
      "F1: 0.540\n",
      "Recall: 0.412\n",
      "Precision: 0.782\n",
      "\n",
      "Accuracy: 0.046\n",
      "F1: 0.166\n",
      "Recall: 0.261\n",
      "Precision: 0.121\n",
      "Accuracy: 0.147\n",
      "F1: 0.687\n",
      "Recall: 0.834\n",
      "Precision: 0.584\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.103\n",
      "Recall: 0.292\n",
      "Precision: 0.063\n",
      "Accuracy: 0.079\n",
      "F1: 0.595\n",
      "Recall: 0.447\n",
      "Precision: 0.892\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.018\n",
      "Recall: 0.034\n",
      "Precision: 0.012\n",
      "Accuracy: 0.161\n",
      "F1: 0.595\n",
      "Recall: 0.914\n",
      "Precision: 0.441\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.505\n",
      "Recall: 0.499\n",
      "Precision: 0.511\n",
      "Accuracy: 0.060\n",
      "F1: 0.134\n",
      "Recall: 0.342\n",
      "Precision: 0.083\n",
      "\n",
      "1 1e-06\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.123\n",
      "F1: 0.742\n",
      "Recall: 0.701\n",
      "Precision: 0.789\n",
      "Accuracy: 0.010\n",
      "F1: 0.031\n",
      "Recall: 0.057\n",
      "Precision: 0.021\n",
      "\n",
      "Accuracy: 0.052\n",
      "F1: 0.377\n",
      "Recall: 0.296\n",
      "Precision: 0.521\n",
      "Accuracy: 0.097\n",
      "F1: 0.363\n",
      "Recall: 0.550\n",
      "Precision: 0.271\n",
      "\n",
      "Accuracy: 0.139\n",
      "F1: 0.461\n",
      "Recall: 0.787\n",
      "Precision: 0.326\n",
      "Accuracy: 0.015\n",
      "F1: 0.074\n",
      "Recall: 0.084\n",
      "Precision: 0.067\n",
      "\n",
      "Accuracy: 0.095\n",
      "F1: 0.630\n",
      "Recall: 0.538\n",
      "Precision: 0.760\n",
      "Accuracy: 0.042\n",
      "F1: 0.085\n",
      "Recall: 0.237\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.046\n",
      "F1: 0.223\n",
      "Recall: 0.261\n",
      "Precision: 0.195\n",
      "Accuracy: 0.002\n",
      "F1: 0.021\n",
      "Recall: 0.011\n",
      "Precision: 0.250\n",
      "\n",
      "Accuracy: 0.025\n",
      "F1: 0.068\n",
      "Recall: 0.140\n",
      "Precision: 0.045\n",
      "Accuracy: 0.072\n",
      "F1: 0.540\n",
      "Recall: 0.412\n",
      "Precision: 0.782\n",
      "\n",
      "Accuracy: 0.046\n",
      "F1: 0.166\n",
      "Recall: 0.261\n",
      "Precision: 0.121\n",
      "Accuracy: 0.147\n",
      "F1: 0.687\n",
      "Recall: 0.834\n",
      "Precision: 0.584\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.103\n",
      "Recall: 0.292\n",
      "Precision: 0.063\n",
      "Accuracy: 0.079\n",
      "F1: 0.595\n",
      "Recall: 0.447\n",
      "Precision: 0.892\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.018\n",
      "Recall: 0.034\n",
      "Precision: 0.012\n",
      "Accuracy: 0.161\n",
      "F1: 0.595\n",
      "Recall: 0.914\n",
      "Precision: 0.441\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.505\n",
      "Recall: 0.499\n",
      "Precision: 0.511\n",
      "Accuracy: 0.060\n",
      "F1: 0.134\n",
      "Recall: 0.342\n",
      "Precision: 0.083\n",
      "\n",
      "1 1e-07\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.123\n",
      "F1: 0.742\n",
      "Recall: 0.701\n",
      "Precision: 0.789\n",
      "Accuracy: 0.010\n",
      "F1: 0.031\n",
      "Recall: 0.057\n",
      "Precision: 0.021\n",
      "\n",
      "Accuracy: 0.052\n",
      "F1: 0.377\n",
      "Recall: 0.296\n",
      "Precision: 0.521\n",
      "Accuracy: 0.097\n",
      "F1: 0.363\n",
      "Recall: 0.550\n",
      "Precision: 0.271\n",
      "\n",
      "Accuracy: 0.139\n",
      "F1: 0.461\n",
      "Recall: 0.787\n",
      "Precision: 0.326\n",
      "Accuracy: 0.015\n",
      "F1: 0.074\n",
      "Recall: 0.084\n",
      "Precision: 0.067\n",
      "\n",
      "Accuracy: 0.095\n",
      "F1: 0.630\n",
      "Recall: 0.538\n",
      "Precision: 0.760\n",
      "Accuracy: 0.042\n",
      "F1: 0.085\n",
      "Recall: 0.237\n",
      "Precision: 0.052\n",
      "\n",
      "Accuracy: 0.046\n",
      "F1: 0.223\n",
      "Recall: 0.261\n",
      "Precision: 0.195\n",
      "Accuracy: 0.002\n",
      "F1: 0.021\n",
      "Recall: 0.011\n",
      "Precision: 0.250\n",
      "\n",
      "Accuracy: 0.025\n",
      "F1: 0.068\n",
      "Recall: 0.140\n",
      "Precision: 0.045\n",
      "Accuracy: 0.072\n",
      "F1: 0.540\n",
      "Recall: 0.412\n",
      "Precision: 0.782\n",
      "\n",
      "Accuracy: 0.046\n",
      "F1: 0.166\n",
      "Recall: 0.261\n",
      "Precision: 0.121\n",
      "Accuracy: 0.147\n",
      "F1: 0.687\n",
      "Recall: 0.834\n",
      "Precision: 0.584\n",
      "\n",
      "Accuracy: 0.051\n",
      "F1: 0.103\n",
      "Recall: 0.292\n",
      "Precision: 0.063\n",
      "Accuracy: 0.079\n",
      "F1: 0.595\n",
      "Recall: 0.447\n",
      "Precision: 0.892\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.018\n",
      "Recall: 0.034\n",
      "Precision: 0.012\n",
      "Accuracy: 0.161\n",
      "F1: 0.595\n",
      "Recall: 0.914\n",
      "Precision: 0.441\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.505\n",
      "Recall: 0.499\n",
      "Precision: 0.511\n",
      "Accuracy: 0.060\n",
      "F1: 0.134\n",
      "Recall: 0.342\n",
      "Precision: 0.083\n",
      "\n",
      "5 0.0001\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.138\n",
      "F1: 0.714\n",
      "Recall: 0.786\n",
      "Precision: 0.654\n",
      "Accuracy: 0.014\n",
      "F1: 0.044\n",
      "Recall: 0.081\n",
      "Precision: 0.030\n",
      "\n",
      "Accuracy: 0.110\n",
      "F1: 0.698\n",
      "Recall: 0.625\n",
      "Precision: 0.789\n",
      "Accuracy: 0.083\n",
      "F1: 0.176\n",
      "Recall: 0.472\n",
      "Precision: 0.108\n",
      "\n",
      "Accuracy: 0.123\n",
      "F1: 0.413\n",
      "Recall: 0.700\n",
      "Precision: 0.292\n",
      "Accuracy: 0.025\n",
      "F1: 0.113\n",
      "Recall: 0.142\n",
      "Precision: 0.093\n",
      "\n",
      "Accuracy: 0.113\n",
      "F1: 0.707\n",
      "Recall: 0.644\n",
      "Precision: 0.783\n",
      "Accuracy: 0.034\n",
      "F1: 0.069\n",
      "Recall: 0.192\n",
      "Precision: 0.042\n",
      "\n",
      "Accuracy: 0.058\n",
      "F1: 0.396\n",
      "Recall: 0.330\n",
      "Precision: 0.494\n",
      "Accuracy: 0.002\n",
      "F1: 0.007\n",
      "Recall: 0.011\n",
      "Precision: 0.005\n",
      "\n",
      "Accuracy: 0.058\n",
      "F1: 0.288\n",
      "Recall: 0.330\n",
      "Precision: 0.256\n",
      "Accuracy: 0.057\n",
      "F1: 0.335\n",
      "Recall: 0.326\n",
      "Precision: 0.345\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.061\n",
      "Recall: 0.082\n",
      "Precision: 0.049\n",
      "Accuracy: 0.161\n",
      "F1: 0.379\n",
      "Recall: 0.914\n",
      "Precision: 0.239\n",
      "\n",
      "Accuracy: 0.128\n",
      "F1: 0.357\n",
      "Recall: 0.725\n",
      "Precision: 0.237\n",
      "Accuracy: 0.014\n",
      "F1: 0.076\n",
      "Recall: 0.082\n",
      "Precision: 0.071\n",
      "\n",
      "Accuracy: 0.022\n",
      "F1: 0.072\n",
      "Recall: 0.125\n",
      "Precision: 0.050\n",
      "Accuracy: 0.152\n",
      "F1: 0.431\n",
      "Recall: 0.866\n",
      "Precision: 0.287\n",
      "\n",
      "Accuracy: 0.153\n",
      "F1: 0.769\n",
      "Recall: 0.867\n",
      "Precision: 0.690\n",
      "Accuracy: 0.026\n",
      "F1: 0.056\n",
      "Recall: 0.151\n",
      "Precision: 0.034\n",
      "\n",
      "5 1e-05\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.138\n",
      "F1: 0.714\n",
      "Recall: 0.786\n",
      "Precision: 0.654\n",
      "Accuracy: 0.014\n",
      "F1: 0.044\n",
      "Recall: 0.081\n",
      "Precision: 0.030\n",
      "\n",
      "Accuracy: 0.110\n",
      "F1: 0.698\n",
      "Recall: 0.625\n",
      "Precision: 0.789\n",
      "Accuracy: 0.083\n",
      "F1: 0.176\n",
      "Recall: 0.472\n",
      "Precision: 0.108\n",
      "\n",
      "Accuracy: 0.123\n",
      "F1: 0.413\n",
      "Recall: 0.700\n",
      "Precision: 0.292\n",
      "Accuracy: 0.025\n",
      "F1: 0.113\n",
      "Recall: 0.142\n",
      "Precision: 0.093\n",
      "\n",
      "Accuracy: 0.113\n",
      "F1: 0.707\n",
      "Recall: 0.644\n",
      "Precision: 0.783\n",
      "Accuracy: 0.034\n",
      "F1: 0.069\n",
      "Recall: 0.192\n",
      "Precision: 0.042\n",
      "\n",
      "Accuracy: 0.058\n",
      "F1: 0.396\n",
      "Recall: 0.330\n",
      "Precision: 0.494\n",
      "Accuracy: 0.002\n",
      "F1: 0.007\n",
      "Recall: 0.011\n",
      "Precision: 0.005\n",
      "\n",
      "Accuracy: 0.058\n",
      "F1: 0.288\n",
      "Recall: 0.330\n",
      "Precision: 0.256\n",
      "Accuracy: 0.057\n",
      "F1: 0.335\n",
      "Recall: 0.326\n",
      "Precision: 0.345\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.061\n",
      "Recall: 0.082\n",
      "Precision: 0.049\n",
      "Accuracy: 0.161\n",
      "F1: 0.379\n",
      "Recall: 0.914\n",
      "Precision: 0.239\n",
      "\n",
      "Accuracy: 0.128\n",
      "F1: 0.357\n",
      "Recall: 0.725\n",
      "Precision: 0.237\n",
      "Accuracy: 0.014\n",
      "F1: 0.076\n",
      "Recall: 0.082\n",
      "Precision: 0.071\n",
      "\n",
      "Accuracy: 0.022\n",
      "F1: 0.072\n",
      "Recall: 0.125\n",
      "Precision: 0.050\n",
      "Accuracy: 0.152\n",
      "F1: 0.431\n",
      "Recall: 0.866\n",
      "Precision: 0.287\n",
      "\n",
      "Accuracy: 0.153\n",
      "F1: 0.769\n",
      "Recall: 0.867\n",
      "Precision: 0.690\n",
      "Accuracy: 0.026\n",
      "F1: 0.056\n",
      "Recall: 0.151\n",
      "Precision: 0.034\n",
      "\n",
      "5 1e-06\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.138\n",
      "F1: 0.714\n",
      "Recall: 0.786\n",
      "Precision: 0.654\n",
      "Accuracy: 0.014\n",
      "F1: 0.044\n",
      "Recall: 0.081\n",
      "Precision: 0.030\n",
      "\n",
      "Accuracy: 0.110\n",
      "F1: 0.698\n",
      "Recall: 0.625\n",
      "Precision: 0.789\n",
      "Accuracy: 0.083\n",
      "F1: 0.176\n",
      "Recall: 0.472\n",
      "Precision: 0.108\n",
      "\n",
      "Accuracy: 0.123\n",
      "F1: 0.413\n",
      "Recall: 0.700\n",
      "Precision: 0.292\n",
      "Accuracy: 0.025\n",
      "F1: 0.113\n",
      "Recall: 0.142\n",
      "Precision: 0.093\n",
      "\n",
      "Accuracy: 0.113\n",
      "F1: 0.707\n",
      "Recall: 0.644\n",
      "Precision: 0.783\n",
      "Accuracy: 0.034\n",
      "F1: 0.069\n",
      "Recall: 0.192\n",
      "Precision: 0.042\n",
      "\n",
      "Accuracy: 0.058\n",
      "F1: 0.396\n",
      "Recall: 0.330\n",
      "Precision: 0.494\n",
      "Accuracy: 0.002\n",
      "F1: 0.007\n",
      "Recall: 0.011\n",
      "Precision: 0.005\n",
      "\n",
      "Accuracy: 0.058\n",
      "F1: 0.288\n",
      "Recall: 0.330\n",
      "Precision: 0.256\n",
      "Accuracy: 0.057\n",
      "F1: 0.335\n",
      "Recall: 0.326\n",
      "Precision: 0.345\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.061\n",
      "Recall: 0.082\n",
      "Precision: 0.049\n",
      "Accuracy: 0.161\n",
      "F1: 0.379\n",
      "Recall: 0.914\n",
      "Precision: 0.239\n",
      "\n",
      "Accuracy: 0.128\n",
      "F1: 0.357\n",
      "Recall: 0.725\n",
      "Precision: 0.237\n",
      "Accuracy: 0.014\n",
      "F1: 0.076\n",
      "Recall: 0.082\n",
      "Precision: 0.071\n",
      "\n",
      "Accuracy: 0.022\n",
      "F1: 0.072\n",
      "Recall: 0.125\n",
      "Precision: 0.050\n",
      "Accuracy: 0.152\n",
      "F1: 0.431\n",
      "Recall: 0.866\n",
      "Precision: 0.287\n",
      "\n",
      "Accuracy: 0.153\n",
      "F1: 0.769\n",
      "Recall: 0.867\n",
      "Precision: 0.690\n",
      "Accuracy: 0.026\n",
      "F1: 0.056\n",
      "Recall: 0.151\n",
      "Precision: 0.034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 1e-07\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.138\n",
      "F1: 0.714\n",
      "Recall: 0.786\n",
      "Precision: 0.654\n",
      "Accuracy: 0.014\n",
      "F1: 0.044\n",
      "Recall: 0.081\n",
      "Precision: 0.030\n",
      "\n",
      "Accuracy: 0.110\n",
      "F1: 0.698\n",
      "Recall: 0.625\n",
      "Precision: 0.789\n",
      "Accuracy: 0.083\n",
      "F1: 0.176\n",
      "Recall: 0.472\n",
      "Precision: 0.108\n",
      "\n",
      "Accuracy: 0.123\n",
      "F1: 0.413\n",
      "Recall: 0.700\n",
      "Precision: 0.292\n",
      "Accuracy: 0.025\n",
      "F1: 0.113\n",
      "Recall: 0.142\n",
      "Precision: 0.093\n",
      "\n",
      "Accuracy: 0.113\n",
      "F1: 0.707\n",
      "Recall: 0.644\n",
      "Precision: 0.783\n",
      "Accuracy: 0.034\n",
      "F1: 0.069\n",
      "Recall: 0.192\n",
      "Precision: 0.042\n",
      "\n",
      "Accuracy: 0.058\n",
      "F1: 0.396\n",
      "Recall: 0.330\n",
      "Precision: 0.494\n",
      "Accuracy: 0.002\n",
      "F1: 0.007\n",
      "Recall: 0.011\n",
      "Precision: 0.005\n",
      "\n",
      "Accuracy: 0.058\n",
      "F1: 0.288\n",
      "Recall: 0.330\n",
      "Precision: 0.256\n",
      "Accuracy: 0.057\n",
      "F1: 0.335\n",
      "Recall: 0.326\n",
      "Precision: 0.345\n",
      "\n",
      "Accuracy: 0.014\n",
      "F1: 0.061\n",
      "Recall: 0.082\n",
      "Precision: 0.049\n",
      "Accuracy: 0.161\n",
      "F1: 0.379\n",
      "Recall: 0.914\n",
      "Precision: 0.239\n",
      "\n",
      "Accuracy: 0.128\n",
      "F1: 0.357\n",
      "Recall: 0.725\n",
      "Precision: 0.237\n",
      "Accuracy: 0.014\n",
      "F1: 0.076\n",
      "Recall: 0.082\n",
      "Precision: 0.071\n",
      "\n",
      "Accuracy: 0.022\n",
      "F1: 0.072\n",
      "Recall: 0.125\n",
      "Precision: 0.050\n",
      "Accuracy: 0.152\n",
      "F1: 0.431\n",
      "Recall: 0.866\n",
      "Precision: 0.287\n",
      "\n",
      "Accuracy: 0.153\n",
      "F1: 0.769\n",
      "Recall: 0.867\n",
      "Precision: 0.690\n",
      "Accuracy: 0.026\n",
      "F1: 0.056\n",
      "Recall: 0.151\n",
      "Precision: 0.034\n",
      "\n",
      "10 0.0001\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=1 loss=5632.9453125\n",
      "iteration=2 loss=4618.89990234375\n",
      "iteration=3 loss=3857.818603515625\n",
      "iteration=4 loss=3410.666748046875\n",
      "iteration=5 loss=3200.419189453125\n",
      "iteration=6 loss=3128.30224609375\n",
      "iteration=7 loss=3067.4853515625\n",
      "iteration=8 loss=2949.888916015625\n",
      "iteration=9 loss=2828.172607421875\n",
      "iteration=9 loss=2828.172607421875\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=1 loss=5822.919921875\n",
      "iteration=2 loss=4285.66455078125\n",
      "iteration=3 loss=3382.89453125\n",
      "iteration=4 loss=2981.481689453125\n",
      "iteration=5 loss=2874.3857421875\n",
      "iteration=6 loss=2885.357177734375\n",
      "iteration=7 loss=2813.029296875\n",
      "iteration=8 loss=2693.588134765625\n",
      "iteration=9 loss=2532.54248046875\n",
      "iteration=9 loss=2532.54248046875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=1 loss=5959.0322265625\n",
      "iteration=2 loss=4745.20166015625\n",
      "iteration=3 loss=3887.12255859375\n",
      "iteration=4 loss=3390.8017578125\n",
      "iteration=5 loss=3163.537109375\n",
      "iteration=6 loss=3107.765869140625\n",
      "iteration=7 loss=3117.98388671875\n",
      "iteration=8 loss=3094.12451171875\n",
      "iteration=9 loss=2981.397705078125\n",
      "iteration=9 loss=2981.397705078125\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=1 loss=4493.43408203125\n",
      "iteration=2 loss=3943.192626953125\n",
      "iteration=3 loss=3505.8544921875\n",
      "iteration=4 loss=3226.01025390625\n",
      "iteration=5 loss=3073.938232421875\n",
      "iteration=6 loss=3000.66064453125\n",
      "iteration=7 loss=2955.044921875\n",
      "iteration=8 loss=2894.46435546875\n",
      "iteration=9 loss=2796.409423828125\n",
      "iteration=9 loss=2796.409423828125\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=1 loss=5610.2197265625\n",
      "iteration=2 loss=4271.1875\n",
      "iteration=3 loss=3498.073486328125\n",
      "iteration=4 loss=3168.522216796875\n",
      "iteration=5 loss=3115.293212890625\n",
      "iteration=6 loss=3187.423583984375\n",
      "iteration=7 loss=3229.34814453125\n",
      "iteration=8 loss=3134.40625\n",
      "iteration=9 loss=2907.79736328125\n",
      "iteration=9 loss=2907.79736328125\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=1 loss=5636.05517578125\n",
      "iteration=2 loss=4333.31689453125\n",
      "iteration=3 loss=3550.0322265625\n",
      "iteration=4 loss=3179.023193359375\n",
      "iteration=5 loss=3051.247314453125\n",
      "iteration=6 loss=3035.751953125\n",
      "iteration=7 loss=3033.904052734375\n",
      "iteration=8 loss=2982.943359375\n",
      "iteration=9 loss=2863.72216796875\n",
      "iteration=9 loss=2863.72216796875\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=1 loss=6476.904296875\n",
      "iteration=2 loss=4289.482421875\n",
      "iteration=3 loss=3394.206298828125\n",
      "iteration=4 loss=3155.933837890625\n",
      "iteration=5 loss=3239.47509765625\n",
      "iteration=6 loss=3395.568359375\n",
      "iteration=7 loss=3388.4248046875\n",
      "iteration=8 loss=3244.39990234375\n",
      "iteration=9 loss=3007.44970703125\n",
      "iteration=9 loss=3007.44970703125\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=1 loss=6776.751953125\n",
      "iteration=2 loss=4109.3974609375\n",
      "iteration=3 loss=3279.71923828125\n",
      "iteration=4 loss=3202.09814453125\n",
      "iteration=5 loss=3462.451416015625\n",
      "iteration=6 loss=3759.118896484375\n",
      "iteration=7 loss=3729.509521484375\n",
      "iteration=8 loss=3461.197265625\n",
      "iteration=9 loss=3091.236328125\n",
      "iteration=9 loss=3091.236328125\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=1 loss=7627.36376953125\n",
      "iteration=2 loss=4756.70458984375\n",
      "iteration=3 loss=3683.372802734375\n",
      "iteration=4 loss=3371.4580078125\n",
      "iteration=5 loss=3420.547119140625\n",
      "iteration=6 loss=3518.11669921875\n",
      "iteration=7 loss=3435.279296875\n",
      "iteration=8 loss=3230.638916015625\n",
      "iteration=9 loss=2953.847412109375\n",
      "iteration=9 loss=2953.847412109375\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=1 loss=5688.439453125\n",
      "iteration=2 loss=4321.1201171875\n",
      "iteration=3 loss=3478.000244140625\n",
      "iteration=4 loss=3031.965087890625\n",
      "iteration=5 loss=2831.9951171875\n",
      "iteration=6 loss=2772.63623046875\n",
      "iteration=7 loss=2763.76416015625\n",
      "iteration=8 loss=2711.339599609375\n",
      "iteration=9 loss=2596.434814453125\n",
      "iteration=9 loss=2596.434814453125\n",
      "Accuracy: 0.149\n",
      "F1: 0.679\n",
      "Recall: 0.848\n",
      "Precision: 0.566\n",
      "Accuracy: 0.013\n",
      "F1: 0.042\n",
      "Recall: 0.074\n",
      "Precision: 0.029\n",
      "\n",
      "Accuracy: 0.151\n",
      "F1: 0.695\n",
      "Recall: 0.856\n",
      "Precision: 0.585\n",
      "Accuracy: 0.023\n",
      "F1: 0.060\n",
      "Recall: 0.133\n",
      "Precision: 0.039\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.443\n",
      "Recall: 0.885\n",
      "Precision: 0.295\n",
      "Accuracy: 0.016\n",
      "F1: 0.077\n",
      "Recall: 0.092\n",
      "Precision: 0.066\n",
      "\n",
      "Accuracy: 0.149\n",
      "F1: 0.798\n",
      "Recall: 0.849\n",
      "Precision: 0.753\n",
      "Accuracy: 0.024\n",
      "F1: 0.049\n",
      "Recall: 0.134\n",
      "Precision: 0.030\n",
      "\n",
      "Accuracy: 0.127\n",
      "F1: 0.635\n",
      "Recall: 0.723\n",
      "Precision: 0.566\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.008\n",
      "Precision: 0.004\n",
      "\n",
      "Accuracy: 0.153\n",
      "F1: 0.590\n",
      "Recall: 0.869\n",
      "Precision: 0.447\n",
      "Accuracy: 0.008\n",
      "F1: 0.052\n",
      "Recall: 0.044\n",
      "Precision: 0.063\n",
      "\n",
      "Accuracy: 0.138\n",
      "F1: 0.413\n",
      "Recall: 0.785\n",
      "Precision: 0.280\n",
      "Accuracy: 0.064\n",
      "F1: 0.182\n",
      "Recall: 0.362\n",
      "Precision: 0.121\n",
      "\n",
      "Accuracy: 0.149\n",
      "F1: 0.387\n",
      "Recall: 0.845\n",
      "Precision: 0.251\n",
      "Accuracy: 0.002\n",
      "F1: 0.012\n",
      "Recall: 0.012\n",
      "Precision: 0.012\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.370\n",
      "Recall: 0.888\n",
      "Precision: 0.233\n",
      "Accuracy: 0.018\n",
      "F1: 0.075\n",
      "Recall: 0.100\n",
      "Precision: 0.059\n",
      "\n",
      "Accuracy: 0.170\n",
      "F1: 0.708\n",
      "Recall: 0.964\n",
      "Precision: 0.560\n",
      "Accuracy: 0.005\n",
      "F1: 0.010\n",
      "Recall: 0.026\n",
      "Precision: 0.006\n",
      "\n",
      "10 1e-05\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=1 loss=5632.9453125\n",
      "iteration=2 loss=4618.89990234375\n",
      "iteration=3 loss=3857.818603515625\n",
      "iteration=4 loss=3410.666748046875\n",
      "iteration=5 loss=3200.419189453125\n",
      "iteration=6 loss=3128.30224609375\n",
      "iteration=7 loss=3067.4853515625\n",
      "iteration=8 loss=2949.888916015625\n",
      "iteration=9 loss=2828.172607421875\n",
      "iteration=9 loss=2828.172607421875\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=1 loss=5822.919921875\n",
      "iteration=2 loss=4285.66455078125\n",
      "iteration=3 loss=3382.89453125\n",
      "iteration=4 loss=2981.481689453125\n",
      "iteration=5 loss=2874.3857421875\n",
      "iteration=6 loss=2885.357177734375\n",
      "iteration=7 loss=2813.029296875\n",
      "iteration=8 loss=2693.588134765625\n",
      "iteration=9 loss=2532.54248046875\n",
      "iteration=9 loss=2532.54248046875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=1 loss=5959.0322265625\n",
      "iteration=2 loss=4745.20166015625\n",
      "iteration=3 loss=3887.12255859375\n",
      "iteration=4 loss=3390.8017578125\n",
      "iteration=5 loss=3163.537109375\n",
      "iteration=6 loss=3107.765869140625\n",
      "iteration=7 loss=3117.98388671875\n",
      "iteration=8 loss=3094.12451171875\n",
      "iteration=9 loss=2981.397705078125\n",
      "iteration=9 loss=2981.397705078125\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=1 loss=4493.43408203125\n",
      "iteration=2 loss=3943.192626953125\n",
      "iteration=3 loss=3505.8544921875\n",
      "iteration=4 loss=3226.01025390625\n",
      "iteration=5 loss=3073.938232421875\n",
      "iteration=6 loss=3000.66064453125\n",
      "iteration=7 loss=2955.044921875\n",
      "iteration=8 loss=2894.46435546875\n",
      "iteration=9 loss=2796.409423828125\n",
      "iteration=9 loss=2796.409423828125\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=1 loss=5610.2197265625\n",
      "iteration=2 loss=4271.1875\n",
      "iteration=3 loss=3498.073486328125\n",
      "iteration=4 loss=3168.522216796875\n",
      "iteration=5 loss=3115.293212890625\n",
      "iteration=6 loss=3187.423583984375\n",
      "iteration=7 loss=3229.34814453125\n",
      "iteration=8 loss=3134.40625\n",
      "iteration=9 loss=2907.79736328125\n",
      "iteration=9 loss=2907.79736328125\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=1 loss=5636.05517578125\n",
      "iteration=2 loss=4333.31689453125\n",
      "iteration=3 loss=3550.0322265625\n",
      "iteration=4 loss=3179.023193359375\n",
      "iteration=5 loss=3051.247314453125\n",
      "iteration=6 loss=3035.751953125\n",
      "iteration=7 loss=3033.904052734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=8 loss=2982.943359375\n",
      "iteration=9 loss=2863.72216796875\n",
      "iteration=9 loss=2863.72216796875\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=1 loss=6476.904296875\n",
      "iteration=2 loss=4289.482421875\n",
      "iteration=3 loss=3394.206298828125\n",
      "iteration=4 loss=3155.933837890625\n",
      "iteration=5 loss=3239.47509765625\n",
      "iteration=6 loss=3395.568359375\n",
      "iteration=7 loss=3388.4248046875\n",
      "iteration=8 loss=3244.39990234375\n",
      "iteration=9 loss=3007.44970703125\n",
      "iteration=9 loss=3007.44970703125\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=1 loss=6776.751953125\n",
      "iteration=2 loss=4109.3974609375\n",
      "iteration=3 loss=3279.71923828125\n",
      "iteration=4 loss=3202.09814453125\n",
      "iteration=5 loss=3462.451416015625\n",
      "iteration=6 loss=3759.118896484375\n",
      "iteration=7 loss=3729.509521484375\n",
      "iteration=8 loss=3461.197265625\n",
      "iteration=9 loss=3091.236328125\n",
      "iteration=9 loss=3091.236328125\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=1 loss=7627.36376953125\n",
      "iteration=2 loss=4756.70458984375\n",
      "iteration=3 loss=3683.372802734375\n",
      "iteration=4 loss=3371.4580078125\n",
      "iteration=5 loss=3420.547119140625\n",
      "iteration=6 loss=3518.11669921875\n",
      "iteration=7 loss=3435.279296875\n",
      "iteration=8 loss=3230.638916015625\n",
      "iteration=9 loss=2953.847412109375\n",
      "iteration=9 loss=2953.847412109375\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=1 loss=5688.439453125\n",
      "iteration=2 loss=4321.1201171875\n",
      "iteration=3 loss=3478.000244140625\n",
      "iteration=4 loss=3031.965087890625\n",
      "iteration=5 loss=2831.9951171875\n",
      "iteration=6 loss=2772.63623046875\n",
      "iteration=7 loss=2763.76416015625\n",
      "iteration=8 loss=2711.339599609375\n",
      "iteration=9 loss=2596.434814453125\n",
      "iteration=9 loss=2596.434814453125\n",
      "Accuracy: 0.149\n",
      "F1: 0.679\n",
      "Recall: 0.848\n",
      "Precision: 0.566\n",
      "Accuracy: 0.013\n",
      "F1: 0.042\n",
      "Recall: 0.074\n",
      "Precision: 0.029\n",
      "\n",
      "Accuracy: 0.151\n",
      "F1: 0.695\n",
      "Recall: 0.856\n",
      "Precision: 0.585\n",
      "Accuracy: 0.023\n",
      "F1: 0.060\n",
      "Recall: 0.133\n",
      "Precision: 0.039\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.443\n",
      "Recall: 0.885\n",
      "Precision: 0.295\n",
      "Accuracy: 0.016\n",
      "F1: 0.077\n",
      "Recall: 0.092\n",
      "Precision: 0.066\n",
      "\n",
      "Accuracy: 0.149\n",
      "F1: 0.798\n",
      "Recall: 0.849\n",
      "Precision: 0.753\n",
      "Accuracy: 0.024\n",
      "F1: 0.049\n",
      "Recall: 0.134\n",
      "Precision: 0.030\n",
      "\n",
      "Accuracy: 0.127\n",
      "F1: 0.635\n",
      "Recall: 0.723\n",
      "Precision: 0.566\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.008\n",
      "Precision: 0.004\n",
      "\n",
      "Accuracy: 0.153\n",
      "F1: 0.590\n",
      "Recall: 0.869\n",
      "Precision: 0.447\n",
      "Accuracy: 0.008\n",
      "F1: 0.052\n",
      "Recall: 0.044\n",
      "Precision: 0.063\n",
      "\n",
      "Accuracy: 0.138\n",
      "F1: 0.413\n",
      "Recall: 0.785\n",
      "Precision: 0.280\n",
      "Accuracy: 0.064\n",
      "F1: 0.182\n",
      "Recall: 0.362\n",
      "Precision: 0.121\n",
      "\n",
      "Accuracy: 0.149\n",
      "F1: 0.387\n",
      "Recall: 0.845\n",
      "Precision: 0.251\n",
      "Accuracy: 0.002\n",
      "F1: 0.012\n",
      "Recall: 0.012\n",
      "Precision: 0.012\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.370\n",
      "Recall: 0.888\n",
      "Precision: 0.233\n",
      "Accuracy: 0.018\n",
      "F1: 0.075\n",
      "Recall: 0.100\n",
      "Precision: 0.059\n",
      "\n",
      "Accuracy: 0.170\n",
      "F1: 0.708\n",
      "Recall: 0.964\n",
      "Precision: 0.560\n",
      "Accuracy: 0.005\n",
      "F1: 0.010\n",
      "Recall: 0.026\n",
      "Precision: 0.006\n",
      "\n",
      "10 1e-06\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=1 loss=5632.9453125\n",
      "iteration=2 loss=4618.89990234375\n",
      "iteration=3 loss=3857.818603515625\n",
      "iteration=4 loss=3410.666748046875\n",
      "iteration=5 loss=3200.419189453125\n",
      "iteration=6 loss=3128.30224609375\n",
      "iteration=7 loss=3067.4853515625\n",
      "iteration=8 loss=2949.888916015625\n",
      "iteration=9 loss=2828.172607421875\n",
      "iteration=9 loss=2828.172607421875\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=1 loss=5822.919921875\n",
      "iteration=2 loss=4285.66455078125\n",
      "iteration=3 loss=3382.89453125\n",
      "iteration=4 loss=2981.481689453125\n",
      "iteration=5 loss=2874.3857421875\n",
      "iteration=6 loss=2885.357177734375\n",
      "iteration=7 loss=2813.029296875\n",
      "iteration=8 loss=2693.588134765625\n",
      "iteration=9 loss=2532.54248046875\n",
      "iteration=9 loss=2532.54248046875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=1 loss=5959.0322265625\n",
      "iteration=2 loss=4745.20166015625\n",
      "iteration=3 loss=3887.12255859375\n",
      "iteration=4 loss=3390.8017578125\n",
      "iteration=5 loss=3163.537109375\n",
      "iteration=6 loss=3107.765869140625\n",
      "iteration=7 loss=3117.98388671875\n",
      "iteration=8 loss=3094.12451171875\n",
      "iteration=9 loss=2981.397705078125\n",
      "iteration=9 loss=2981.397705078125\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=1 loss=4493.43408203125\n",
      "iteration=2 loss=3943.192626953125\n",
      "iteration=3 loss=3505.8544921875\n",
      "iteration=4 loss=3226.01025390625\n",
      "iteration=5 loss=3073.938232421875\n",
      "iteration=6 loss=3000.66064453125\n",
      "iteration=7 loss=2955.044921875\n",
      "iteration=8 loss=2894.46435546875\n",
      "iteration=9 loss=2796.409423828125\n",
      "iteration=9 loss=2796.409423828125\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=1 loss=5610.2197265625\n",
      "iteration=2 loss=4271.1875\n",
      "iteration=3 loss=3498.073486328125\n",
      "iteration=4 loss=3168.522216796875\n",
      "iteration=5 loss=3115.293212890625\n",
      "iteration=6 loss=3187.423583984375\n",
      "iteration=7 loss=3229.34814453125\n",
      "iteration=8 loss=3134.40625\n",
      "iteration=9 loss=2907.79736328125\n",
      "iteration=9 loss=2907.79736328125\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=1 loss=5636.05517578125\n",
      "iteration=2 loss=4333.31689453125\n",
      "iteration=3 loss=3550.0322265625\n",
      "iteration=4 loss=3179.023193359375\n",
      "iteration=5 loss=3051.247314453125\n",
      "iteration=6 loss=3035.751953125\n",
      "iteration=7 loss=3033.904052734375\n",
      "iteration=8 loss=2982.943359375\n",
      "iteration=9 loss=2863.72216796875\n",
      "iteration=9 loss=2863.72216796875\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=1 loss=6476.904296875\n",
      "iteration=2 loss=4289.482421875\n",
      "iteration=3 loss=3394.206298828125\n",
      "iteration=4 loss=3155.933837890625\n",
      "iteration=5 loss=3239.47509765625\n",
      "iteration=6 loss=3395.568359375\n",
      "iteration=7 loss=3388.4248046875\n",
      "iteration=8 loss=3244.39990234375\n",
      "iteration=9 loss=3007.44970703125\n",
      "iteration=9 loss=3007.44970703125\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=1 loss=6776.751953125\n",
      "iteration=2 loss=4109.3974609375\n",
      "iteration=3 loss=3279.71923828125\n",
      "iteration=4 loss=3202.09814453125\n",
      "iteration=5 loss=3462.451416015625\n",
      "iteration=6 loss=3759.118896484375\n",
      "iteration=7 loss=3729.509521484375\n",
      "iteration=8 loss=3461.197265625\n",
      "iteration=9 loss=3091.236328125\n",
      "iteration=9 loss=3091.236328125\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=1 loss=7627.36376953125\n",
      "iteration=2 loss=4756.70458984375\n",
      "iteration=3 loss=3683.372802734375\n",
      "iteration=4 loss=3371.4580078125\n",
      "iteration=5 loss=3420.547119140625\n",
      "iteration=6 loss=3518.11669921875\n",
      "iteration=7 loss=3435.279296875\n",
      "iteration=8 loss=3230.638916015625\n",
      "iteration=9 loss=2953.847412109375\n",
      "iteration=9 loss=2953.847412109375\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=1 loss=5688.439453125\n",
      "iteration=2 loss=4321.1201171875\n",
      "iteration=3 loss=3478.000244140625\n",
      "iteration=4 loss=3031.965087890625\n",
      "iteration=5 loss=2831.9951171875\n",
      "iteration=6 loss=2772.63623046875\n",
      "iteration=7 loss=2763.76416015625\n",
      "iteration=8 loss=2711.339599609375\n",
      "iteration=9 loss=2596.434814453125\n",
      "iteration=9 loss=2596.434814453125\n",
      "Accuracy: 0.149\n",
      "F1: 0.679\n",
      "Recall: 0.848\n",
      "Precision: 0.566\n",
      "Accuracy: 0.013\n",
      "F1: 0.042\n",
      "Recall: 0.074\n",
      "Precision: 0.029\n",
      "\n",
      "Accuracy: 0.151\n",
      "F1: 0.695\n",
      "Recall: 0.856\n",
      "Precision: 0.585\n",
      "Accuracy: 0.023\n",
      "F1: 0.060\n",
      "Recall: 0.133\n",
      "Precision: 0.039\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.443\n",
      "Recall: 0.885\n",
      "Precision: 0.295\n",
      "Accuracy: 0.016\n",
      "F1: 0.077\n",
      "Recall: 0.092\n",
      "Precision: 0.066\n",
      "\n",
      "Accuracy: 0.149\n",
      "F1: 0.798\n",
      "Recall: 0.849\n",
      "Precision: 0.753\n",
      "Accuracy: 0.024\n",
      "F1: 0.049\n",
      "Recall: 0.134\n",
      "Precision: 0.030\n",
      "\n",
      "Accuracy: 0.127\n",
      "F1: 0.635\n",
      "Recall: 0.723\n",
      "Precision: 0.566\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.008\n",
      "Precision: 0.004\n",
      "\n",
      "Accuracy: 0.153\n",
      "F1: 0.590\n",
      "Recall: 0.869\n",
      "Precision: 0.447\n",
      "Accuracy: 0.008\n",
      "F1: 0.052\n",
      "Recall: 0.044\n",
      "Precision: 0.063\n",
      "\n",
      "Accuracy: 0.138\n",
      "F1: 0.413\n",
      "Recall: 0.785\n",
      "Precision: 0.280\n",
      "Accuracy: 0.064\n",
      "F1: 0.182\n",
      "Recall: 0.362\n",
      "Precision: 0.121\n",
      "\n",
      "Accuracy: 0.149\n",
      "F1: 0.387\n",
      "Recall: 0.845\n",
      "Precision: 0.251\n",
      "Accuracy: 0.002\n",
      "F1: 0.012\n",
      "Recall: 0.012\n",
      "Precision: 0.012\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.370\n",
      "Recall: 0.888\n",
      "Precision: 0.233\n",
      "Accuracy: 0.018\n",
      "F1: 0.075\n",
      "Recall: 0.100\n",
      "Precision: 0.059\n",
      "\n",
      "Accuracy: 0.170\n",
      "F1: 0.708\n",
      "Recall: 0.964\n",
      "Precision: 0.560\n",
      "Accuracy: 0.005\n",
      "F1: 0.010\n",
      "Recall: 0.026\n",
      "Precision: 0.006\n",
      "\n",
      "10 1e-07\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=1 loss=5632.9453125\n",
      "iteration=2 loss=4618.89990234375\n",
      "iteration=3 loss=3857.818603515625\n",
      "iteration=4 loss=3410.666748046875\n",
      "iteration=5 loss=3200.419189453125\n",
      "iteration=6 loss=3128.30224609375\n",
      "iteration=7 loss=3067.4853515625\n",
      "iteration=8 loss=2949.888916015625\n",
      "iteration=9 loss=2828.172607421875\n",
      "iteration=9 loss=2828.172607421875\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=1 loss=5822.919921875\n",
      "iteration=2 loss=4285.66455078125\n",
      "iteration=3 loss=3382.89453125\n",
      "iteration=4 loss=2981.481689453125\n",
      "iteration=5 loss=2874.3857421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=6 loss=2885.357177734375\n",
      "iteration=7 loss=2813.029296875\n",
      "iteration=8 loss=2693.588134765625\n",
      "iteration=9 loss=2532.54248046875\n",
      "iteration=9 loss=2532.54248046875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=1 loss=5959.0322265625\n",
      "iteration=2 loss=4745.20166015625\n",
      "iteration=3 loss=3887.12255859375\n",
      "iteration=4 loss=3390.8017578125\n",
      "iteration=5 loss=3163.537109375\n",
      "iteration=6 loss=3107.765869140625\n",
      "iteration=7 loss=3117.98388671875\n",
      "iteration=8 loss=3094.12451171875\n",
      "iteration=9 loss=2981.397705078125\n",
      "iteration=9 loss=2981.397705078125\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=1 loss=4493.43408203125\n",
      "iteration=2 loss=3943.192626953125\n",
      "iteration=3 loss=3505.8544921875\n",
      "iteration=4 loss=3226.01025390625\n",
      "iteration=5 loss=3073.938232421875\n",
      "iteration=6 loss=3000.66064453125\n",
      "iteration=7 loss=2955.044921875\n",
      "iteration=8 loss=2894.46435546875\n",
      "iteration=9 loss=2796.409423828125\n",
      "iteration=9 loss=2796.409423828125\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=1 loss=5610.2197265625\n",
      "iteration=2 loss=4271.1875\n",
      "iteration=3 loss=3498.073486328125\n",
      "iteration=4 loss=3168.522216796875\n",
      "iteration=5 loss=3115.293212890625\n",
      "iteration=6 loss=3187.423583984375\n",
      "iteration=7 loss=3229.34814453125\n",
      "iteration=8 loss=3134.40625\n",
      "iteration=9 loss=2907.79736328125\n",
      "iteration=9 loss=2907.79736328125\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=1 loss=5636.05517578125\n",
      "iteration=2 loss=4333.31689453125\n",
      "iteration=3 loss=3550.0322265625\n",
      "iteration=4 loss=3179.023193359375\n",
      "iteration=5 loss=3051.247314453125\n",
      "iteration=6 loss=3035.751953125\n",
      "iteration=7 loss=3033.904052734375\n",
      "iteration=8 loss=2982.943359375\n",
      "iteration=9 loss=2863.72216796875\n",
      "iteration=9 loss=2863.72216796875\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=1 loss=6476.904296875\n",
      "iteration=2 loss=4289.482421875\n",
      "iteration=3 loss=3394.206298828125\n",
      "iteration=4 loss=3155.933837890625\n",
      "iteration=5 loss=3239.47509765625\n",
      "iteration=6 loss=3395.568359375\n",
      "iteration=7 loss=3388.4248046875\n",
      "iteration=8 loss=3244.39990234375\n",
      "iteration=9 loss=3007.44970703125\n",
      "iteration=9 loss=3007.44970703125\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=1 loss=6776.751953125\n",
      "iteration=2 loss=4109.3974609375\n",
      "iteration=3 loss=3279.71923828125\n",
      "iteration=4 loss=3202.09814453125\n",
      "iteration=5 loss=3462.451416015625\n",
      "iteration=6 loss=3759.118896484375\n",
      "iteration=7 loss=3729.509521484375\n",
      "iteration=8 loss=3461.197265625\n",
      "iteration=9 loss=3091.236328125\n",
      "iteration=9 loss=3091.236328125\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=1 loss=7627.36376953125\n",
      "iteration=2 loss=4756.70458984375\n",
      "iteration=3 loss=3683.372802734375\n",
      "iteration=4 loss=3371.4580078125\n",
      "iteration=5 loss=3420.547119140625\n",
      "iteration=6 loss=3518.11669921875\n",
      "iteration=7 loss=3435.279296875\n",
      "iteration=8 loss=3230.638916015625\n",
      "iteration=9 loss=2953.847412109375\n",
      "iteration=9 loss=2953.847412109375\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=1 loss=5688.439453125\n",
      "iteration=2 loss=4321.1201171875\n",
      "iteration=3 loss=3478.000244140625\n",
      "iteration=4 loss=3031.965087890625\n",
      "iteration=5 loss=2831.9951171875\n",
      "iteration=6 loss=2772.63623046875\n",
      "iteration=7 loss=2763.76416015625\n",
      "iteration=8 loss=2711.339599609375\n",
      "iteration=9 loss=2596.434814453125\n",
      "iteration=9 loss=2596.434814453125\n",
      "Accuracy: 0.149\n",
      "F1: 0.679\n",
      "Recall: 0.848\n",
      "Precision: 0.566\n",
      "Accuracy: 0.013\n",
      "F1: 0.042\n",
      "Recall: 0.074\n",
      "Precision: 0.029\n",
      "\n",
      "Accuracy: 0.151\n",
      "F1: 0.695\n",
      "Recall: 0.856\n",
      "Precision: 0.585\n",
      "Accuracy: 0.023\n",
      "F1: 0.060\n",
      "Recall: 0.133\n",
      "Precision: 0.039\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.443\n",
      "Recall: 0.885\n",
      "Precision: 0.295\n",
      "Accuracy: 0.016\n",
      "F1: 0.077\n",
      "Recall: 0.092\n",
      "Precision: 0.066\n",
      "\n",
      "Accuracy: 0.149\n",
      "F1: 0.798\n",
      "Recall: 0.849\n",
      "Precision: 0.753\n",
      "Accuracy: 0.024\n",
      "F1: 0.049\n",
      "Recall: 0.134\n",
      "Precision: 0.030\n",
      "\n",
      "Accuracy: 0.127\n",
      "F1: 0.635\n",
      "Recall: 0.723\n",
      "Precision: 0.566\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.008\n",
      "Precision: 0.004\n",
      "\n",
      "Accuracy: 0.153\n",
      "F1: 0.590\n",
      "Recall: 0.869\n",
      "Precision: 0.447\n",
      "Accuracy: 0.008\n",
      "F1: 0.052\n",
      "Recall: 0.044\n",
      "Precision: 0.063\n",
      "\n",
      "Accuracy: 0.138\n",
      "F1: 0.413\n",
      "Recall: 0.785\n",
      "Precision: 0.280\n",
      "Accuracy: 0.064\n",
      "F1: 0.182\n",
      "Recall: 0.362\n",
      "Precision: 0.121\n",
      "\n",
      "Accuracy: 0.149\n",
      "F1: 0.387\n",
      "Recall: 0.845\n",
      "Precision: 0.251\n",
      "Accuracy: 0.002\n",
      "F1: 0.012\n",
      "Recall: 0.012\n",
      "Precision: 0.012\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.370\n",
      "Recall: 0.888\n",
      "Precision: 0.233\n",
      "Accuracy: 0.018\n",
      "F1: 0.075\n",
      "Recall: 0.100\n",
      "Precision: 0.059\n",
      "\n",
      "Accuracy: 0.170\n",
      "F1: 0.708\n",
      "Recall: 0.964\n",
      "Precision: 0.560\n",
      "Accuracy: 0.005\n",
      "F1: 0.010\n",
      "Recall: 0.026\n",
      "Precision: 0.006\n",
      "\n",
      "25 0.0001\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=2 loss=4618.89990234375\n",
      "iteration=4 loss=3410.666748046875\n",
      "iteration=6 loss=3128.30224609375\n",
      "iteration=8 loss=2949.888916015625\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=12 loss=2459.74658203125\n",
      "iteration=14 loss=2219.7265625\n",
      "iteration=16 loss=2031.896484375\n",
      "iteration=18 loss=1896.626708984375\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=22 loss=1736.6287841796875\n",
      "iteration=24 loss=1669.70166015625\n",
      "iteration=24 loss=1669.70166015625\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=2 loss=4285.66455078125\n",
      "iteration=4 loss=2981.481689453125\n",
      "iteration=6 loss=2885.357177734375\n",
      "iteration=8 loss=2693.588134765625\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=12 loss=1995.99072265625\n",
      "iteration=14 loss=1736.3631591796875\n",
      "iteration=16 loss=1581.7587890625\n",
      "iteration=18 loss=1503.8212890625\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=22 loss=1332.7711181640625\n",
      "iteration=24 loss=1264.9178466796875\n",
      "iteration=24 loss=1264.9178466796875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=2 loss=4745.20166015625\n",
      "iteration=4 loss=3390.8017578125\n",
      "iteration=6 loss=3107.765869140625\n",
      "iteration=8 loss=3094.12451171875\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=12 loss=2367.657470703125\n",
      "iteration=14 loss=2079.301513671875\n",
      "iteration=16 loss=1904.7030029296875\n",
      "iteration=18 loss=1769.861328125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=22 loss=1593.4652099609375\n",
      "iteration=24 loss=1511.18359375\n",
      "iteration=24 loss=1511.18359375\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=2 loss=3943.192626953125\n",
      "iteration=4 loss=3226.01025390625\n",
      "iteration=6 loss=3000.66064453125\n",
      "iteration=8 loss=2894.46435546875\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=12 loss=2361.0751953125\n",
      "iteration=14 loss=2112.454345703125\n",
      "iteration=16 loss=1933.751708984375\n",
      "iteration=18 loss=1799.790771484375\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=22 loss=1636.7718505859375\n",
      "iteration=24 loss=1555.408447265625\n",
      "iteration=24 loss=1555.408447265625\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=2 loss=4271.1875\n",
      "iteration=4 loss=3168.522216796875\n",
      "iteration=6 loss=3187.423583984375\n",
      "iteration=8 loss=3134.40625\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=12 loss=2239.02197265625\n",
      "iteration=14 loss=2081.58251953125\n",
      "iteration=16 loss=1999.05029296875\n",
      "iteration=18 loss=1894.8907470703125\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=22 loss=1652.212646484375\n",
      "iteration=24 loss=1567.056884765625\n",
      "iteration=24 loss=1567.056884765625\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=2 loss=4333.31689453125\n",
      "iteration=4 loss=3179.023193359375\n",
      "iteration=6 loss=3035.751953125\n",
      "iteration=8 loss=2982.943359375\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=12 loss=2330.686767578125\n",
      "iteration=14 loss=2070.18212890625\n",
      "iteration=16 loss=1918.8173828125\n",
      "iteration=18 loss=1828.7611083984375\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=22 loss=1682.695068359375\n",
      "iteration=24 loss=1592.2313232421875\n",
      "iteration=24 loss=1592.2313232421875\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=2 loss=4289.482421875\n",
      "iteration=4 loss=3155.933837890625\n",
      "iteration=6 loss=3395.568359375\n",
      "iteration=8 loss=3244.39990234375\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=12 loss=2327.548095703125\n",
      "iteration=14 loss=2144.0380859375\n",
      "iteration=16 loss=2079.00537109375\n",
      "iteration=18 loss=2014.098876953125\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=22 loss=1795.36767578125\n",
      "iteration=24 loss=1708.42041015625\n",
      "iteration=24 loss=1708.42041015625\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=2 loss=4109.3974609375\n",
      "iteration=4 loss=3202.09814453125\n",
      "iteration=6 loss=3759.118896484375\n",
      "iteration=8 loss=3461.197265625\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=12 loss=2239.464599609375\n",
      "iteration=14 loss=2072.904541015625\n",
      "iteration=16 loss=2018.91064453125\n",
      "iteration=18 loss=1936.46142578125\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=22 loss=1681.436279296875\n",
      "iteration=24 loss=1593.7056884765625\n",
      "iteration=24 loss=1593.7056884765625\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=12965.1884765625\n",
      "iteration=2 loss=4756.70458984375\n",
      "iteration=4 loss=3371.4580078125\n",
      "iteration=6 loss=3518.11669921875\n",
      "iteration=8 loss=3230.638916015625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=12 loss=2211.412109375\n",
      "iteration=14 loss=1969.341552734375\n",
      "iteration=16 loss=1852.2978515625\n",
      "iteration=18 loss=1773.5533447265625\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=22 loss=1581.428955078125\n",
      "iteration=24 loss=1479.546875\n",
      "iteration=24 loss=1479.546875\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=2 loss=4321.1201171875\n",
      "iteration=4 loss=3031.965087890625\n",
      "iteration=6 loss=2772.63623046875\n",
      "iteration=8 loss=2711.339599609375\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=12 loss=2020.8433837890625\n",
      "iteration=14 loss=1713.6357421875\n",
      "iteration=16 loss=1520.3199462890625\n",
      "iteration=18 loss=1386.8096923828125\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=22 loss=1220.9449462890625\n",
      "iteration=24 loss=1171.158935546875\n",
      "iteration=24 loss=1171.158935546875\n",
      "Accuracy: 0.157\n",
      "F1: 0.616\n",
      "Recall: 0.893\n",
      "Precision: 0.471\n",
      "Accuracy: 0.017\n",
      "F1: 0.087\n",
      "Recall: 0.096\n",
      "Precision: 0.079\n",
      "\n",
      "Accuracy: 0.134\n",
      "F1: 0.242\n",
      "Recall: 0.764\n",
      "Precision: 0.144\n",
      "Accuracy: 0.088\n",
      "F1: 0.601\n",
      "Recall: 0.503\n",
      "Precision: 0.747\n",
      "\n",
      "Accuracy: 0.158\n",
      "F1: 0.366\n",
      "Recall: 0.898\n",
      "Precision: 0.230\n",
      "Accuracy: 0.029\n",
      "F1: 0.251\n",
      "Recall: 0.167\n",
      "Precision: 0.505\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.758\n",
      "Recall: 0.885\n",
      "Precision: 0.663\n",
      "Accuracy: 0.022\n",
      "F1: 0.057\n",
      "Recall: 0.123\n",
      "Precision: 0.037\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.586\n",
      "Recall: 0.711\n",
      "Precision: 0.498\n",
      "Accuracy: 0.003\n",
      "F1: 0.028\n",
      "Recall: 0.016\n",
      "Precision: 0.104\n",
      "\n",
      "Accuracy: 0.123\n",
      "F1: 0.312\n",
      "Recall: 0.697\n",
      "Precision: 0.201\n",
      "Accuracy: 0.113\n",
      "F1: 0.432\n",
      "Recall: 0.643\n",
      "Precision: 0.325\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.354\n",
      "Recall: 0.663\n",
      "Precision: 0.241\n",
      "Accuracy: 0.098\n",
      "F1: 0.340\n",
      "Recall: 0.557\n",
      "Precision: 0.245\n",
      "\n",
      "Accuracy: 0.106\n",
      "F1: 0.243\n",
      "Recall: 0.602\n",
      "Precision: 0.152\n",
      "Accuracy: 0.102\n",
      "F1: 0.673\n",
      "Recall: 0.580\n",
      "Precision: 0.803\n",
      "\n",
      "Accuracy: 0.056\n",
      "F1: 0.110\n",
      "Recall: 0.320\n",
      "Precision: 0.067\n",
      "Accuracy: 0.141\n",
      "F1: 0.823\n",
      "Recall: 0.803\n",
      "Precision: 0.844\n",
      "\n",
      "Accuracy: 0.166\n",
      "F1: 0.432\n",
      "Recall: 0.946\n",
      "Precision: 0.280\n",
      "Accuracy: 0.033\n",
      "F1: 0.136\n",
      "Recall: 0.188\n",
      "Precision: 0.107\n",
      "\n",
      "25 1e-05\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=2 loss=4618.89990234375\n",
      "iteration=4 loss=3410.666748046875\n",
      "iteration=6 loss=3128.30224609375\n",
      "iteration=8 loss=2949.888916015625\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=12 loss=2459.74658203125\n",
      "iteration=14 loss=2219.7265625\n",
      "iteration=16 loss=2031.896484375\n",
      "iteration=18 loss=1896.626708984375\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=22 loss=1736.6287841796875\n",
      "iteration=24 loss=1669.70166015625\n",
      "iteration=24 loss=1669.70166015625\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=2 loss=4285.66455078125\n",
      "iteration=4 loss=2981.481689453125\n",
      "iteration=6 loss=2885.357177734375\n",
      "iteration=8 loss=2693.588134765625\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=12 loss=1995.99072265625\n",
      "iteration=14 loss=1736.3631591796875\n",
      "iteration=16 loss=1581.7587890625\n",
      "iteration=18 loss=1503.8212890625\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=22 loss=1332.7711181640625\n",
      "iteration=24 loss=1264.9178466796875\n",
      "iteration=24 loss=1264.9178466796875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=2 loss=4745.20166015625\n",
      "iteration=4 loss=3390.8017578125\n",
      "iteration=6 loss=3107.765869140625\n",
      "iteration=8 loss=3094.12451171875\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=12 loss=2367.657470703125\n",
      "iteration=14 loss=2079.301513671875\n",
      "iteration=16 loss=1904.7030029296875\n",
      "iteration=18 loss=1769.861328125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=22 loss=1593.4652099609375\n",
      "iteration=24 loss=1511.18359375\n",
      "iteration=24 loss=1511.18359375\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=2 loss=3943.192626953125\n",
      "iteration=4 loss=3226.01025390625\n",
      "iteration=6 loss=3000.66064453125\n",
      "iteration=8 loss=2894.46435546875\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=12 loss=2361.0751953125\n",
      "iteration=14 loss=2112.454345703125\n",
      "iteration=16 loss=1933.751708984375\n",
      "iteration=18 loss=1799.790771484375\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=22 loss=1636.7718505859375\n",
      "iteration=24 loss=1555.408447265625\n",
      "iteration=24 loss=1555.408447265625\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=2 loss=4271.1875\n",
      "iteration=4 loss=3168.522216796875\n",
      "iteration=6 loss=3187.423583984375\n",
      "iteration=8 loss=3134.40625\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=12 loss=2239.02197265625\n",
      "iteration=14 loss=2081.58251953125\n",
      "iteration=16 loss=1999.05029296875\n",
      "iteration=18 loss=1894.8907470703125\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=22 loss=1652.212646484375\n",
      "iteration=24 loss=1567.056884765625\n",
      "iteration=24 loss=1567.056884765625\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=2 loss=4333.31689453125\n",
      "iteration=4 loss=3179.023193359375\n",
      "iteration=6 loss=3035.751953125\n",
      "iteration=8 loss=2982.943359375\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=12 loss=2330.686767578125\n",
      "iteration=14 loss=2070.18212890625\n",
      "iteration=16 loss=1918.8173828125\n",
      "iteration=18 loss=1828.7611083984375\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=22 loss=1682.695068359375\n",
      "iteration=24 loss=1592.2313232421875\n",
      "iteration=24 loss=1592.2313232421875\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=2 loss=4289.482421875\n",
      "iteration=4 loss=3155.933837890625\n",
      "iteration=6 loss=3395.568359375\n",
      "iteration=8 loss=3244.39990234375\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=12 loss=2327.548095703125\n",
      "iteration=14 loss=2144.0380859375\n",
      "iteration=16 loss=2079.00537109375\n",
      "iteration=18 loss=2014.098876953125\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=22 loss=1795.36767578125\n",
      "iteration=24 loss=1708.42041015625\n",
      "iteration=24 loss=1708.42041015625\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=2 loss=4109.3974609375\n",
      "iteration=4 loss=3202.09814453125\n",
      "iteration=6 loss=3759.118896484375\n",
      "iteration=8 loss=3461.197265625\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=12 loss=2239.464599609375\n",
      "iteration=14 loss=2072.904541015625\n",
      "iteration=16 loss=2018.91064453125\n",
      "iteration=18 loss=1936.46142578125\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=22 loss=1681.436279296875\n",
      "iteration=24 loss=1593.7056884765625\n",
      "iteration=24 loss=1593.7056884765625\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=2 loss=4756.70458984375\n",
      "iteration=4 loss=3371.4580078125\n",
      "iteration=6 loss=3518.11669921875\n",
      "iteration=8 loss=3230.638916015625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=12 loss=2211.412109375\n",
      "iteration=14 loss=1969.341552734375\n",
      "iteration=16 loss=1852.2978515625\n",
      "iteration=18 loss=1773.5533447265625\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=22 loss=1581.428955078125\n",
      "iteration=24 loss=1479.546875\n",
      "iteration=24 loss=1479.546875\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=2 loss=4321.1201171875\n",
      "iteration=4 loss=3031.965087890625\n",
      "iteration=6 loss=2772.63623046875\n",
      "iteration=8 loss=2711.339599609375\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=12 loss=2020.8433837890625\n",
      "iteration=14 loss=1713.6357421875\n",
      "iteration=16 loss=1520.3199462890625\n",
      "iteration=18 loss=1386.8096923828125\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=22 loss=1220.9449462890625\n",
      "iteration=24 loss=1171.158935546875\n",
      "iteration=24 loss=1171.158935546875\n",
      "Accuracy: 0.157\n",
      "F1: 0.616\n",
      "Recall: 0.893\n",
      "Precision: 0.471\n",
      "Accuracy: 0.017\n",
      "F1: 0.087\n",
      "Recall: 0.096\n",
      "Precision: 0.079\n",
      "\n",
      "Accuracy: 0.134\n",
      "F1: 0.242\n",
      "Recall: 0.764\n",
      "Precision: 0.144\n",
      "Accuracy: 0.088\n",
      "F1: 0.601\n",
      "Recall: 0.503\n",
      "Precision: 0.747\n",
      "\n",
      "Accuracy: 0.158\n",
      "F1: 0.366\n",
      "Recall: 0.898\n",
      "Precision: 0.230\n",
      "Accuracy: 0.029\n",
      "F1: 0.251\n",
      "Recall: 0.167\n",
      "Precision: 0.505\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.758\n",
      "Recall: 0.885\n",
      "Precision: 0.663\n",
      "Accuracy: 0.022\n",
      "F1: 0.057\n",
      "Recall: 0.123\n",
      "Precision: 0.037\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.586\n",
      "Recall: 0.711\n",
      "Precision: 0.498\n",
      "Accuracy: 0.003\n",
      "F1: 0.028\n",
      "Recall: 0.016\n",
      "Precision: 0.104\n",
      "\n",
      "Accuracy: 0.123\n",
      "F1: 0.312\n",
      "Recall: 0.697\n",
      "Precision: 0.201\n",
      "Accuracy: 0.113\n",
      "F1: 0.432\n",
      "Recall: 0.643\n",
      "Precision: 0.325\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.354\n",
      "Recall: 0.663\n",
      "Precision: 0.241\n",
      "Accuracy: 0.098\n",
      "F1: 0.340\n",
      "Recall: 0.557\n",
      "Precision: 0.245\n",
      "\n",
      "Accuracy: 0.106\n",
      "F1: 0.243\n",
      "Recall: 0.602\n",
      "Precision: 0.152\n",
      "Accuracy: 0.102\n",
      "F1: 0.673\n",
      "Recall: 0.580\n",
      "Precision: 0.803\n",
      "\n",
      "Accuracy: 0.056\n",
      "F1: 0.110\n",
      "Recall: 0.320\n",
      "Precision: 0.067\n",
      "Accuracy: 0.141\n",
      "F1: 0.823\n",
      "Recall: 0.803\n",
      "Precision: 0.844\n",
      "\n",
      "Accuracy: 0.166\n",
      "F1: 0.432\n",
      "Recall: 0.946\n",
      "Precision: 0.280\n",
      "Accuracy: 0.033\n",
      "F1: 0.136\n",
      "Recall: 0.188\n",
      "Precision: 0.107\n",
      "\n",
      "25 1e-06\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2 loss=4618.89990234375\n",
      "iteration=4 loss=3410.666748046875\n",
      "iteration=6 loss=3128.30224609375\n",
      "iteration=8 loss=2949.888916015625\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=12 loss=2459.74658203125\n",
      "iteration=14 loss=2219.7265625\n",
      "iteration=16 loss=2031.896484375\n",
      "iteration=18 loss=1896.626708984375\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=22 loss=1736.6287841796875\n",
      "iteration=24 loss=1669.70166015625\n",
      "iteration=24 loss=1669.70166015625\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=2 loss=4285.66455078125\n",
      "iteration=4 loss=2981.481689453125\n",
      "iteration=6 loss=2885.357177734375\n",
      "iteration=8 loss=2693.588134765625\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=12 loss=1995.99072265625\n",
      "iteration=14 loss=1736.3631591796875\n",
      "iteration=16 loss=1581.7587890625\n",
      "iteration=18 loss=1503.8212890625\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=22 loss=1332.7711181640625\n",
      "iteration=24 loss=1264.9178466796875\n",
      "iteration=24 loss=1264.9178466796875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=2 loss=4745.20166015625\n",
      "iteration=4 loss=3390.8017578125\n",
      "iteration=6 loss=3107.765869140625\n",
      "iteration=8 loss=3094.12451171875\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=12 loss=2367.657470703125\n",
      "iteration=14 loss=2079.301513671875\n",
      "iteration=16 loss=1904.7030029296875\n",
      "iteration=18 loss=1769.861328125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=22 loss=1593.4652099609375\n",
      "iteration=24 loss=1511.18359375\n",
      "iteration=24 loss=1511.18359375\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=2 loss=3943.192626953125\n",
      "iteration=4 loss=3226.01025390625\n",
      "iteration=6 loss=3000.66064453125\n",
      "iteration=8 loss=2894.46435546875\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=12 loss=2361.0751953125\n",
      "iteration=14 loss=2112.454345703125\n",
      "iteration=16 loss=1933.751708984375\n",
      "iteration=18 loss=1799.790771484375\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=22 loss=1636.7718505859375\n",
      "iteration=24 loss=1555.408447265625\n",
      "iteration=24 loss=1555.408447265625\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=2 loss=4271.1875\n",
      "iteration=4 loss=3168.522216796875\n",
      "iteration=6 loss=3187.423583984375\n",
      "iteration=8 loss=3134.40625\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=12 loss=2239.02197265625\n",
      "iteration=14 loss=2081.58251953125\n",
      "iteration=16 loss=1999.05029296875\n",
      "iteration=18 loss=1894.8907470703125\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=22 loss=1652.212646484375\n",
      "iteration=24 loss=1567.056884765625\n",
      "iteration=24 loss=1567.056884765625\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=2 loss=4333.31689453125\n",
      "iteration=4 loss=3179.023193359375\n",
      "iteration=6 loss=3035.751953125\n",
      "iteration=8 loss=2982.943359375\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=12 loss=2330.686767578125\n",
      "iteration=14 loss=2070.18212890625\n",
      "iteration=16 loss=1918.8173828125\n",
      "iteration=18 loss=1828.7611083984375\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=22 loss=1682.695068359375\n",
      "iteration=24 loss=1592.2313232421875\n",
      "iteration=24 loss=1592.2313232421875\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=2 loss=4289.482421875\n",
      "iteration=4 loss=3155.933837890625\n",
      "iteration=6 loss=3395.568359375\n",
      "iteration=8 loss=3244.39990234375\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=12 loss=2327.548095703125\n",
      "iteration=14 loss=2144.0380859375\n",
      "iteration=16 loss=2079.00537109375\n",
      "iteration=18 loss=2014.098876953125\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=22 loss=1795.36767578125\n",
      "iteration=24 loss=1708.42041015625\n",
      "iteration=24 loss=1708.42041015625\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=2 loss=4109.3974609375\n",
      "iteration=4 loss=3202.09814453125\n",
      "iteration=6 loss=3759.118896484375\n",
      "iteration=8 loss=3461.197265625\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=12 loss=2239.464599609375\n",
      "iteration=14 loss=2072.904541015625\n",
      "iteration=16 loss=2018.91064453125\n",
      "iteration=18 loss=1936.46142578125\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=22 loss=1681.436279296875\n",
      "iteration=24 loss=1593.7056884765625\n",
      "iteration=24 loss=1593.7056884765625\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=2 loss=4756.70458984375\n",
      "iteration=4 loss=3371.4580078125\n",
      "iteration=6 loss=3518.11669921875\n",
      "iteration=8 loss=3230.638916015625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=12 loss=2211.412109375\n",
      "iteration=14 loss=1969.341552734375\n",
      "iteration=16 loss=1852.2978515625\n",
      "iteration=18 loss=1773.5533447265625\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=22 loss=1581.428955078125\n",
      "iteration=24 loss=1479.546875\n",
      "iteration=24 loss=1479.546875\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=2 loss=4321.1201171875\n",
      "iteration=4 loss=3031.965087890625\n",
      "iteration=6 loss=2772.63623046875\n",
      "iteration=8 loss=2711.339599609375\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=12 loss=2020.8433837890625\n",
      "iteration=14 loss=1713.6357421875\n",
      "iteration=16 loss=1520.3199462890625\n",
      "iteration=18 loss=1386.8096923828125\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=22 loss=1220.9449462890625\n",
      "iteration=24 loss=1171.158935546875\n",
      "iteration=24 loss=1171.158935546875\n",
      "Accuracy: 0.157\n",
      "F1: 0.616\n",
      "Recall: 0.893\n",
      "Precision: 0.471\n",
      "Accuracy: 0.017\n",
      "F1: 0.087\n",
      "Recall: 0.096\n",
      "Precision: 0.079\n",
      "\n",
      "Accuracy: 0.134\n",
      "F1: 0.242\n",
      "Recall: 0.764\n",
      "Precision: 0.144\n",
      "Accuracy: 0.088\n",
      "F1: 0.601\n",
      "Recall: 0.503\n",
      "Precision: 0.747\n",
      "\n",
      "Accuracy: 0.158\n",
      "F1: 0.366\n",
      "Recall: 0.898\n",
      "Precision: 0.230\n",
      "Accuracy: 0.029\n",
      "F1: 0.251\n",
      "Recall: 0.167\n",
      "Precision: 0.505\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.758\n",
      "Recall: 0.885\n",
      "Precision: 0.663\n",
      "Accuracy: 0.022\n",
      "F1: 0.057\n",
      "Recall: 0.123\n",
      "Precision: 0.037\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.586\n",
      "Recall: 0.711\n",
      "Precision: 0.498\n",
      "Accuracy: 0.003\n",
      "F1: 0.028\n",
      "Recall: 0.016\n",
      "Precision: 0.104\n",
      "\n",
      "Accuracy: 0.123\n",
      "F1: 0.312\n",
      "Recall: 0.697\n",
      "Precision: 0.201\n",
      "Accuracy: 0.113\n",
      "F1: 0.432\n",
      "Recall: 0.643\n",
      "Precision: 0.325\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.354\n",
      "Recall: 0.663\n",
      "Precision: 0.241\n",
      "Accuracy: 0.098\n",
      "F1: 0.340\n",
      "Recall: 0.557\n",
      "Precision: 0.245\n",
      "\n",
      "Accuracy: 0.106\n",
      "F1: 0.243\n",
      "Recall: 0.602\n",
      "Precision: 0.152\n",
      "Accuracy: 0.102\n",
      "F1: 0.673\n",
      "Recall: 0.580\n",
      "Precision: 0.803\n",
      "\n",
      "Accuracy: 0.056\n",
      "F1: 0.110\n",
      "Recall: 0.320\n",
      "Precision: 0.067\n",
      "Accuracy: 0.141\n",
      "F1: 0.823\n",
      "Recall: 0.803\n",
      "Precision: 0.844\n",
      "\n",
      "Accuracy: 0.166\n",
      "F1: 0.432\n",
      "Recall: 0.946\n",
      "Precision: 0.280\n",
      "Accuracy: 0.033\n",
      "F1: 0.136\n",
      "Recall: 0.188\n",
      "Precision: 0.107\n",
      "\n",
      "25 1e-07\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=2 loss=4618.89990234375\n",
      "iteration=4 loss=3410.666748046875\n",
      "iteration=6 loss=3128.30224609375\n",
      "iteration=8 loss=2949.888916015625\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=12 loss=2459.74658203125\n",
      "iteration=14 loss=2219.7265625\n",
      "iteration=16 loss=2031.896484375\n",
      "iteration=18 loss=1896.626708984375\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=22 loss=1736.6287841796875\n",
      "iteration=24 loss=1669.70166015625\n",
      "iteration=24 loss=1669.70166015625\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=2 loss=4285.66455078125\n",
      "iteration=4 loss=2981.481689453125\n",
      "iteration=6 loss=2885.357177734375\n",
      "iteration=8 loss=2693.588134765625\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=12 loss=1995.99072265625\n",
      "iteration=14 loss=1736.3631591796875\n",
      "iteration=16 loss=1581.7587890625\n",
      "iteration=18 loss=1503.8212890625\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=22 loss=1332.7711181640625\n",
      "iteration=24 loss=1264.9178466796875\n",
      "iteration=24 loss=1264.9178466796875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=2 loss=4745.20166015625\n",
      "iteration=4 loss=3390.8017578125\n",
      "iteration=6 loss=3107.765869140625\n",
      "iteration=8 loss=3094.12451171875\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=12 loss=2367.657470703125\n",
      "iteration=14 loss=2079.301513671875\n",
      "iteration=16 loss=1904.7030029296875\n",
      "iteration=18 loss=1769.861328125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=22 loss=1593.4652099609375\n",
      "iteration=24 loss=1511.18359375\n",
      "iteration=24 loss=1511.18359375\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=2 loss=3943.192626953125\n",
      "iteration=4 loss=3226.01025390625\n",
      "iteration=6 loss=3000.66064453125\n",
      "iteration=8 loss=2894.46435546875\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=12 loss=2361.0751953125\n",
      "iteration=14 loss=2112.454345703125\n",
      "iteration=16 loss=1933.751708984375\n",
      "iteration=18 loss=1799.790771484375\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=22 loss=1636.7718505859375\n",
      "iteration=24 loss=1555.408447265625\n",
      "iteration=24 loss=1555.408447265625\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=2 loss=4271.1875\n",
      "iteration=4 loss=3168.522216796875\n",
      "iteration=6 loss=3187.423583984375\n",
      "iteration=8 loss=3134.40625\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=12 loss=2239.02197265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=14 loss=2081.58251953125\n",
      "iteration=16 loss=1999.05029296875\n",
      "iteration=18 loss=1894.8907470703125\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=22 loss=1652.212646484375\n",
      "iteration=24 loss=1567.056884765625\n",
      "iteration=24 loss=1567.056884765625\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=2 loss=4333.31689453125\n",
      "iteration=4 loss=3179.023193359375\n",
      "iteration=6 loss=3035.751953125\n",
      "iteration=8 loss=2982.943359375\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=12 loss=2330.686767578125\n",
      "iteration=14 loss=2070.18212890625\n",
      "iteration=16 loss=1918.8173828125\n",
      "iteration=18 loss=1828.7611083984375\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=22 loss=1682.695068359375\n",
      "iteration=24 loss=1592.2313232421875\n",
      "iteration=24 loss=1592.2313232421875\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=2 loss=4289.482421875\n",
      "iteration=4 loss=3155.933837890625\n",
      "iteration=6 loss=3395.568359375\n",
      "iteration=8 loss=3244.39990234375\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=12 loss=2327.548095703125\n",
      "iteration=14 loss=2144.0380859375\n",
      "iteration=16 loss=2079.00537109375\n",
      "iteration=18 loss=2014.098876953125\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=22 loss=1795.36767578125\n",
      "iteration=24 loss=1708.42041015625\n",
      "iteration=24 loss=1708.42041015625\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=2 loss=4109.3974609375\n",
      "iteration=4 loss=3202.09814453125\n",
      "iteration=6 loss=3759.118896484375\n",
      "iteration=8 loss=3461.197265625\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=12 loss=2239.464599609375\n",
      "iteration=14 loss=2072.904541015625\n",
      "iteration=16 loss=2018.91064453125\n",
      "iteration=18 loss=1936.46142578125\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=22 loss=1681.436279296875\n",
      "iteration=24 loss=1593.7056884765625\n",
      "iteration=24 loss=1593.7056884765625\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=2 loss=4756.70458984375\n",
      "iteration=4 loss=3371.4580078125\n",
      "iteration=6 loss=3518.11669921875\n",
      "iteration=8 loss=3230.638916015625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=12 loss=2211.412109375\n",
      "iteration=14 loss=1969.341552734375\n",
      "iteration=16 loss=1852.2978515625\n",
      "iteration=18 loss=1773.5533447265625\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=22 loss=1581.428955078125\n",
      "iteration=24 loss=1479.546875\n",
      "iteration=24 loss=1479.546875\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=2 loss=4321.1201171875\n",
      "iteration=4 loss=3031.965087890625\n",
      "iteration=6 loss=2772.63623046875\n",
      "iteration=8 loss=2711.339599609375\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=12 loss=2020.8433837890625\n",
      "iteration=14 loss=1713.6357421875\n",
      "iteration=16 loss=1520.3199462890625\n",
      "iteration=18 loss=1386.8096923828125\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=22 loss=1220.9449462890625\n",
      "iteration=24 loss=1171.158935546875\n",
      "iteration=24 loss=1171.158935546875\n",
      "Accuracy: 0.157\n",
      "F1: 0.616\n",
      "Recall: 0.893\n",
      "Precision: 0.471\n",
      "Accuracy: 0.017\n",
      "F1: 0.087\n",
      "Recall: 0.096\n",
      "Precision: 0.079\n",
      "\n",
      "Accuracy: 0.134\n",
      "F1: 0.242\n",
      "Recall: 0.764\n",
      "Precision: 0.144\n",
      "Accuracy: 0.088\n",
      "F1: 0.601\n",
      "Recall: 0.503\n",
      "Precision: 0.747\n",
      "\n",
      "Accuracy: 0.158\n",
      "F1: 0.366\n",
      "Recall: 0.898\n",
      "Precision: 0.230\n",
      "Accuracy: 0.029\n",
      "F1: 0.251\n",
      "Recall: 0.167\n",
      "Precision: 0.505\n",
      "\n",
      "Accuracy: 0.156\n",
      "F1: 0.758\n",
      "Recall: 0.885\n",
      "Precision: 0.663\n",
      "Accuracy: 0.022\n",
      "F1: 0.057\n",
      "Recall: 0.123\n",
      "Precision: 0.037\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.586\n",
      "Recall: 0.711\n",
      "Precision: 0.498\n",
      "Accuracy: 0.003\n",
      "F1: 0.028\n",
      "Recall: 0.016\n",
      "Precision: 0.104\n",
      "\n",
      "Accuracy: 0.123\n",
      "F1: 0.312\n",
      "Recall: 0.697\n",
      "Precision: 0.201\n",
      "Accuracy: 0.113\n",
      "F1: 0.432\n",
      "Recall: 0.643\n",
      "Precision: 0.325\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.354\n",
      "Recall: 0.663\n",
      "Precision: 0.241\n",
      "Accuracy: 0.098\n",
      "F1: 0.340\n",
      "Recall: 0.557\n",
      "Precision: 0.245\n",
      "\n",
      "Accuracy: 0.106\n",
      "F1: 0.243\n",
      "Recall: 0.602\n",
      "Precision: 0.152\n",
      "Accuracy: 0.102\n",
      "F1: 0.673\n",
      "Recall: 0.580\n",
      "Precision: 0.803\n",
      "\n",
      "Accuracy: 0.056\n",
      "F1: 0.110\n",
      "Recall: 0.320\n",
      "Precision: 0.067\n",
      "Accuracy: 0.141\n",
      "F1: 0.823\n",
      "Recall: 0.803\n",
      "Precision: 0.844\n",
      "\n",
      "Accuracy: 0.166\n",
      "F1: 0.432\n",
      "Recall: 0.946\n",
      "Precision: 0.280\n",
      "Accuracy: 0.033\n",
      "F1: 0.136\n",
      "Recall: 0.188\n",
      "Precision: 0.107\n",
      "\n",
      "50 0.0001\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=5 loss=3200.419189453125\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=15 loss=2118.43505859375\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=25 loss=1625.321044921875\n",
      "iteration=30 loss=1459.548583984375\n",
      "iteration=35 loss=1318.0911865234375\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=45 loss=1099.1505126953125\n",
      "iteration=49 loss=1036.2803955078125\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=5 loss=2874.3857421875\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=15 loss=1647.8909912109375\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=25 loss=1233.5576171875\n",
      "iteration=30 loss=1090.6767578125\n",
      "iteration=35 loss=986.0718994140625\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=45 loss=840.8680419921875\n",
      "iteration=49 loss=800.1483154296875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=5 loss=3163.537109375\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=15 loss=1983.4013671875\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=25 loss=1464.844482421875\n",
      "iteration=30 loss=1316.7613525390625\n",
      "iteration=35 loss=1185.549560546875\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=45 loss=1013.34228515625\n",
      "iteration=49 loss=962.6436157226562\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=5 loss=3073.938232421875\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=15 loss=2015.9947509765625\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=25 loss=1509.5096435546875\n",
      "iteration=30 loss=1343.236083984375\n",
      "iteration=35 loss=1200.277587890625\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=45 loss=997.0213623046875\n",
      "iteration=49 loss=940.4842529296875\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=5 loss=3115.293212890625\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=15 loss=2039.8388671875\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=25 loss=1529.83642578125\n",
      "iteration=30 loss=1342.99072265625\n",
      "iteration=35 loss=1190.330078125\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=45 loss=982.098876953125\n",
      "iteration=49 loss=926.5582275390625\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=5 loss=3051.247314453125\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=15 loss=1983.706298828125\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=25 loss=1554.050537109375\n",
      "iteration=30 loss=1404.4222412109375\n",
      "iteration=35 loss=1271.953369140625\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=45 loss=1062.5484619140625\n",
      "iteration=49 loss=990.7612915039062\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=5 loss=3239.47509765625\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=15 loss=2105.424072265625\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=25 loss=1671.796142578125\n",
      "iteration=30 loss=1508.4581298828125\n",
      "iteration=35 loss=1363.0155029296875\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=45 loss=1163.2313232421875\n",
      "iteration=49 loss=1102.8865966796875\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=5 loss=3462.451416015625\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=15 loss=2042.861572265625\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=25 loss=1556.6419677734375\n",
      "iteration=30 loss=1387.144287109375\n",
      "iteration=35 loss=1242.898193359375\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=45 loss=1047.691650390625\n",
      "iteration=49 loss=994.4852294921875\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=5 loss=3420.547119140625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=15 loss=1902.0194091796875\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=25 loss=1436.858154296875\n",
      "iteration=30 loss=1274.5953369140625\n",
      "iteration=35 loss=1144.396240234375\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=45 loss=963.435791015625\n",
      "iteration=49 loss=911.1605224609375\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=5 loss=2831.9951171875\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=15 loss=1606.3018798828125\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=25 loss=1130.3294677734375\n",
      "iteration=30 loss=1026.1448974609375\n",
      "iteration=35 loss=947.2098388671875\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=45 loss=838.2081298828125\n",
      "iteration=49 loss=803.800537109375\n",
      "Accuracy: 0.169\n",
      "F1: 0.292\n",
      "Recall: 0.963\n",
      "Precision: 0.172\n",
      "Accuracy: 0.028\n",
      "F1: 0.248\n",
      "Recall: 0.160\n",
      "Precision: 0.554\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.280\n",
      "Recall: 0.915\n",
      "Precision: 0.165\n",
      "Accuracy: 0.103\n",
      "F1: 0.664\n",
      "Recall: 0.587\n",
      "Precision: 0.763\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 0.998\n",
      "Precision: 0.176\n",
      "Accuracy: 0.096\n",
      "F1: 0.647\n",
      "Recall: 0.545\n",
      "Precision: 0.796\n",
      "\n",
      "Accuracy: 0.169\n",
      "F1: 0.290\n",
      "Recall: 0.960\n",
      "Precision: 0.171\n",
      "Accuracy: 0.010\n",
      "F1: 0.109\n",
      "Recall: 0.059\n",
      "Precision: 0.745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.158\n",
      "F1: 0.278\n",
      "Recall: 0.897\n",
      "Precision: 0.164\n",
      "Accuracy: 0.013\n",
      "F1: 0.130\n",
      "Recall: 0.074\n",
      "Precision: 0.540\n",
      "\n",
      "Accuracy: 0.044\n",
      "F1: 0.133\n",
      "Recall: 0.249\n",
      "Precision: 0.091\n",
      "Accuracy: 0.143\n",
      "F1: 0.429\n",
      "Recall: 0.811\n",
      "Precision: 0.292\n",
      "\n",
      "Accuracy: 0.168\n",
      "F1: 0.290\n",
      "Recall: 0.955\n",
      "Precision: 0.171\n",
      "Accuracy: 0.091\n",
      "F1: 0.615\n",
      "Recall: 0.519\n",
      "Precision: 0.755\n",
      "\n",
      "Accuracy: 0.150\n",
      "F1: 0.265\n",
      "Recall: 0.855\n",
      "Precision: 0.157\n",
      "Accuracy: 0.126\n",
      "F1: 0.760\n",
      "Recall: 0.718\n",
      "Precision: 0.807\n",
      "\n",
      "Accuracy: 0.146\n",
      "F1: 0.256\n",
      "Recall: 0.831\n",
      "Precision: 0.152\n",
      "Accuracy: 0.122\n",
      "F1: 0.756\n",
      "Recall: 0.692\n",
      "Precision: 0.834\n",
      "\n",
      "Accuracy: 0.173\n",
      "F1: 0.297\n",
      "Recall: 0.985\n",
      "Precision: 0.175\n",
      "Accuracy: 0.061\n",
      "F1: 0.458\n",
      "Recall: 0.348\n",
      "Precision: 0.669\n",
      "\n",
      "50 1e-05\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=5 loss=3200.419189453125\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=15 loss=2118.43505859375\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=25 loss=1625.321044921875\n",
      "iteration=30 loss=1459.548583984375\n",
      "iteration=35 loss=1318.0911865234375\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=45 loss=1099.1505126953125\n",
      "iteration=49 loss=1036.2803955078125\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=5 loss=2874.3857421875\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=15 loss=1647.8909912109375\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=25 loss=1233.5576171875\n",
      "iteration=30 loss=1090.6767578125\n",
      "iteration=35 loss=986.0718994140625\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=45 loss=840.8680419921875\n",
      "iteration=49 loss=800.1483154296875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=5 loss=3163.537109375\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=15 loss=1983.4013671875\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=25 loss=1464.844482421875\n",
      "iteration=30 loss=1316.7613525390625\n",
      "iteration=35 loss=1185.549560546875\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=45 loss=1013.34228515625\n",
      "iteration=49 loss=962.6436157226562\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=5 loss=3073.938232421875\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=15 loss=2015.9947509765625\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=25 loss=1509.5096435546875\n",
      "iteration=30 loss=1343.236083984375\n",
      "iteration=35 loss=1200.277587890625\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=45 loss=997.0213623046875\n",
      "iteration=49 loss=940.4842529296875\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=5 loss=3115.293212890625\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=15 loss=2039.8388671875\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=25 loss=1529.83642578125\n",
      "iteration=30 loss=1342.99072265625\n",
      "iteration=35 loss=1190.330078125\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=45 loss=982.098876953125\n",
      "iteration=49 loss=926.5582275390625\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=5 loss=3051.247314453125\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=15 loss=1983.706298828125\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=25 loss=1554.050537109375\n",
      "iteration=30 loss=1404.4222412109375\n",
      "iteration=35 loss=1271.953369140625\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=45 loss=1062.5484619140625\n",
      "iteration=49 loss=990.7612915039062\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=5 loss=3239.47509765625\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=15 loss=2105.424072265625\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=25 loss=1671.796142578125\n",
      "iteration=30 loss=1508.4581298828125\n",
      "iteration=35 loss=1363.0155029296875\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=45 loss=1163.2313232421875\n",
      "iteration=49 loss=1102.8865966796875\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=5 loss=3462.451416015625\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=15 loss=2042.861572265625\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=25 loss=1556.6419677734375\n",
      "iteration=30 loss=1387.144287109375\n",
      "iteration=35 loss=1242.898193359375\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=45 loss=1047.691650390625\n",
      "iteration=49 loss=994.4852294921875\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=5 loss=3420.547119140625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=15 loss=1902.0194091796875\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=25 loss=1436.858154296875\n",
      "iteration=30 loss=1274.5953369140625\n",
      "iteration=35 loss=1144.396240234375\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=45 loss=963.435791015625\n",
      "iteration=49 loss=911.1605224609375\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=5 loss=2831.9951171875\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=15 loss=1606.3018798828125\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=25 loss=1130.3294677734375\n",
      "iteration=30 loss=1026.1448974609375\n",
      "iteration=35 loss=947.2098388671875\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=45 loss=838.2081298828125\n",
      "iteration=49 loss=803.800537109375\n",
      "Accuracy: 0.169\n",
      "F1: 0.292\n",
      "Recall: 0.963\n",
      "Precision: 0.172\n",
      "Accuracy: 0.028\n",
      "F1: 0.248\n",
      "Recall: 0.160\n",
      "Precision: 0.554\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.280\n",
      "Recall: 0.915\n",
      "Precision: 0.165\n",
      "Accuracy: 0.103\n",
      "F1: 0.664\n",
      "Recall: 0.587\n",
      "Precision: 0.763\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 0.998\n",
      "Precision: 0.176\n",
      "Accuracy: 0.096\n",
      "F1: 0.647\n",
      "Recall: 0.545\n",
      "Precision: 0.796\n",
      "\n",
      "Accuracy: 0.169\n",
      "F1: 0.290\n",
      "Recall: 0.960\n",
      "Precision: 0.171\n",
      "Accuracy: 0.010\n",
      "F1: 0.109\n",
      "Recall: 0.059\n",
      "Precision: 0.745\n",
      "\n",
      "Accuracy: 0.158\n",
      "F1: 0.278\n",
      "Recall: 0.897\n",
      "Precision: 0.164\n",
      "Accuracy: 0.013\n",
      "F1: 0.130\n",
      "Recall: 0.074\n",
      "Precision: 0.540\n",
      "\n",
      "Accuracy: 0.044\n",
      "F1: 0.133\n",
      "Recall: 0.249\n",
      "Precision: 0.091\n",
      "Accuracy: 0.143\n",
      "F1: 0.429\n",
      "Recall: 0.811\n",
      "Precision: 0.292\n",
      "\n",
      "Accuracy: 0.168\n",
      "F1: 0.290\n",
      "Recall: 0.955\n",
      "Precision: 0.171\n",
      "Accuracy: 0.091\n",
      "F1: 0.615\n",
      "Recall: 0.519\n",
      "Precision: 0.755\n",
      "\n",
      "Accuracy: 0.150\n",
      "F1: 0.265\n",
      "Recall: 0.855\n",
      "Precision: 0.157\n",
      "Accuracy: 0.126\n",
      "F1: 0.760\n",
      "Recall: 0.718\n",
      "Precision: 0.807\n",
      "\n",
      "Accuracy: 0.146\n",
      "F1: 0.256\n",
      "Recall: 0.831\n",
      "Precision: 0.152\n",
      "Accuracy: 0.122\n",
      "F1: 0.756\n",
      "Recall: 0.692\n",
      "Precision: 0.834\n",
      "\n",
      "Accuracy: 0.173\n",
      "F1: 0.297\n",
      "Recall: 0.985\n",
      "Precision: 0.175\n",
      "Accuracy: 0.061\n",
      "F1: 0.458\n",
      "Recall: 0.348\n",
      "Precision: 0.669\n",
      "\n",
      "50 1e-06\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=5 loss=3200.419189453125\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=15 loss=2118.43505859375\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=25 loss=1625.321044921875\n",
      "iteration=30 loss=1459.548583984375\n",
      "iteration=35 loss=1318.0911865234375\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=45 loss=1099.1505126953125\n",
      "iteration=49 loss=1036.2803955078125\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=5 loss=2874.3857421875\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=15 loss=1647.8909912109375\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=25 loss=1233.5576171875\n",
      "iteration=30 loss=1090.6767578125\n",
      "iteration=35 loss=986.0718994140625\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=45 loss=840.8680419921875\n",
      "iteration=49 loss=800.1483154296875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=5 loss=3163.537109375\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=15 loss=1983.4013671875\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=25 loss=1464.844482421875\n",
      "iteration=30 loss=1316.7613525390625\n",
      "iteration=35 loss=1185.549560546875\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=45 loss=1013.34228515625\n",
      "iteration=49 loss=962.6436157226562\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=5 loss=3073.938232421875\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=15 loss=2015.9947509765625\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=25 loss=1509.5096435546875\n",
      "iteration=30 loss=1343.236083984375\n",
      "iteration=35 loss=1200.277587890625\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=45 loss=997.0213623046875\n",
      "iteration=49 loss=940.4842529296875\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=5 loss=3115.293212890625\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=15 loss=2039.8388671875\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=25 loss=1529.83642578125\n",
      "iteration=30 loss=1342.99072265625\n",
      "iteration=35 loss=1190.330078125\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=45 loss=982.098876953125\n",
      "iteration=49 loss=926.5582275390625\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=5 loss=3051.247314453125\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=15 loss=1983.706298828125\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=25 loss=1554.050537109375\n",
      "iteration=30 loss=1404.4222412109375\n",
      "iteration=35 loss=1271.953369140625\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=45 loss=1062.5484619140625\n",
      "iteration=49 loss=990.7612915039062\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=5 loss=3239.47509765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=10 loss=2742.29052734375\n",
      "iteration=15 loss=2105.424072265625\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=25 loss=1671.796142578125\n",
      "iteration=30 loss=1508.4581298828125\n",
      "iteration=35 loss=1363.0155029296875\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=45 loss=1163.2313232421875\n",
      "iteration=49 loss=1102.8865966796875\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=5 loss=3462.451416015625\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=15 loss=2042.861572265625\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=25 loss=1556.6419677734375\n",
      "iteration=30 loss=1387.144287109375\n",
      "iteration=35 loss=1242.898193359375\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=45 loss=1047.691650390625\n",
      "iteration=49 loss=994.4852294921875\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=5 loss=3420.547119140625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=15 loss=1902.0194091796875\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=25 loss=1436.858154296875\n",
      "iteration=30 loss=1274.5953369140625\n",
      "iteration=35 loss=1144.396240234375\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=45 loss=963.435791015625\n",
      "iteration=49 loss=911.1605224609375\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=5 loss=2831.9951171875\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=15 loss=1606.3018798828125\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=25 loss=1130.3294677734375\n",
      "iteration=30 loss=1026.1448974609375\n",
      "iteration=35 loss=947.2098388671875\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=45 loss=838.2081298828125\n",
      "iteration=49 loss=803.800537109375\n",
      "Accuracy: 0.169\n",
      "F1: 0.292\n",
      "Recall: 0.963\n",
      "Precision: 0.172\n",
      "Accuracy: 0.028\n",
      "F1: 0.248\n",
      "Recall: 0.160\n",
      "Precision: 0.554\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.280\n",
      "Recall: 0.915\n",
      "Precision: 0.165\n",
      "Accuracy: 0.103\n",
      "F1: 0.664\n",
      "Recall: 0.587\n",
      "Precision: 0.763\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 0.998\n",
      "Precision: 0.176\n",
      "Accuracy: 0.096\n",
      "F1: 0.647\n",
      "Recall: 0.545\n",
      "Precision: 0.796\n",
      "\n",
      "Accuracy: 0.169\n",
      "F1: 0.290\n",
      "Recall: 0.960\n",
      "Precision: 0.171\n",
      "Accuracy: 0.010\n",
      "F1: 0.109\n",
      "Recall: 0.059\n",
      "Precision: 0.745\n",
      "\n",
      "Accuracy: 0.158\n",
      "F1: 0.278\n",
      "Recall: 0.897\n",
      "Precision: 0.164\n",
      "Accuracy: 0.013\n",
      "F1: 0.130\n",
      "Recall: 0.074\n",
      "Precision: 0.540\n",
      "\n",
      "Accuracy: 0.044\n",
      "F1: 0.133\n",
      "Recall: 0.249\n",
      "Precision: 0.091\n",
      "Accuracy: 0.143\n",
      "F1: 0.429\n",
      "Recall: 0.811\n",
      "Precision: 0.292\n",
      "\n",
      "Accuracy: 0.168\n",
      "F1: 0.290\n",
      "Recall: 0.955\n",
      "Precision: 0.171\n",
      "Accuracy: 0.091\n",
      "F1: 0.615\n",
      "Recall: 0.519\n",
      "Precision: 0.755\n",
      "\n",
      "Accuracy: 0.150\n",
      "F1: 0.265\n",
      "Recall: 0.855\n",
      "Precision: 0.157\n",
      "Accuracy: 0.126\n",
      "F1: 0.760\n",
      "Recall: 0.718\n",
      "Precision: 0.807\n",
      "\n",
      "Accuracy: 0.146\n",
      "F1: 0.256\n",
      "Recall: 0.831\n",
      "Precision: 0.152\n",
      "Accuracy: 0.122\n",
      "F1: 0.756\n",
      "Recall: 0.692\n",
      "Precision: 0.834\n",
      "\n",
      "Accuracy: 0.173\n",
      "F1: 0.297\n",
      "Recall: 0.985\n",
      "Precision: 0.175\n",
      "Accuracy: 0.061\n",
      "F1: 0.458\n",
      "Recall: 0.348\n",
      "Precision: 0.669\n",
      "\n",
      "50 1e-07\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=5 loss=3200.419189453125\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=15 loss=2118.43505859375\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=25 loss=1625.321044921875\n",
      "iteration=30 loss=1459.548583984375\n",
      "iteration=35 loss=1318.0911865234375\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=45 loss=1099.1505126953125\n",
      "iteration=49 loss=1036.2803955078125\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=5 loss=2874.3857421875\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=15 loss=1647.8909912109375\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=25 loss=1233.5576171875\n",
      "iteration=30 loss=1090.6767578125\n",
      "iteration=35 loss=986.0718994140625\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=45 loss=840.8680419921875\n",
      "iteration=49 loss=800.1483154296875\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=5 loss=3163.537109375\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=15 loss=1983.4013671875\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=25 loss=1464.844482421875\n",
      "iteration=30 loss=1316.7613525390625\n",
      "iteration=35 loss=1185.549560546875\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=45 loss=1013.34228515625\n",
      "iteration=49 loss=962.6436157226562\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=5 loss=3073.938232421875\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=15 loss=2015.9947509765625\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=25 loss=1509.5096435546875\n",
      "iteration=30 loss=1343.236083984375\n",
      "iteration=35 loss=1200.277587890625\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=45 loss=997.0213623046875\n",
      "iteration=49 loss=940.4842529296875\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=5 loss=3115.293212890625\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=15 loss=2039.8388671875\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=25 loss=1529.83642578125\n",
      "iteration=30 loss=1342.99072265625\n",
      "iteration=35 loss=1190.330078125\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=45 loss=982.098876953125\n",
      "iteration=49 loss=926.5582275390625\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=5 loss=3051.247314453125\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=15 loss=1983.706298828125\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=25 loss=1554.050537109375\n",
      "iteration=30 loss=1404.4222412109375\n",
      "iteration=35 loss=1271.953369140625\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=45 loss=1062.5484619140625\n",
      "iteration=49 loss=990.7612915039062\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=5 loss=3239.47509765625\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=15 loss=2105.424072265625\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=25 loss=1671.796142578125\n",
      "iteration=30 loss=1508.4581298828125\n",
      "iteration=35 loss=1363.0155029296875\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=45 loss=1163.2313232421875\n",
      "iteration=49 loss=1102.8865966796875\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=5 loss=3462.451416015625\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=15 loss=2042.861572265625\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=25 loss=1556.6419677734375\n",
      "iteration=30 loss=1387.144287109375\n",
      "iteration=35 loss=1242.898193359375\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=45 loss=1047.691650390625\n",
      "iteration=49 loss=994.4852294921875\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=5 loss=3420.547119140625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=15 loss=1902.0194091796875\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=25 loss=1436.858154296875\n",
      "iteration=30 loss=1274.5953369140625\n",
      "iteration=35 loss=1144.396240234375\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=45 loss=963.435791015625\n",
      "iteration=49 loss=911.1605224609375\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=5 loss=2831.9951171875\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=15 loss=1606.3018798828125\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=25 loss=1130.3294677734375\n",
      "iteration=30 loss=1026.1448974609375\n",
      "iteration=35 loss=947.2098388671875\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=45 loss=838.2081298828125\n",
      "iteration=49 loss=803.800537109375\n",
      "Accuracy: 0.169\n",
      "F1: 0.292\n",
      "Recall: 0.963\n",
      "Precision: 0.172\n",
      "Accuracy: 0.028\n",
      "F1: 0.248\n",
      "Recall: 0.160\n",
      "Precision: 0.554\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.280\n",
      "Recall: 0.915\n",
      "Precision: 0.165\n",
      "Accuracy: 0.103\n",
      "F1: 0.664\n",
      "Recall: 0.587\n",
      "Precision: 0.763\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 0.998\n",
      "Precision: 0.176\n",
      "Accuracy: 0.096\n",
      "F1: 0.647\n",
      "Recall: 0.545\n",
      "Precision: 0.796\n",
      "\n",
      "Accuracy: 0.169\n",
      "F1: 0.290\n",
      "Recall: 0.960\n",
      "Precision: 0.171\n",
      "Accuracy: 0.010\n",
      "F1: 0.109\n",
      "Recall: 0.059\n",
      "Precision: 0.745\n",
      "\n",
      "Accuracy: 0.158\n",
      "F1: 0.278\n",
      "Recall: 0.897\n",
      "Precision: 0.164\n",
      "Accuracy: 0.013\n",
      "F1: 0.130\n",
      "Recall: 0.074\n",
      "Precision: 0.540\n",
      "\n",
      "Accuracy: 0.044\n",
      "F1: 0.133\n",
      "Recall: 0.249\n",
      "Precision: 0.091\n",
      "Accuracy: 0.143\n",
      "F1: 0.429\n",
      "Recall: 0.811\n",
      "Precision: 0.292\n",
      "\n",
      "Accuracy: 0.168\n",
      "F1: 0.290\n",
      "Recall: 0.955\n",
      "Precision: 0.171\n",
      "Accuracy: 0.091\n",
      "F1: 0.615\n",
      "Recall: 0.519\n",
      "Precision: 0.755\n",
      "\n",
      "Accuracy: 0.150\n",
      "F1: 0.265\n",
      "Recall: 0.855\n",
      "Precision: 0.157\n",
      "Accuracy: 0.126\n",
      "F1: 0.760\n",
      "Recall: 0.718\n",
      "Precision: 0.807\n",
      "\n",
      "Accuracy: 0.146\n",
      "F1: 0.256\n",
      "Recall: 0.831\n",
      "Precision: 0.152\n",
      "Accuracy: 0.122\n",
      "F1: 0.756\n",
      "Recall: 0.692\n",
      "Precision: 0.834\n",
      "\n",
      "Accuracy: 0.173\n",
      "F1: 0.297\n",
      "Recall: 0.985\n",
      "Precision: 0.175\n",
      "Accuracy: 0.061\n",
      "F1: 0.458\n",
      "Recall: 0.348\n",
      "Precision: 0.669\n",
      "\n",
      "100 0.0001\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=30 loss=1459.548583984375\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=50 loss=1022.2281494140625\n",
      "iteration=60 loss=908.0228271484375\n",
      "iteration=70 loss=825.66259765625\n",
      "iteration=80 loss=760.16552734375\n",
      "iteration=90 loss=705.644287109375\n",
      "iteration=99 loss=663.3721313476562\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=10 loss=2350.362060546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=20 loss=1429.170654296875\n",
      "iteration=30 loss=1090.6767578125\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=50 loss=791.0970458984375\n",
      "iteration=60 loss=715.3494873046875\n",
      "iteration=70 loss=657.9431762695312\n",
      "iteration=80 loss=612.602294921875\n",
      "iteration=90 loss=576.8271484375\n",
      "iteration=99 loss=550.7755737304688\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=30 loss=1316.7613525390625\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=50 loss=951.0770263671875\n",
      "iteration=60 loss=853.886962890625\n",
      "iteration=70 loss=779.861328125\n",
      "iteration=80 loss=722.4713134765625\n",
      "iteration=90 loss=675.684326171875\n",
      "iteration=99 loss=640.166259765625\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=30 loss=1343.236083984375\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=50 loss=927.8428955078125\n",
      "iteration=60 loss=825.1005859375\n",
      "iteration=70 loss=751.2860107421875\n",
      "iteration=80 loss=696.3060302734375\n",
      "iteration=90 loss=652.7235717773438\n",
      "iteration=99 loss=620.2095336914062\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=30 loss=1342.99072265625\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=50 loss=914.3002319335938\n",
      "iteration=60 loss=816.7399291992188\n",
      "iteration=70 loss=748.3306274414062\n",
      "iteration=80 loss=698.6795654296875\n",
      "iteration=90 loss=659.5263671875\n",
      "iteration=99 loss=630.3766479492188\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=30 loss=1404.4222412109375\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=50 loss=973.7070922851562\n",
      "iteration=60 loss=826.7042846679688\n",
      "iteration=70 loss=722.2608642578125\n",
      "iteration=80 loss=647.0758056640625\n",
      "iteration=90 loss=591.1217651367188\n",
      "iteration=99 loss=552.4406127929688\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=30 loss=1508.4581298828125\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=50 loss=1089.046875\n",
      "iteration=60 loss=971.8869018554688\n",
      "iteration=70 loss=884.2269287109375\n",
      "iteration=80 loss=817.0056762695312\n",
      "iteration=90 loss=764.0267944335938\n",
      "iteration=99 loss=724.8623046875\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=30 loss=1387.144287109375\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=50 loss=982.6883544921875\n",
      "iteration=60 loss=888.9819946289062\n",
      "iteration=70 loss=821.7407836914062\n",
      "iteration=80 loss=769.253662109375\n",
      "iteration=90 loss=725.4114990234375\n",
      "iteration=99 loss=691.37646484375\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=30 loss=1274.5953369140625\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=50 loss=899.6014404296875\n",
      "iteration=60 loss=805.6924438476562\n",
      "iteration=70 loss=736.9815673828125\n",
      "iteration=80 loss=683.49951171875\n",
      "iteration=90 loss=639.6753540039062\n",
      "iteration=99 loss=606.5809326171875\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=30 loss=1026.1448974609375\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=50 loss=795.8414306640625\n",
      "iteration=60 loss=728.4292602539062\n",
      "iteration=70 loss=675.828369140625\n",
      "iteration=80 loss=633.3814086914062\n",
      "iteration=90 loss=598.0888671875\n",
      "iteration=99 loss=570.816162109375\n",
      "Accuracy: 0.172\n",
      "F1: 0.294\n",
      "Recall: 0.977\n",
      "Precision: 0.173\n",
      "Accuracy: 0.129\n",
      "F1: 0.766\n",
      "Recall: 0.732\n",
      "Precision: 0.804\n",
      "\n",
      "Accuracy: 0.159\n",
      "F1: 0.276\n",
      "Recall: 0.902\n",
      "Precision: 0.163\n",
      "Accuracy: 0.093\n",
      "F1: 0.633\n",
      "Recall: 0.531\n",
      "Precision: 0.784\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "Accuracy: 0.102\n",
      "F1: 0.669\n",
      "Recall: 0.579\n",
      "Precision: 0.793\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.064\n",
      "F1: 0.493\n",
      "Recall: 0.363\n",
      "Precision: 0.767\n",
      "\n",
      "Accuracy: 0.170\n",
      "F1: 0.293\n",
      "Recall: 0.967\n",
      "Precision: 0.172\n",
      "Accuracy: 0.045\n",
      "F1: 0.360\n",
      "Recall: 0.253\n",
      "Precision: 0.623\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.072\n",
      "F1: 0.537\n",
      "Recall: 0.410\n",
      "Precision: 0.780\n",
      "\n",
      "Accuracy: 0.175\n",
      "F1: 0.298\n",
      "Recall: 0.994\n",
      "Precision: 0.175\n",
      "Accuracy: 0.111\n",
      "F1: 0.700\n",
      "Recall: 0.628\n",
      "Precision: 0.790\n",
      "\n",
      "Accuracy: 0.162\n",
      "F1: 0.281\n",
      "Recall: 0.922\n",
      "Precision: 0.166\n",
      "Accuracy: 0.139\n",
      "F1: 0.797\n",
      "Recall: 0.787\n",
      "Precision: 0.807\n",
      "\n",
      "Accuracy: 0.168\n",
      "F1: 0.288\n",
      "Recall: 0.956\n",
      "Precision: 0.170\n",
      "Accuracy: 0.075\n",
      "F1: 0.557\n",
      "Recall: 0.424\n",
      "Precision: 0.809\n",
      "\n",
      "Accuracy: 0.171\n",
      "F1: 0.294\n",
      "Recall: 0.973\n",
      "Precision: 0.173\n",
      "Accuracy: 0.133\n",
      "F1: 0.776\n",
      "Recall: 0.756\n",
      "Precision: 0.797\n",
      "\n",
      "100 1e-05\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=30 loss=1459.548583984375\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=50 loss=1022.2281494140625\n",
      "iteration=60 loss=908.0228271484375\n",
      "iteration=70 loss=825.66259765625\n",
      "iteration=80 loss=760.16552734375\n",
      "iteration=90 loss=705.644287109375\n",
      "iteration=99 loss=663.3721313476562\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=30 loss=1090.6767578125\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=50 loss=791.0970458984375\n",
      "iteration=60 loss=715.3494873046875\n",
      "iteration=70 loss=657.9431762695312\n",
      "iteration=80 loss=612.602294921875\n",
      "iteration=90 loss=576.8271484375\n",
      "iteration=99 loss=550.7755737304688\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=30 loss=1316.7613525390625\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=50 loss=951.0770263671875\n",
      "iteration=60 loss=853.886962890625\n",
      "iteration=70 loss=779.861328125\n",
      "iteration=80 loss=722.4713134765625\n",
      "iteration=90 loss=675.684326171875\n",
      "iteration=99 loss=640.166259765625\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=30 loss=1343.236083984375\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=50 loss=927.8428955078125\n",
      "iteration=60 loss=825.1005859375\n",
      "iteration=70 loss=751.2860107421875\n",
      "iteration=80 loss=696.3060302734375\n",
      "iteration=90 loss=652.7235717773438\n",
      "iteration=99 loss=620.2095336914062\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=30 loss=1342.99072265625\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=50 loss=914.3002319335938\n",
      "iteration=60 loss=816.7399291992188\n",
      "iteration=70 loss=748.3306274414062\n",
      "iteration=80 loss=698.6795654296875\n",
      "iteration=90 loss=659.5263671875\n",
      "iteration=99 loss=630.3766479492188\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=30 loss=1404.4222412109375\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=50 loss=973.7070922851562\n",
      "iteration=60 loss=826.7042846679688\n",
      "iteration=70 loss=722.2608642578125\n",
      "iteration=80 loss=647.0758056640625\n",
      "iteration=90 loss=591.1217651367188\n",
      "iteration=99 loss=552.4406127929688\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=30 loss=1508.4581298828125\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=50 loss=1089.046875\n",
      "iteration=60 loss=971.8869018554688\n",
      "iteration=70 loss=884.2269287109375\n",
      "iteration=80 loss=817.0056762695312\n",
      "iteration=90 loss=764.0267944335938\n",
      "iteration=99 loss=724.8623046875\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=30 loss=1387.144287109375\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=50 loss=982.6883544921875\n",
      "iteration=60 loss=888.9819946289062\n",
      "iteration=70 loss=821.7407836914062\n",
      "iteration=80 loss=769.253662109375\n",
      "iteration=90 loss=725.4114990234375\n",
      "iteration=99 loss=691.37646484375\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=30 loss=1274.5953369140625\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=50 loss=899.6014404296875\n",
      "iteration=60 loss=805.6924438476562\n",
      "iteration=70 loss=736.9815673828125\n",
      "iteration=80 loss=683.49951171875\n",
      "iteration=90 loss=639.6753540039062\n",
      "iteration=99 loss=606.5809326171875\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=20 loss=1285.5823974609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=30 loss=1026.1448974609375\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=50 loss=795.8414306640625\n",
      "iteration=60 loss=728.4292602539062\n",
      "iteration=70 loss=675.828369140625\n",
      "iteration=80 loss=633.3814086914062\n",
      "iteration=90 loss=598.0888671875\n",
      "iteration=99 loss=570.816162109375\n",
      "Accuracy: 0.172\n",
      "F1: 0.294\n",
      "Recall: 0.977\n",
      "Precision: 0.173\n",
      "Accuracy: 0.129\n",
      "F1: 0.766\n",
      "Recall: 0.732\n",
      "Precision: 0.804\n",
      "\n",
      "Accuracy: 0.159\n",
      "F1: 0.276\n",
      "Recall: 0.902\n",
      "Precision: 0.163\n",
      "Accuracy: 0.093\n",
      "F1: 0.633\n",
      "Recall: 0.531\n",
      "Precision: 0.784\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "Accuracy: 0.102\n",
      "F1: 0.669\n",
      "Recall: 0.579\n",
      "Precision: 0.793\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.064\n",
      "F1: 0.493\n",
      "Recall: 0.363\n",
      "Precision: 0.767\n",
      "\n",
      "Accuracy: 0.170\n",
      "F1: 0.293\n",
      "Recall: 0.967\n",
      "Precision: 0.172\n",
      "Accuracy: 0.045\n",
      "F1: 0.360\n",
      "Recall: 0.253\n",
      "Precision: 0.623\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.072\n",
      "F1: 0.537\n",
      "Recall: 0.410\n",
      "Precision: 0.780\n",
      "\n",
      "Accuracy: 0.175\n",
      "F1: 0.298\n",
      "Recall: 0.994\n",
      "Precision: 0.175\n",
      "Accuracy: 0.111\n",
      "F1: 0.700\n",
      "Recall: 0.628\n",
      "Precision: 0.790\n",
      "\n",
      "Accuracy: 0.162\n",
      "F1: 0.281\n",
      "Recall: 0.922\n",
      "Precision: 0.166\n",
      "Accuracy: 0.139\n",
      "F1: 0.797\n",
      "Recall: 0.787\n",
      "Precision: 0.807\n",
      "\n",
      "Accuracy: 0.168\n",
      "F1: 0.288\n",
      "Recall: 0.956\n",
      "Precision: 0.170\n",
      "Accuracy: 0.075\n",
      "F1: 0.557\n",
      "Recall: 0.424\n",
      "Precision: 0.809\n",
      "\n",
      "Accuracy: 0.171\n",
      "F1: 0.294\n",
      "Recall: 0.973\n",
      "Precision: 0.173\n",
      "Accuracy: 0.133\n",
      "F1: 0.776\n",
      "Recall: 0.756\n",
      "Precision: 0.797\n",
      "\n",
      "100 1e-06\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=30 loss=1459.548583984375\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=50 loss=1022.2281494140625\n",
      "iteration=60 loss=908.0228271484375\n",
      "iteration=70 loss=825.66259765625\n",
      "iteration=80 loss=760.16552734375\n",
      "iteration=90 loss=705.644287109375\n",
      "iteration=99 loss=663.3721313476562\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=30 loss=1090.6767578125\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=50 loss=791.0970458984375\n",
      "iteration=60 loss=715.3494873046875\n",
      "iteration=70 loss=657.9431762695312\n",
      "iteration=80 loss=612.602294921875\n",
      "iteration=90 loss=576.8271484375\n",
      "iteration=99 loss=550.7755737304688\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=30 loss=1316.7613525390625\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=50 loss=951.0770263671875\n",
      "iteration=60 loss=853.886962890625\n",
      "iteration=70 loss=779.861328125\n",
      "iteration=80 loss=722.4713134765625\n",
      "iteration=90 loss=675.684326171875\n",
      "iteration=99 loss=640.166259765625\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=30 loss=1343.236083984375\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=50 loss=927.8428955078125\n",
      "iteration=60 loss=825.1005859375\n",
      "iteration=70 loss=751.2860107421875\n",
      "iteration=80 loss=696.3060302734375\n",
      "iteration=90 loss=652.7235717773438\n",
      "iteration=99 loss=620.2095336914062\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=30 loss=1342.99072265625\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=50 loss=914.3002319335938\n",
      "iteration=60 loss=816.7399291992188\n",
      "iteration=70 loss=748.3306274414062\n",
      "iteration=80 loss=698.6795654296875\n",
      "iteration=90 loss=659.5263671875\n",
      "iteration=99 loss=630.3766479492188\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=30 loss=1404.4222412109375\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=50 loss=973.7070922851562\n",
      "iteration=60 loss=826.7042846679688\n",
      "iteration=70 loss=722.2608642578125\n",
      "iteration=80 loss=647.0758056640625\n",
      "iteration=90 loss=591.1217651367188\n",
      "iteration=99 loss=552.4406127929688\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=30 loss=1508.4581298828125\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=50 loss=1089.046875\n",
      "iteration=60 loss=971.8869018554688\n",
      "iteration=70 loss=884.2269287109375\n",
      "iteration=80 loss=817.0056762695312\n",
      "iteration=90 loss=764.0267944335938\n",
      "iteration=99 loss=724.8623046875\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=30 loss=1387.144287109375\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=50 loss=982.6883544921875\n",
      "iteration=60 loss=888.9819946289062\n",
      "iteration=70 loss=821.7407836914062\n",
      "iteration=80 loss=769.253662109375\n",
      "iteration=90 loss=725.4114990234375\n",
      "iteration=99 loss=691.37646484375\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=30 loss=1274.5953369140625\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=50 loss=899.6014404296875\n",
      "iteration=60 loss=805.6924438476562\n",
      "iteration=70 loss=736.9815673828125\n",
      "iteration=80 loss=683.49951171875\n",
      "iteration=90 loss=639.6753540039062\n",
      "iteration=99 loss=606.5809326171875\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=30 loss=1026.1448974609375\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=50 loss=795.8414306640625\n",
      "iteration=60 loss=728.4292602539062\n",
      "iteration=70 loss=675.828369140625\n",
      "iteration=80 loss=633.3814086914062\n",
      "iteration=90 loss=598.0888671875\n",
      "iteration=99 loss=570.816162109375\n",
      "Accuracy: 0.172\n",
      "F1: 0.294\n",
      "Recall: 0.977\n",
      "Precision: 0.173\n",
      "Accuracy: 0.129\n",
      "F1: 0.766\n",
      "Recall: 0.732\n",
      "Precision: 0.804\n",
      "\n",
      "Accuracy: 0.159\n",
      "F1: 0.276\n",
      "Recall: 0.902\n",
      "Precision: 0.163\n",
      "Accuracy: 0.093\n",
      "F1: 0.633\n",
      "Recall: 0.531\n",
      "Precision: 0.784\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "Accuracy: 0.102\n",
      "F1: 0.669\n",
      "Recall: 0.579\n",
      "Precision: 0.793\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.064\n",
      "F1: 0.493\n",
      "Recall: 0.363\n",
      "Precision: 0.767\n",
      "\n",
      "Accuracy: 0.170\n",
      "F1: 0.293\n",
      "Recall: 0.967\n",
      "Precision: 0.172\n",
      "Accuracy: 0.045\n",
      "F1: 0.360\n",
      "Recall: 0.253\n",
      "Precision: 0.623\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.072\n",
      "F1: 0.537\n",
      "Recall: 0.410\n",
      "Precision: 0.780\n",
      "\n",
      "Accuracy: 0.175\n",
      "F1: 0.298\n",
      "Recall: 0.994\n",
      "Precision: 0.175\n",
      "Accuracy: 0.111\n",
      "F1: 0.700\n",
      "Recall: 0.628\n",
      "Precision: 0.790\n",
      "\n",
      "Accuracy: 0.162\n",
      "F1: 0.281\n",
      "Recall: 0.922\n",
      "Precision: 0.166\n",
      "Accuracy: 0.139\n",
      "F1: 0.797\n",
      "Recall: 0.787\n",
      "Precision: 0.807\n",
      "\n",
      "Accuracy: 0.168\n",
      "F1: 0.288\n",
      "Recall: 0.956\n",
      "Precision: 0.170\n",
      "Accuracy: 0.075\n",
      "F1: 0.557\n",
      "Recall: 0.424\n",
      "Precision: 0.809\n",
      "\n",
      "Accuracy: 0.171\n",
      "F1: 0.294\n",
      "Recall: 0.973\n",
      "Precision: 0.173\n",
      "Accuracy: 0.133\n",
      "F1: 0.776\n",
      "Recall: 0.756\n",
      "Precision: 0.797\n",
      "\n",
      "100 1e-07\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=10 loss=2707.240966796875\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=30 loss=1459.548583984375\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=50 loss=1022.2281494140625\n",
      "iteration=60 loss=908.0228271484375\n",
      "iteration=70 loss=825.66259765625\n",
      "iteration=80 loss=760.16552734375\n",
      "iteration=90 loss=705.644287109375\n",
      "iteration=99 loss=663.3721313476562\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=10 loss=2350.362060546875\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=30 loss=1090.6767578125\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=50 loss=791.0970458984375\n",
      "iteration=60 loss=715.3494873046875\n",
      "iteration=70 loss=657.9431762695312\n",
      "iteration=80 loss=612.602294921875\n",
      "iteration=90 loss=576.8271484375\n",
      "iteration=99 loss=550.7755737304688\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=10 loss=2790.302001953125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=30 loss=1316.7613525390625\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=50 loss=951.0770263671875\n",
      "iteration=60 loss=853.886962890625\n",
      "iteration=70 loss=779.861328125\n",
      "iteration=80 loss=722.4713134765625\n",
      "iteration=90 loss=675.684326171875\n",
      "iteration=99 loss=640.166259765625\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=10 loss=2662.64794921875\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=30 loss=1343.236083984375\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=50 loss=927.8428955078125\n",
      "iteration=60 loss=825.1005859375\n",
      "iteration=70 loss=751.2860107421875\n",
      "iteration=80 loss=696.3060302734375\n",
      "iteration=90 loss=652.7235717773438\n",
      "iteration=99 loss=620.2095336914062\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=10 loss=2636.284423828125\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=30 loss=1342.99072265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=40 loss=1071.38232421875\n",
      "iteration=50 loss=914.3002319335938\n",
      "iteration=60 loss=816.7399291992188\n",
      "iteration=70 loss=748.3306274414062\n",
      "iteration=80 loss=698.6795654296875\n",
      "iteration=90 loss=659.5263671875\n",
      "iteration=99 loss=630.3766479492188\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=10 loss=2693.669921875\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=30 loss=1404.4222412109375\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=50 loss=973.7070922851562\n",
      "iteration=60 loss=826.7042846679688\n",
      "iteration=70 loss=722.2608642578125\n",
      "iteration=80 loss=647.0758056640625\n",
      "iteration=90 loss=591.1217651367188\n",
      "iteration=99 loss=552.4406127929688\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=10 loss=2742.29052734375\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=30 loss=1508.4581298828125\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=50 loss=1089.046875\n",
      "iteration=60 loss=971.8869018554688\n",
      "iteration=70 loss=884.2269287109375\n",
      "iteration=80 loss=817.0056762695312\n",
      "iteration=90 loss=764.0267944335938\n",
      "iteration=99 loss=724.8623046875\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=10 loss=2719.8134765625\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=30 loss=1387.144287109375\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=50 loss=982.6883544921875\n",
      "iteration=60 loss=888.9819946289062\n",
      "iteration=70 loss=821.7407836914062\n",
      "iteration=80 loss=769.253662109375\n",
      "iteration=90 loss=725.4114990234375\n",
      "iteration=99 loss=691.37646484375\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=10 loss=2667.57568359375\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=30 loss=1274.5953369140625\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=50 loss=899.6014404296875\n",
      "iteration=60 loss=805.6924438476562\n",
      "iteration=70 loss=736.9815673828125\n",
      "iteration=80 loss=683.49951171875\n",
      "iteration=90 loss=639.6753540039062\n",
      "iteration=99 loss=606.5809326171875\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=10 loss=2423.983154296875\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=30 loss=1026.1448974609375\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=50 loss=795.8414306640625\n",
      "iteration=60 loss=728.4292602539062\n",
      "iteration=70 loss=675.828369140625\n",
      "iteration=80 loss=633.3814086914062\n",
      "iteration=90 loss=598.0888671875\n",
      "iteration=99 loss=570.816162109375\n",
      "Accuracy: 0.172\n",
      "F1: 0.294\n",
      "Recall: 0.977\n",
      "Precision: 0.173\n",
      "Accuracy: 0.129\n",
      "F1: 0.766\n",
      "Recall: 0.732\n",
      "Precision: 0.804\n",
      "\n",
      "Accuracy: 0.159\n",
      "F1: 0.276\n",
      "Recall: 0.902\n",
      "Precision: 0.163\n",
      "Accuracy: 0.093\n",
      "F1: 0.633\n",
      "Recall: 0.531\n",
      "Precision: 0.784\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "Accuracy: 0.102\n",
      "F1: 0.669\n",
      "Recall: 0.579\n",
      "Precision: 0.793\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.064\n",
      "F1: 0.493\n",
      "Recall: 0.363\n",
      "Precision: 0.767\n",
      "\n",
      "Accuracy: 0.170\n",
      "F1: 0.293\n",
      "Recall: 0.967\n",
      "Precision: 0.172\n",
      "Accuracy: 0.045\n",
      "F1: 0.360\n",
      "Recall: 0.253\n",
      "Precision: 0.623\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.072\n",
      "F1: 0.537\n",
      "Recall: 0.410\n",
      "Precision: 0.780\n",
      "\n",
      "Accuracy: 0.175\n",
      "F1: 0.298\n",
      "Recall: 0.994\n",
      "Precision: 0.175\n",
      "Accuracy: 0.111\n",
      "F1: 0.700\n",
      "Recall: 0.628\n",
      "Precision: 0.790\n",
      "\n",
      "Accuracy: 0.162\n",
      "F1: 0.281\n",
      "Recall: 0.922\n",
      "Precision: 0.166\n",
      "Accuracy: 0.139\n",
      "F1: 0.797\n",
      "Recall: 0.787\n",
      "Precision: 0.807\n",
      "\n",
      "Accuracy: 0.168\n",
      "F1: 0.288\n",
      "Recall: 0.956\n",
      "Precision: 0.170\n",
      "Accuracy: 0.075\n",
      "F1: 0.557\n",
      "Recall: 0.424\n",
      "Precision: 0.809\n",
      "\n",
      "Accuracy: 0.171\n",
      "F1: 0.294\n",
      "Recall: 0.973\n",
      "Precision: 0.173\n",
      "Accuracy: 0.133\n",
      "F1: 0.776\n",
      "Recall: 0.756\n",
      "Precision: 0.797\n",
      "\n",
      "200 0.0001\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=60 loss=908.0228271484375\n",
      "iteration=80 loss=760.16552734375\n",
      "iteration=100 loss=658.9944458007812\n",
      "iteration=120 loss=584.0817260742188\n",
      "iteration=140 loss=530.9254760742188\n",
      "iteration=160 loss=491.4053955078125\n",
      "iteration=180 loss=461.4503479003906\n",
      "iteration=199 loss=439.26202392578125\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=60 loss=715.3494873046875\n",
      "iteration=80 loss=612.602294921875\n",
      "iteration=100 loss=548.1060791015625\n",
      "iteration=120 loss=501.87310791015625\n",
      "iteration=140 loss=466.4742736816406\n",
      "iteration=160 loss=441.8441162109375\n",
      "iteration=180 loss=421.2817077636719\n",
      "iteration=199 loss=404.4196472167969\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=60 loss=853.886962890625\n",
      "iteration=80 loss=722.4713134765625\n",
      "iteration=100 loss=636.5574951171875\n",
      "iteration=120 loss=576.177734375\n",
      "iteration=140 loss=533.3216552734375\n",
      "iteration=160 loss=502.042724609375\n",
      "iteration=180 loss=478.6576843261719\n",
      "iteration=199 loss=462.13604736328125\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=60 loss=825.1005859375\n",
      "iteration=80 loss=696.3060302734375\n",
      "iteration=100 loss=616.915771484375\n",
      "iteration=120 loss=561.1666259765625\n",
      "iteration=140 loss=520.2505493164062\n",
      "iteration=160 loss=489.23638916015625\n",
      "iteration=180 loss=464.9713439941406\n",
      "iteration=199 loss=446.3029479980469\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=60 loss=816.7399291992188\n",
      "iteration=80 loss=698.6795654296875\n",
      "iteration=100 loss=627.4421997070312\n",
      "iteration=120 loss=578.1500244140625\n",
      "iteration=140 loss=541.8201904296875\n",
      "iteration=160 loss=513.1576538085938\n",
      "iteration=180 loss=489.2718200683594\n",
      "iteration=199 loss=469.3633117675781\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=60 loss=826.7042846679688\n",
      "iteration=80 loss=647.0758056640625\n",
      "iteration=100 loss=548.8148193359375\n",
      "iteration=120 loss=491.05242919921875\n",
      "iteration=140 loss=2972.516357421875\n",
      "iteration=160 loss=2682.64697265625\n",
      "iteration=180 loss=2597.65576171875\n",
      "iteration=199 loss=2548.941162109375\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=60 loss=971.8869018554688\n",
      "iteration=80 loss=817.0056762695312\n",
      "iteration=100 loss=720.9212036132812\n",
      "iteration=120 loss=657.0328369140625\n",
      "iteration=140 loss=608.8980102539062\n",
      "iteration=160 loss=571.4812622070312\n",
      "iteration=180 loss=542.8126831054688\n",
      "iteration=199 loss=520.6321411132812\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=60 loss=888.9819946289062\n",
      "iteration=80 loss=769.253662109375\n",
      "iteration=100 loss=687.852783203125\n",
      "iteration=120 loss=625.7869262695312\n",
      "iteration=140 loss=575.5350952148438\n",
      "iteration=160 loss=531.596435546875\n",
      "iteration=180 loss=491.82818603515625\n",
      "iteration=199 loss=460.21856689453125\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=60 loss=805.6924438476562\n",
      "iteration=80 loss=683.49951171875\n",
      "iteration=100 loss=603.1998291015625\n",
      "iteration=120 loss=545.5264282226562\n",
      "iteration=140 loss=502.804931640625\n",
      "iteration=160 loss=470.2015686035156\n",
      "iteration=180 loss=442.6883544921875\n",
      "iteration=199 loss=423.9693908691406\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=60 loss=728.4292602539062\n",
      "iteration=80 loss=633.3814086914062\n",
      "iteration=100 loss=568.012451171875\n",
      "iteration=120 loss=520.2413330078125\n",
      "iteration=140 loss=485.3612976074219\n",
      "iteration=160 loss=459.7163391113281\n",
      "iteration=180 loss=439.499755859375\n",
      "iteration=199 loss=423.7730712890625\n",
      "Accuracy: 0.011\n",
      "F1: 0.043\n",
      "Recall: 0.063\n",
      "Precision: 0.032\n",
      "Accuracy: 0.063\n",
      "F1: 0.488\n",
      "Recall: 0.358\n",
      "Precision: 0.762\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.031\n",
      "Recall: 0.025\n",
      "Precision: 0.040\n",
      "Accuracy: 0.029\n",
      "F1: 0.265\n",
      "Recall: 0.162\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "Accuracy: 0.045\n",
      "F1: 0.384\n",
      "Recall: 0.258\n",
      "Precision: 0.750\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.069\n",
      "F1: 0.519\n",
      "Recall: 0.394\n",
      "Precision: 0.762\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.280\n",
      "Recall: 0.917\n",
      "Precision: 0.165\n",
      "Accuracy: 0.107\n",
      "F1: 0.693\n",
      "Recall: 0.611\n",
      "Precision: 0.801\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.006\n",
      "F1: 0.068\n",
      "Recall: 0.036\n",
      "Precision: 0.522\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 0.998\n",
      "Precision: 0.176\n",
      "Accuracy: 0.115\n",
      "F1: 0.716\n",
      "Recall: 0.656\n",
      "Precision: 0.788\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.050\n",
      "Recall: 0.075\n",
      "Precision: 0.037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.057\n",
      "F1: 0.461\n",
      "Recall: 0.327\n",
      "Precision: 0.781\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.056\n",
      "F1: 0.453\n",
      "Recall: 0.320\n",
      "Precision: 0.777\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.069\n",
      "F1: 0.519\n",
      "Recall: 0.394\n",
      "Precision: 0.762\n",
      "\n",
      "200 1e-05\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=60 loss=908.0228271484375\n",
      "iteration=80 loss=760.16552734375\n",
      "iteration=100 loss=658.9944458007812\n",
      "iteration=120 loss=584.0817260742188\n",
      "iteration=140 loss=530.9254760742188\n",
      "iteration=160 loss=491.4053955078125\n",
      "iteration=180 loss=461.4503479003906\n",
      "iteration=199 loss=439.26202392578125\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=60 loss=715.3494873046875\n",
      "iteration=80 loss=612.602294921875\n",
      "iteration=100 loss=548.1060791015625\n",
      "iteration=120 loss=501.87310791015625\n",
      "iteration=140 loss=466.4742736816406\n",
      "iteration=160 loss=441.8441162109375\n",
      "iteration=180 loss=421.2817077636719\n",
      "iteration=199 loss=404.4196472167969\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=60 loss=853.886962890625\n",
      "iteration=80 loss=722.4713134765625\n",
      "iteration=100 loss=636.5574951171875\n",
      "iteration=120 loss=576.177734375\n",
      "iteration=140 loss=533.3216552734375\n",
      "iteration=160 loss=502.042724609375\n",
      "iteration=180 loss=478.6576843261719\n",
      "iteration=199 loss=462.13604736328125\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=60 loss=825.1005859375\n",
      "iteration=80 loss=696.3060302734375\n",
      "iteration=100 loss=616.915771484375\n",
      "iteration=120 loss=561.1666259765625\n",
      "iteration=140 loss=520.2505493164062\n",
      "iteration=160 loss=489.23638916015625\n",
      "iteration=180 loss=464.9713439941406\n",
      "iteration=199 loss=446.3029479980469\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=60 loss=816.7399291992188\n",
      "iteration=80 loss=698.6795654296875\n",
      "iteration=100 loss=627.4421997070312\n",
      "iteration=120 loss=578.1500244140625\n",
      "iteration=140 loss=541.8201904296875\n",
      "iteration=160 loss=513.1576538085938\n",
      "iteration=180 loss=489.2718200683594\n",
      "iteration=199 loss=469.3633117675781\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=60 loss=826.7042846679688\n",
      "iteration=80 loss=647.0758056640625\n",
      "iteration=100 loss=548.8148193359375\n",
      "iteration=120 loss=491.05242919921875\n",
      "iteration=140 loss=2972.516357421875\n",
      "iteration=160 loss=2682.64697265625\n",
      "iteration=180 loss=2597.65576171875\n",
      "iteration=199 loss=2548.941162109375\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=60 loss=971.8869018554688\n",
      "iteration=80 loss=817.0056762695312\n",
      "iteration=100 loss=720.9212036132812\n",
      "iteration=120 loss=657.0328369140625\n",
      "iteration=140 loss=608.8980102539062\n",
      "iteration=160 loss=571.4812622070312\n",
      "iteration=180 loss=542.8126831054688\n",
      "iteration=199 loss=520.6321411132812\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=60 loss=888.9819946289062\n",
      "iteration=80 loss=769.253662109375\n",
      "iteration=100 loss=687.852783203125\n",
      "iteration=120 loss=625.7869262695312\n",
      "iteration=140 loss=575.5350952148438\n",
      "iteration=160 loss=531.596435546875\n",
      "iteration=180 loss=491.82818603515625\n",
      "iteration=199 loss=460.21856689453125\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=60 loss=805.6924438476562\n",
      "iteration=80 loss=683.49951171875\n",
      "iteration=100 loss=603.1998291015625\n",
      "iteration=120 loss=545.5264282226562\n",
      "iteration=140 loss=502.804931640625\n",
      "iteration=160 loss=470.2015686035156\n",
      "iteration=180 loss=442.6883544921875\n",
      "iteration=199 loss=423.9693908691406\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=60 loss=728.4292602539062\n",
      "iteration=80 loss=633.3814086914062\n",
      "iteration=100 loss=568.012451171875\n",
      "iteration=120 loss=520.2413330078125\n",
      "iteration=140 loss=485.3612976074219\n",
      "iteration=160 loss=459.7163391113281\n",
      "iteration=180 loss=439.499755859375\n",
      "iteration=199 loss=423.7730712890625\n",
      "Accuracy: 0.011\n",
      "F1: 0.043\n",
      "Recall: 0.063\n",
      "Precision: 0.032\n",
      "Accuracy: 0.063\n",
      "F1: 0.488\n",
      "Recall: 0.358\n",
      "Precision: 0.762\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.031\n",
      "Recall: 0.025\n",
      "Precision: 0.040\n",
      "Accuracy: 0.029\n",
      "F1: 0.265\n",
      "Recall: 0.162\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "Accuracy: 0.045\n",
      "F1: 0.384\n",
      "Recall: 0.258\n",
      "Precision: 0.750\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.069\n",
      "F1: 0.519\n",
      "Recall: 0.394\n",
      "Precision: 0.762\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.280\n",
      "Recall: 0.917\n",
      "Precision: 0.165\n",
      "Accuracy: 0.107\n",
      "F1: 0.693\n",
      "Recall: 0.611\n",
      "Precision: 0.801\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.006\n",
      "F1: 0.068\n",
      "Recall: 0.036\n",
      "Precision: 0.522\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 0.998\n",
      "Precision: 0.176\n",
      "Accuracy: 0.115\n",
      "F1: 0.716\n",
      "Recall: 0.656\n",
      "Precision: 0.788\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.050\n",
      "Recall: 0.075\n",
      "Precision: 0.037\n",
      "Accuracy: 0.057\n",
      "F1: 0.461\n",
      "Recall: 0.327\n",
      "Precision: 0.781\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.056\n",
      "F1: 0.453\n",
      "Recall: 0.320\n",
      "Precision: 0.777\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.069\n",
      "F1: 0.519\n",
      "Recall: 0.394\n",
      "Precision: 0.762\n",
      "\n",
      "200 1e-06\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=60 loss=908.0228271484375\n",
      "iteration=80 loss=760.16552734375\n",
      "iteration=100 loss=658.9944458007812\n",
      "iteration=120 loss=584.0817260742188\n",
      "iteration=140 loss=530.9254760742188\n",
      "iteration=160 loss=491.4053955078125\n",
      "iteration=180 loss=461.4503479003906\n",
      "iteration=199 loss=439.26202392578125\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=60 loss=715.3494873046875\n",
      "iteration=80 loss=612.602294921875\n",
      "iteration=100 loss=548.1060791015625\n",
      "iteration=120 loss=501.87310791015625\n",
      "iteration=140 loss=466.4742736816406\n",
      "iteration=160 loss=441.8441162109375\n",
      "iteration=180 loss=421.2817077636719\n",
      "iteration=199 loss=404.4196472167969\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=60 loss=853.886962890625\n",
      "iteration=80 loss=722.4713134765625\n",
      "iteration=100 loss=636.5574951171875\n",
      "iteration=120 loss=576.177734375\n",
      "iteration=140 loss=533.3216552734375\n",
      "iteration=160 loss=502.042724609375\n",
      "iteration=180 loss=478.6576843261719\n",
      "iteration=199 loss=462.13604736328125\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=60 loss=825.1005859375\n",
      "iteration=80 loss=696.3060302734375\n",
      "iteration=100 loss=616.915771484375\n",
      "iteration=120 loss=561.1666259765625\n",
      "iteration=140 loss=520.2505493164062\n",
      "iteration=160 loss=489.23638916015625\n",
      "iteration=180 loss=464.9713439941406\n",
      "iteration=199 loss=446.3029479980469\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=60 loss=816.7399291992188\n",
      "iteration=80 loss=698.6795654296875\n",
      "iteration=100 loss=627.4421997070312\n",
      "iteration=120 loss=578.1500244140625\n",
      "iteration=140 loss=541.8201904296875\n",
      "iteration=160 loss=513.1576538085938\n",
      "iteration=180 loss=489.2718200683594\n",
      "iteration=199 loss=469.3633117675781\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=60 loss=826.7042846679688\n",
      "iteration=80 loss=647.0758056640625\n",
      "iteration=100 loss=548.8148193359375\n",
      "iteration=120 loss=491.05242919921875\n",
      "iteration=140 loss=2972.516357421875\n",
      "iteration=160 loss=2682.64697265625\n",
      "iteration=180 loss=2597.65576171875\n",
      "iteration=199 loss=2548.941162109375\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=60 loss=971.8869018554688\n",
      "iteration=80 loss=817.0056762695312\n",
      "iteration=100 loss=720.9212036132812\n",
      "iteration=120 loss=657.0328369140625\n",
      "iteration=140 loss=608.8980102539062\n",
      "iteration=160 loss=571.4812622070312\n",
      "iteration=180 loss=542.8126831054688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=199 loss=520.6321411132812\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=60 loss=888.9819946289062\n",
      "iteration=80 loss=769.253662109375\n",
      "iteration=100 loss=687.852783203125\n",
      "iteration=120 loss=625.7869262695312\n",
      "iteration=140 loss=575.5350952148438\n",
      "iteration=160 loss=531.596435546875\n",
      "iteration=180 loss=491.82818603515625\n",
      "iteration=199 loss=460.21856689453125\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=60 loss=805.6924438476562\n",
      "iteration=80 loss=683.49951171875\n",
      "iteration=100 loss=603.1998291015625\n",
      "iteration=120 loss=545.5264282226562\n",
      "iteration=140 loss=502.804931640625\n",
      "iteration=160 loss=470.2015686035156\n",
      "iteration=180 loss=442.6883544921875\n",
      "iteration=199 loss=423.9693908691406\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=60 loss=728.4292602539062\n",
      "iteration=80 loss=633.3814086914062\n",
      "iteration=100 loss=568.012451171875\n",
      "iteration=120 loss=520.2413330078125\n",
      "iteration=140 loss=485.3612976074219\n",
      "iteration=160 loss=459.7163391113281\n",
      "iteration=180 loss=439.499755859375\n",
      "iteration=199 loss=423.7730712890625\n",
      "Accuracy: 0.011\n",
      "F1: 0.043\n",
      "Recall: 0.063\n",
      "Precision: 0.032\n",
      "Accuracy: 0.063\n",
      "F1: 0.488\n",
      "Recall: 0.358\n",
      "Precision: 0.762\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.031\n",
      "Recall: 0.025\n",
      "Precision: 0.040\n",
      "Accuracy: 0.029\n",
      "F1: 0.265\n",
      "Recall: 0.162\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "Accuracy: 0.045\n",
      "F1: 0.384\n",
      "Recall: 0.258\n",
      "Precision: 0.750\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.069\n",
      "F1: 0.519\n",
      "Recall: 0.394\n",
      "Precision: 0.762\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.280\n",
      "Recall: 0.917\n",
      "Precision: 0.165\n",
      "Accuracy: 0.107\n",
      "F1: 0.693\n",
      "Recall: 0.611\n",
      "Precision: 0.801\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.006\n",
      "F1: 0.068\n",
      "Recall: 0.036\n",
      "Precision: 0.522\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 0.998\n",
      "Precision: 0.176\n",
      "Accuracy: 0.115\n",
      "F1: 0.716\n",
      "Recall: 0.656\n",
      "Precision: 0.788\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.050\n",
      "Recall: 0.075\n",
      "Precision: 0.037\n",
      "Accuracy: 0.057\n",
      "F1: 0.461\n",
      "Recall: 0.327\n",
      "Precision: 0.781\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.056\n",
      "F1: 0.453\n",
      "Recall: 0.320\n",
      "Precision: 0.777\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.069\n",
      "F1: 0.519\n",
      "Recall: 0.394\n",
      "Precision: 0.762\n",
      "\n",
      "200 1e-07\n",
      "0\n",
      "iteration=0 loss=6488.87841796875\n",
      "iteration=20 loss=1800.0379638671875\n",
      "iteration=40 loss=1196.556640625\n",
      "iteration=60 loss=908.0228271484375\n",
      "iteration=80 loss=760.16552734375\n",
      "iteration=100 loss=658.9944458007812\n",
      "iteration=120 loss=584.0817260742188\n",
      "iteration=140 loss=530.9254760742188\n",
      "iteration=160 loss=491.4053955078125\n",
      "iteration=180 loss=461.4503479003906\n",
      "iteration=199 loss=439.26202392578125\n",
      "1\n",
      "iteration=0 loss=7427.11376953125\n",
      "iteration=20 loss=1429.170654296875\n",
      "iteration=40 loss=904.5748291015625\n",
      "iteration=60 loss=715.3494873046875\n",
      "iteration=80 loss=612.602294921875\n",
      "iteration=100 loss=548.1060791015625\n",
      "iteration=120 loss=501.87310791015625\n",
      "iteration=140 loss=466.4742736816406\n",
      "iteration=160 loss=441.8441162109375\n",
      "iteration=180 loss=421.2817077636719\n",
      "iteration=199 loss=404.4196472167969\n",
      "2\n",
      "iteration=0 loss=7071.05126953125\n",
      "iteration=20 loss=1657.4114990234375\n",
      "iteration=40 loss=1090.309326171875\n",
      "iteration=60 loss=853.886962890625\n",
      "iteration=80 loss=722.4713134765625\n",
      "iteration=100 loss=636.5574951171875\n",
      "iteration=120 loss=576.177734375\n",
      "iteration=140 loss=533.3216552734375\n",
      "iteration=160 loss=502.042724609375\n",
      "iteration=180 loss=478.6576843261719\n",
      "iteration=199 loss=462.13604736328125\n",
      "3\n",
      "iteration=0 loss=4940.92236328125\n",
      "iteration=20 loss=1701.636962890625\n",
      "iteration=40 loss=1085.40478515625\n",
      "iteration=60 loss=825.1005859375\n",
      "iteration=80 loss=696.3060302734375\n",
      "iteration=100 loss=616.915771484375\n",
      "iteration=120 loss=561.1666259765625\n",
      "iteration=140 loss=520.2505493164062\n",
      "iteration=160 loss=489.23638916015625\n",
      "iteration=180 loss=464.9713439941406\n",
      "iteration=199 loss=446.3029479980469\n",
      "4\n",
      "iteration=0 loss=7040.8896484375\n",
      "iteration=20 loss=1767.0186767578125\n",
      "iteration=40 loss=1071.38232421875\n",
      "iteration=60 loss=816.7399291992188\n",
      "iteration=80 loss=698.6795654296875\n",
      "iteration=100 loss=627.4421997070312\n",
      "iteration=120 loss=578.1500244140625\n",
      "iteration=140 loss=541.8201904296875\n",
      "iteration=160 loss=513.1576538085938\n",
      "iteration=180 loss=489.2718200683594\n",
      "iteration=199 loss=469.3633117675781\n",
      "5\n",
      "iteration=0 loss=6987.1318359375\n",
      "iteration=20 loss=1762.215087890625\n",
      "iteration=40 loss=1161.2279052734375\n",
      "iteration=60 loss=826.7042846679688\n",
      "iteration=80 loss=647.0758056640625\n",
      "iteration=100 loss=548.8148193359375\n",
      "iteration=120 loss=491.05242919921875\n",
      "iteration=140 loss=2972.516357421875\n",
      "iteration=160 loss=2682.64697265625\n",
      "iteration=180 loss=2597.65576171875\n",
      "iteration=199 loss=2548.941162109375\n",
      "6\n",
      "iteration=0 loss=9812.7412109375\n",
      "iteration=20 loss=1906.3740234375\n",
      "iteration=40 loss=1253.12841796875\n",
      "iteration=60 loss=971.8869018554688\n",
      "iteration=80 loss=817.0056762695312\n",
      "iteration=100 loss=720.9212036132812\n",
      "iteration=120 loss=657.0328369140625\n",
      "iteration=140 loss=608.8980102539062\n",
      "iteration=160 loss=571.4812622070312\n",
      "iteration=180 loss=542.8126831054688\n",
      "iteration=199 loss=520.6321411132812\n",
      "7\n",
      "iteration=0 loss=12266.5830078125\n",
      "iteration=20 loss=1801.80126953125\n",
      "iteration=40 loss=1131.6474609375\n",
      "iteration=60 loss=888.9819946289062\n",
      "iteration=80 loss=769.253662109375\n",
      "iteration=100 loss=687.852783203125\n",
      "iteration=120 loss=625.7869262695312\n",
      "iteration=140 loss=575.5350952148438\n",
      "iteration=160 loss=531.596435546875\n",
      "iteration=180 loss=491.82818603515625\n",
      "iteration=199 loss=460.21856689453125\n",
      "8\n",
      "iteration=0 loss=12965.1884765625\n",
      "iteration=20 loss=1689.99365234375\n",
      "iteration=40 loss=1043.099853515625\n",
      "iteration=60 loss=805.6924438476562\n",
      "iteration=80 loss=683.49951171875\n",
      "iteration=100 loss=603.1998291015625\n",
      "iteration=120 loss=545.5264282226562\n",
      "iteration=140 loss=502.804931640625\n",
      "iteration=160 loss=470.2015686035156\n",
      "iteration=180 loss=442.6883544921875\n",
      "iteration=199 loss=423.9693908691406\n",
      "9\n",
      "iteration=0 loss=7130.74853515625\n",
      "iteration=20 loss=1285.5823974609375\n",
      "iteration=40 loss=887.8050537109375\n",
      "iteration=60 loss=728.4292602539062\n",
      "iteration=80 loss=633.3814086914062\n",
      "iteration=100 loss=568.012451171875\n",
      "iteration=120 loss=520.2413330078125\n",
      "iteration=140 loss=485.3612976074219\n",
      "iteration=160 loss=459.7163391113281\n",
      "iteration=180 loss=439.499755859375\n",
      "iteration=199 loss=423.7730712890625\n",
      "Accuracy: 0.011\n",
      "F1: 0.043\n",
      "Recall: 0.063\n",
      "Precision: 0.032\n",
      "Accuracy: 0.063\n",
      "F1: 0.488\n",
      "Recall: 0.358\n",
      "Precision: 0.762\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.031\n",
      "Recall: 0.025\n",
      "Precision: 0.040\n",
      "Accuracy: 0.029\n",
      "F1: 0.265\n",
      "Recall: 0.162\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "Accuracy: 0.045\n",
      "F1: 0.384\n",
      "Recall: 0.258\n",
      "Precision: 0.750\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.069\n",
      "F1: 0.519\n",
      "Recall: 0.394\n",
      "Precision: 0.762\n",
      "\n",
      "Accuracy: 0.161\n",
      "F1: 0.280\n",
      "Recall: 0.917\n",
      "Precision: 0.165\n",
      "Accuracy: 0.107\n",
      "F1: 0.693\n",
      "Recall: 0.611\n",
      "Precision: 0.801\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.006\n",
      "F1: 0.068\n",
      "Recall: 0.036\n",
      "Precision: 0.522\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 0.998\n",
      "Precision: 0.176\n",
      "Accuracy: 0.115\n",
      "F1: 0.716\n",
      "Recall: 0.656\n",
      "Precision: 0.788\n",
      "\n",
      "Accuracy: 0.013\n",
      "F1: 0.050\n",
      "Recall: 0.075\n",
      "Precision: 0.037\n",
      "Accuracy: 0.057\n",
      "F1: 0.461\n",
      "Recall: 0.327\n",
      "Precision: 0.781\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.056\n",
      "F1: 0.453\n",
      "Recall: 0.320\n",
      "Precision: 0.777\n",
      "\n",
      "Accuracy: 0.020\n",
      "F1: 0.073\n",
      "Recall: 0.114\n",
      "Precision: 0.053\n",
      "Accuracy: 0.069\n",
      "F1: 0.519\n",
      "Recall: 0.394\n",
      "Precision: 0.762\n",
      "\n",
      "[25, 8, DPLabelModel(), 0.0562457337883959, 0.11016042780748664, 0.31962761830876646, 0.06654821515102569, 0.14129692832764504, 0.8227344992050875, 0.8029480217222653, 0.843520782396088]\n",
      "CPU times: user 2h 21min 28s, sys: 10min 2s, total: 2h 31min 31s\n",
      "Wall time: 1h 52min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best = None\n",
    "for iterations in [1, 5, 10, 25, 50, 100, 200]:\n",
    "    for learning_rate in [1e-4, 1e-5, 1e-6, 1e-7]:\n",
    "        print(iterations, learning_rate)\n",
    "        max_seed = 10\n",
    "        temporal_models = [None,]*max_seed\n",
    "        for seed in range(max_seed):\n",
    "            print(seed)\n",
    "            markov_model = DPLabelModel(m=m_per_task*T, \n",
    "                                        T=T,\n",
    "                                        edges=[(i,i+m_per_task) for i in range((T-1)*m_per_task)],\n",
    "                                        coverage_sets=[[t,] for t in range(T) for _ in range(m_per_task)],\n",
    "                                        mu_sharing=[[t*m_per_task+i for t in range(T)] for i in range(m_per_task)],\n",
    "                                        phi_sharing=[[(t*m_per_task+i, (t+1)*m_per_task+i)\n",
    "                                                      for t in range(T-1)] for i in range(m_per_task)],\n",
    "                                        device=device,\n",
    "                                        class_balance=torch.tensor(class_balance).float().to(device),\n",
    "                                        seed=seed)\n",
    "            optimize(markov_model, L_hat=MRI_data_temporal['Li_train'], num_iter=iterations,\n",
    "                     lr=1e-5, momentum=0.8, clamp=True, \n",
    "                     verbose=iterations >= 10, seed=seed)\n",
    "            temporal_models[seed] = markov_model\n",
    "\n",
    "        for seed, model in enumerate(temporal_models):\n",
    "            Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            scores = [iterations, seed, model]\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "                \n",
    "            model.flip_params()\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "            \n",
    "            model.flip_params()\n",
    "\n",
    "            if best == None or scores[4] > max(best[4], best[8]) or scores[8] > max(best[4], best[8]):\n",
    "                best = scores\n",
    "            print()\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.056\n",
      "F1: 0.110\n",
      "Recall: 0.320\n",
      "Precision: 0.067\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.141\n",
      "F1: 0.823\n",
      "Recall: 0.803\n",
      "Precision: 0.844\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5767.,  176.,   59.,   45.,   51.,   62.,   55.,  118.,  156.,\n",
       "         836.]),\n",
       " array([4.37341436e-09, 9.99996130e-02, 1.99999222e-01, 2.99998830e-01,\n",
       "        3.99998439e-01, 4.99998047e-01, 5.99997656e-01, 6.99997265e-01,\n",
       "        7.99996873e-01, 8.99996482e-01, 9.99996090e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_frame_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, 'models/ts_labelmodel_best_tuning.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best[2], 'models/ts_labelmodel_best_many_iterations.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_preds = best[2].predict_element_proba(Li_dev.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5450.,  253.,   82.,   44.,   41.,   22.,   15.,   33.,   52.,\n",
       "        1333.]),\n",
       " array([1.20012491e-05, 1.00009002e-01, 2.00006003e-01, 3.00003004e-01,\n",
       "        4.00000005e-01, 4.99997006e-01, 5.99994007e-01, 6.99991008e-01,\n",
       "        7.99988009e-01, 8.99985010e-01, 9.99982011e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(best_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.054\n",
      "Recall: 0.029\n",
      "Precision: 0.493\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.176\n",
      "F1: 0.299\n",
      "Recall: 1.000\n",
      "Precision: 0.176\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.068\n",
      "Recall: 0.036\n",
      "Precision: 0.522\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed, model in enumerate(temporal_models):\n",
    "    model.flip_params()\n",
    "    Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "    R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "    scores = [iterations, seed, model]\n",
    "    for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "        score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "        print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    if best == None or scores[4] > best[4]:\n",
    "        best = scores\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500, 0, DPLabelModel(), 0.17597269624573378, 0.2994192799070848, 1.0, 0.17606884305422757]\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best, 'models/ts_labelmodel_best_2500.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_pred_frame_label = best[2].predict_element_proba(Li_dev.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 7.321e+03]),\n",
       " array([0.01257361, 0.11131625, 0.21005889, 0.30880153, 0.40754417,\n",
       "        0.50628681, 0.60502945, 0.70377209, 0.80251473, 0.90125737,\n",
       "        1.00000001]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_frame_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = markov_model.predict_element_proba(Li_dev.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7325,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.154\n",
      "F1: 0.377\n",
      "Recall: 0.876\n",
      "Precision: 0.241\n"
     ]
    }
   ],
   "source": [
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "        score = metric_score(Y_dev.cpu(), np.round(preds), metric)\n",
    "        print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99934805, 0.9738352 , 0.97474937, 0.93077044, 0.39215547,\n",
       "       0.06551916, 0.03445013, 0.03749803, 0.28004386, 0.77988954])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.516888  , 0.516888  , 0.516888  , 0.516888  , 0.48311198],\n",
       "       [0.53924423, 0.53924423, 0.53924423, 0.53924423, 0.46075577]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_probs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1638.,  276.,  236.,  135.,  346.,  731.,  854.,  962.,  534.,\n",
       "        1613.]),\n",
       " array([3.50714667e-04, 1.00315638e-01, 2.00280562e-01, 3.00245486e-01,\n",
       "        4.00210410e-01, 5.00175333e-01, 6.00140257e-01, 7.00105181e-01,\n",
       "        8.00070105e-01, 9.00035028e-01, 9.99999952e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.41327795e-05, 0.00000000e+00, 2.17160606e-03, ...,\n",
       "        4.27972118e-05, 0.00000000e+00, 3.89957458e-01],\n",
       "       [2.02080816e-01, 0.00000000e+00, 9.10843536e-03, ...,\n",
       "        7.04662083e-03, 0.00000000e+00, 5.14468737e-03],\n",
       "       [6.88242763e-02, 0.00000000e+00, 2.51607329e-01, ...,\n",
       "        7.77709298e-04, 0.00000000e+00, 4.60530259e-02],\n",
       "       ...,\n",
       "       [1.05558075e-02, 0.00000000e+00, 3.71003598e-02, ...,\n",
       "        7.99736604e-02, 0.00000000e+00, 2.26837486e-01],\n",
       "       [9.67053056e-05, 0.00000000e+00, 5.52507641e-04, ...,\n",
       "        4.20493714e-04, 0.00000000e+00, 2.81781822e-01],\n",
       "       [1.14595937e-03, 0.00000000e+00, 4.02768841e-03, ...,\n",
       "        7.53226504e-02, 0.00000000e+00, 2.13645339e-01]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(R_pred_frame.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1523.,  724., 1270.,  681.,  958.,  720.,  430.,  350.,  427.,\n",
       "         242.]),\n",
       " array([0.08463663, 0.17535003, 0.26606343, 0.35677683, 0.44749023,\n",
       "        0.53820363, 0.62891703, 0.71963043, 0.81034383, 0.90105723,\n",
       "        0.99177063]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_probs.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.3900],\n",
       "        [0.2021, 0.0000, 0.0091,  ..., 0.0070, 0.0000, 0.0051],\n",
       "        [0.0688, 0.0000, 0.2516,  ..., 0.0008, 0.0000, 0.0461],\n",
       "        ...,\n",
       "        [0.0106, 0.0000, 0.0371,  ..., 0.0800, 0.0000, 0.2268],\n",
       "        [0.0001, 0.0000, 0.0006,  ..., 0.0004, 0.0000, 0.2818],\n",
       "        [0.0011, 0.0000, 0.0040,  ..., 0.0753, 0.0000, 0.2136]],\n",
       "       device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0022, 0.0209, 0.0000, 0.0000, 0.0000, 0.0455, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0005, 0.0000, 0.0218, 0.5169, 0.0000, 0.0000,\n",
       "        0.0001, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0015,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.3900],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feasible_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -1, -1, -1, -1],\n",
       "        [-1,  1, -1, -1, -1],\n",
       "        [ 1, -1, -1, -1, -1],\n",
       "        [ 1,  1, -1, -1, -1],\n",
       "        [-1, -1,  1, -1, -1],\n",
       "        [-1,  1,  1, -1, -1],\n",
       "        [ 1, -1,  1, -1, -1],\n",
       "        [ 1,  1,  1, -1, -1],\n",
       "        [-1, -1, -1,  1, -1],\n",
       "        [-1,  1, -1,  1, -1],\n",
       "        [ 1, -1, -1,  1, -1],\n",
       "        [ 1,  1, -1,  1, -1],\n",
       "        [-1, -1,  1,  1, -1],\n",
       "        [-1,  1,  1,  1, -1],\n",
       "        [ 1, -1,  1,  1, -1],\n",
       "        [ 1,  1,  1,  1, -1],\n",
       "        [-1, -1, -1, -1,  1],\n",
       "        [-1,  1, -1, -1,  1],\n",
       "        [ 1, -1, -1, -1,  1],\n",
       "        [ 1,  1, -1, -1,  1],\n",
       "        [-1, -1,  1, -1,  1],\n",
       "        [-1,  1,  1, -1,  1],\n",
       "        [ 1, -1,  1, -1,  1],\n",
       "        [ 1,  1,  1, -1,  1],\n",
       "        [-1, -1, -1,  1,  1],\n",
       "        [-1,  1, -1,  1,  1],\n",
       "        [ 1, -1, -1,  1,  1],\n",
       "        [ 1,  1, -1,  1,  1],\n",
       "        [-1, -1,  1,  1,  1],\n",
       "        [-1,  1,  1,  1,  1],\n",
       "        [ 1, -1,  1,  1,  1],\n",
       "        [ 1,  1,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feasible_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1465, 32])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99934804, 0.9738352 , 0.97474945, 0.93077046, 0.39215547,\n",
       "       0.06551916, 0.03445013, 0.03749803, 0.28004387, 0.7798896 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(R_pred_frame, torch.where(\n",
    "    model.feasible_y==-1,torch.tensor(0).to(device),model.feasible_y).float()\n",
    ").detach().cpu().numpy().ravel()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = model.feasible_y.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[arr == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [1, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99934805, 0.9738352 , 0.97474937, ..., 0.58535333, 0.58560057,\n",
       "       0.89727236])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feasible_y_np = model.feasible_y.detach().cpu().numpy()\n",
    "feasible_y_np[feasible_y_np == -1] = 0\n",
    "np.matmul(\n",
    "    R_pred_frame.detach().cpu().numpy(),\n",
    "    feasible_y_np\n",
    ").ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0022, 0.0209, 0.0000, 0.0000, 0.0000, 0.0455, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0005, 0.0000, 0.0218, 0.5169, 0.0000, 0.0000,\n",
       "         0.0001, 0.0005, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0015,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.3900],\n",
       "        [0.2021, 0.0000, 0.0091, 0.0045, 0.0000, 0.0020, 0.0000, 0.0003, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0019, 0.0000, 0.0001, 0.0001, 0.4608, 0.0000,\n",
       "         0.0258, 0.0084, 0.0000, 0.0069, 0.0000, 0.0000, 0.2397, 0.0000, 0.0121,\n",
       "         0.0000, 0.0140, 0.0070, 0.0000, 0.0051]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -1, -1, -1, -1],\n",
       "        [-1,  1, -1, -1, -1],\n",
       "        [ 1, -1, -1, -1, -1],\n",
       "        [ 1,  1, -1, -1, -1],\n",
       "        [-1, -1,  1, -1, -1],\n",
       "        [-1,  1,  1, -1, -1],\n",
       "        [ 1, -1,  1, -1, -1],\n",
       "        [ 1,  1,  1, -1, -1],\n",
       "        [-1, -1, -1,  1, -1],\n",
       "        [-1,  1, -1,  1, -1],\n",
       "        [ 1, -1, -1,  1, -1],\n",
       "        [ 1,  1, -1,  1, -1],\n",
       "        [-1, -1,  1,  1, -1],\n",
       "        [-1,  1,  1,  1, -1],\n",
       "        [ 1, -1,  1,  1, -1],\n",
       "        [ 1,  1,  1,  1, -1],\n",
       "        [-1, -1, -1, -1,  1],\n",
       "        [-1,  1, -1, -1,  1],\n",
       "        [ 1, -1, -1, -1,  1],\n",
       "        [ 1,  1, -1, -1,  1],\n",
       "        [-1, -1,  1, -1,  1],\n",
       "        [-1,  1,  1, -1,  1],\n",
       "        [ 1, -1,  1, -1,  1],\n",
       "        [ 1,  1,  1, -1,  1],\n",
       "        [-1, -1, -1,  1,  1],\n",
       "        [-1,  1, -1,  1,  1],\n",
       "        [ 1, -1, -1,  1,  1],\n",
       "        [ 1,  1, -1,  1,  1],\n",
       "        [-1, -1,  1,  1,  1],\n",
       "        [-1,  1,  1,  1,  1],\n",
       "        [ 1, -1,  1,  1,  1],\n",
       "        [ 1,  1,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feasible_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.3900],\n",
       "        [0.2021, 0.0000, 0.0091,  ..., 0.0070, 0.0000, 0.0051],\n",
       "        [0.0688, 0.0000, 0.2516,  ..., 0.0008, 0.0000, 0.0461],\n",
       "        ...,\n",
       "        [0.0106, 0.0000, 0.0371,  ..., 0.0800, 0.0000, 0.2268],\n",
       "        [0.0001, 0.0000, 0.0006,  ..., 0.0004, 0.0000, 0.2818],\n",
       "        [0.0011, 0.0000, 0.0040,  ..., 0.0753, 0.0000, 0.2136]],\n",
       "       device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2021, 0.0688,  ..., 0.0106, 0.0001, 0.0011],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0022, 0.0091, 0.2516,  ..., 0.0371, 0.0006, 0.0040],\n",
       "        ...,\n",
       "        [0.0000, 0.0070, 0.0008,  ..., 0.0800, 0.0004, 0.0753],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3900, 0.0051, 0.0461,  ..., 0.2268, 0.2818, 0.2136]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9987,  0.9477,  0.9495,  0.8615, -0.2157],\n",
       "        [-0.8690, -0.9311, -0.9250, -0.4399,  0.5598],\n",
       "        [ 0.6989, -0.4822, -0.8752, -0.6246,  0.0817],\n",
       "        ...,\n",
       "        [ 0.5028,  0.2961,  0.2270,  0.2729,  0.1078],\n",
       "        [ 0.9919,  0.9809,  0.2497,  0.0447, -0.1122],\n",
       "        [ 0.2565,  0.3115,  0.1707,  0.1712,  0.7945]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(model.feasible_y.transpose(dim0=0, dim1=1).float(), R_pred_frame.transpose(0, 1)).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dot: Expected 1-D argument self, but got 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9896a5f7d120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeasible_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_pred_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: dot: Expected 1-D argument self, but got 2-D"
     ]
    }
   ],
   "source": [
    "torch.dot(model.feasible_y.T, R_pred_frame[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1465, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_frame[0].abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.511\n",
      "F1: 0.304\n",
      "Recall: 0.605\n",
      "Precision: 0.203\n"
     ]
    }
   ],
   "source": [
    "Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "#find sequence label config. with highest prob.\n",
    "config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "R_pred_config = model.feasible_y[config_index]\n",
    "R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "#for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "for idx in range(R_pred_config.shape[0]):\n",
    "    R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "R_pred_probs = R_pred_probs.numpy()\n",
    "R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "\n",
    "Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "#find sequence label config. with highest prob.\n",
    "config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "R_pred_config = model.feasible_y[config_index]\n",
    "R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "#for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "for idx in range(R_pred_config.shape[0]):\n",
    "    R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "R_pred_probs = R_pred_probs.numpy()\n",
    "R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "R_pred_frame_label = np.round(R_pred_probs.ravel())\n",
    "R_pred_frame_label[R_pred_frame_label == 0.] = 2.\n",
    "\n",
    "scores = [iterations, seed, model]\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), R_pred_frame_label, metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.939\n",
      "F1: 0.827\n",
      "Recall: 0.836\n",
      "Precision: 0.819\n"
     ]
    }
   ],
   "source": [
    "model = best_model\n",
    "Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "#find sequence label config. with highest prob.\n",
    "config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "R_pred_config = model.feasible_y[config_index]\n",
    "R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "#for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "for idx in range(R_pred_config.shape[0]):\n",
    "    R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "R_pred_probs = R_pred_probs.numpy()\n",
    "R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "\n",
    "Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "#find sequence label config. with highest prob.\n",
    "config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "R_pred_config = model.feasible_y[config_index]\n",
    "R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "#for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "for idx in range(R_pred_config.shape[0]):\n",
    "    R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "R_pred_probs = R_pred_probs.numpy()\n",
    "R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "R_pred_frame_label = np.round(R_pred_probs.ravel())\n",
    "R_pred_frame_label[R_pred_frame_label == 0.] = 2.\n",
    "\n",
    "scores = [iterations]\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), R_pred_frame_label, metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/ts_labelmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel_best_many_iterations.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel_best_tuning.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.159\n",
      "F1: 0.805\n",
      "Recall: 0.761\n",
      "Precision: 0.855\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = model.eval().predict_element_proba(\n",
    "    MRI_data_temporal['Li_test'].to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_test.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions for everything and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sparse\n",
    "import pickle\n",
    "import rekall\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "from rekall.interval_list import IntervalList\n",
    "from rekall.temporal_predicates import *\n",
    "from metal.label_model.baselines import MajorityLabelVoter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load manually annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:00<00:00, 6041.49it/s]\n",
      "100%|| 28/28 [00:00<00:00, 39608.94it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/manually_annotated_shots.pkl', 'rb') as f:\n",
    "    shots = VideoIntervalCollection(pickle.load(f))\n",
    "with open('../../data/shot_detection_folds.pkl', 'rb') as f:\n",
    "    shot_detection_folds = pickle.load(f)\n",
    "clips = shots.dilate(1).coalesce().dilate(-1)\n",
    "shot_boundaries = shots.map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.start, intrvl.payload)\n",
    ").set_union(\n",
    "    shots.map(lambda intrvl: (intrvl.end + 1, intrvl.end + 1, intrvl.payload))\n",
    ").coalesce()\n",
    "boundary_frames = {\n",
    "    video_id: [\n",
    "        intrvl.start\n",
    "        for intrvl in shot_boundaries.get_intervallist(video_id).get_intervals()\n",
    "    ]\n",
    "    for video_id in shot_boundaries.get_allintervals()\n",
    "}\n",
    "video_ids = sorted(list(clips.get_allintervals().keys()))\n",
    "frames_per_video = {\n",
    "    video_id: sorted([\n",
    "        f\n",
    "        for interval in clips.get_intervallist(video_id).get_intervals()\n",
    "        for f in range(interval.start, interval.end + 2)\n",
    "    ])\n",
    "    for video_id in video_ids\n",
    "}\n",
    "ground_truth = {\n",
    "    video_id: [\n",
    "        1 if f in boundary_frames[video_id] else 2\n",
    "        for f in frames_per_video[video_id]\n",
    "    ] \n",
    "    for video_id in video_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load label matrix with all frames in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/all_labels.pkl', 'rb') as f:\n",
    "    weak_labels_all_movies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load videos and number of frames per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/frame_counts.pkl', 'rb') as f:\n",
    "    frame_counts = pickle.load(f)\n",
    "video_ids_all = sorted(list(frame_counts.keys()))\n",
    "video_ids_train = sorted(list(set(video_ids_all).difference(set(video_ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct windows for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, construct windows of 16 frames for each video\n",
    "windows = VideoIntervalCollection({\n",
    "    video_id: [\n",
    "        (f, f + 16, video_id)\n",
    "        for f in range(0, frame_counts[video_id] - 16, 8)\n",
    "    ]\n",
    "    for video_id in video_ids_all\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truth labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, intersect the windows with ground truth and get ground truth labels for the windows\n",
    "windows_intersecting_ground_truth = windows.filter_against(\n",
    "    clips,\n",
    "    predicate=overlaps()\n",
    ").map(lambda intrvl: (intrvl.start, intrvl.end, 2))\n",
    "windows_with_shot_boundaries = windows_intersecting_ground_truth.filter_against(\n",
    "    shot_boundaries,\n",
    "    predicate = lambda window, shot_boundary:\n",
    "        shot_boundary.start >= window.start and shot_boundary.start < window.end\n",
    ").map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.end, 1)\n",
    ")\n",
    "windows_with_labels = windows_with_shot_boundaries.set_union(\n",
    "    windows_intersecting_ground_truth\n",
    ").coalesce(\n",
    "    predicate = equal(),\n",
    "    payload_merge_op = lambda p1, p2: min(p1, p2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weak labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label windows with the weak labels in our labeling functions\n",
    "def label_window(per_frame_weak_labels):\n",
    "    if 1 in per_frame_weak_labels:\n",
    "        return 1\n",
    "    if len([l for l in per_frame_weak_labels if l == 2]) >= len(per_frame_weak_labels) / 2:\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "windows_with_weak_labels = windows.map(\n",
    "    lambda window: (\n",
    "        window.start,\n",
    "        window.end,\n",
    "        [\n",
    "            label_window([\n",
    "                lf[window.payload][f-1]\n",
    "                for f in range(window.start, window.end)\n",
    "            ])\n",
    "            for lf in weak_labels_all_movies\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_everything_windows = csr_matrix([\n",
    "    intrvl.payload\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows.npy', 'wb') as f:\n",
    "    np.save(f, L_everything_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows.npy', 'rb') as f:\n",
    "    L_everything_windows = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert L matrix to timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5\n",
    "m_per_task = L_everything_windows.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled = torch.FloatTensor(L_everything_windows[:L_everything_windows.shape[0] -\n",
    "                                                      (L_everything_windows.shape[0] % T)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_per_task_unlabelled = L_unlabelled.size(1)\n",
    "n_frames_unlabelled = L_unlabelled.size(0)\n",
    "n_patients_unlabelled = n_frames_unlabelled//T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled_ts = torch.LongTensor(\n",
    "    L_unlabelled.view(n_patients_unlabelled, (m_per_task*T)).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2470104"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled_ts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val = model.eval().predict_element_proba(MRI_data_temporal['Li_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3068.,   80.,   32.,   30.,   18.,   18.,   28.,   34.,   57.,\n",
       "         405.]),\n",
       " array([8.55662978e-09, 9.99996262e-02, 1.99999244e-01, 2.99998862e-01,\n",
       "        3.99998479e-01, 4.99998097e-01, 5.99997715e-01, 6.99997332e-01,\n",
       "        7.99996950e-01, 8.99996568e-01, 9.99996185e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.951\n",
      "F1: 0.832\n",
      "Recall: 0.829\n",
      "Precision: 0.834\n"
     ]
    }
   ],
   "source": [
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu().where(Y_dev.cpu() == torch.tensor(1.), torch.tensor(0.)),\n",
    "                         np.round(predictions_val), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev.cpu().where(Y_dev.cpu() == torch.tensor(1.), torch.tensor(0.)).numpy()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(predictions_val)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40581259e-01, 1.40227477e-01, 1.61227802e-04, 3.84301745e-06,\n",
       "       5.94221284e-04, 1.06778730e-01, 2.09982789e-02, 2.54221653e-02,\n",
       "       1.38871080e-02, 1.46137525e-02])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_val[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2., 2., 2., 2., 2., 2., 2., 2.], device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n"
     ]
    }
   ],
   "source": [
    "predictions_everything = []\n",
    "for i in range(0, L_unlabelled_ts.shape[0], 100000):\n",
    "    print(i)\n",
    "    start = i\n",
    "    end = i + 100000\n",
    "    labels = L_unlabelled_ts[start:end] if end < L_unlabelled_ts.shape[0] else L_unlabelled_ts[start:]\n",
    "    predictions_for_labels = model.eval().predict_element_proba(labels.to(device))\n",
    "    predictions_everything.append(predictions_for_labels)\n",
    "    del predictions_for_labels\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12350520,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(predictions_everything).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_pred_probs_per_frame = np.concatenate(predictions_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_pred_frame = predictions_everything_together\n",
    "\n",
    "# #find sequence label config. with highest prob.\n",
    "# config_index = np.argmax(R_pred_frame, axis=1)\n",
    "# R_pred_config = model.feasible_y[config_index].detach().cpu()\n",
    "# R_pred_max = torch.FloatTensor(np.max(R_pred_frame.numpy(), axis=1))\n",
    "\n",
    "# #for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "# R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "# for idx in range(R_pred_config.shape[0]):\n",
    "#     R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:]).float()*R_pred_max[idx]\n",
    "\n",
    "# R_pred_probs = R_pred_probs.numpy()\n",
    "# R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "# R_pred_frame_label = np.round(R_pred_probs.ravel())\n",
    "# R_pred_frame_label[R_pred_frame_label == 0.] = 2.\n",
    "\n",
    "# R_pred_probs_per_frame = R_pred_probs.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.3420200e+05, 2.1149000e+04, 7.7870000e+03, 6.8100000e+03,\n",
       "        8.7200000e+03, 7.8980000e+03, 1.3076000e+04, 1.0069500e+05,\n",
       "        1.1969400e+05, 1.1230489e+07]),\n",
       " array([1.08796358e-09, 9.99999900e-02, 1.99999979e-01, 2.99999968e-01,\n",
       "        3.99999957e-01, 4.99999946e-01, 5.99999935e-01, 6.99999924e-01,\n",
       "        7.99999913e-01, 8.99999902e-01, 9.99999891e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADt5JREFUeJzt3V2MXHd9h/Hni92AKsKL6kVCtsmG1qFYUdugVUiLBKEJlRMk+waorUaU1sKCkvQiqJKrVGkUbiCojYRqClYbUZBICLmAFZhaKgQFIUy9UV6IHZluTUpWiZqFhFQIQeL214uZRKPN2nPWnp3Z/fv5SCvNOfPfmd/J7j6ZnHlJqgpJUlteNukBJEmjZ9wlqUHGXZIaZNwlqUHGXZIaZNwlqUETjXuSO5I8leSRDmtvT/Jg/+uHSX42jhklaT3KJF/nnuTtwM+Bz1fVpSv4vhuAy6rqz1dtOElaxyb6yL2q7gOeHtyX5DeT/GuS+5N8J8lvL/Ote4A7xzKkJK1DGyc9wDIOAh+qqv9I8lbg08AfvnBlkouAi4FvTWg+SVrz1lTck7wS+APgy0le2P3yJct2A/dU1f+OczZJWk/WVNzpnSb6WVX93hnW7AY+MqZ5JGldWlMvhayq/wF+lOS9AOn53ReuT/Im4LXA9yY0oiStC5N+KeSd9EL9piQLSfYCfwLsTfIQcAzYNfAte4C7yo+ylKQzmuhLISVJq2NNnZaRJI3GxJ5Q3bRpU01PT0/q7iVpXbr//vt/UlVTw9ZNLO7T09PMzc1N6u4laV1K8l9d1nlaRpIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIatNY+z12SxmJ6/9cndt+Pffzdq34fPnKXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lq0NC4J7kjyVNJHjnN9UnyqSTzSR5O8pbRjylJWokuj9w/B+w4w/XXANv6X/uAfzz3sSRJ52Jo3KvqPuDpMyzZBXy+eo4Ar0ny+lENKElauVGcc98MPD6wvdDf9xJJ9iWZSzK3uLg4gruWJC1nFHHPMvtquYVVdbCqZqpqZmpqagR3LUlazijivgBsHdjeAjwxgtuVJJ2lUcR9Fnh//1UzVwDPVtWTI7hdSdJZ2jhsQZI7gSuBTUkWgL8Ffg2gqj4DHAKuBeaBXwB/tlrDSpK6GRr3qtoz5PoCPjKyiSRJ58x3qEpSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDWoU9yT7EhyIsl8kv3LXP+GJPcmeSDJw0muHf2okqSuhsY9yQbgAHANsB3Yk2T7kmV/A9xdVZcBu4FPj3pQSVJ3XR65Xw7MV9XJqnoOuAvYtWRNAa/qX3418MToRpQkrVSXuG8GHh/YXujvG3QLcF2SBeAQcMNyN5RkX5K5JHOLi4tnMa4kqYsucc8y+2rJ9h7gc1W1BbgW+EKSl9x2VR2sqpmqmpmamlr5tJKkTrrEfQHYOrC9hZeedtkL3A1QVd8DXgFsGsWAkqSV6xL3o8C2JBcnuYDeE6azS9b8GLgKIMmb6cXd8y6SNCFD415Vp4DrgcPAo/ReFXMsya1JdvaXfRT4YJKHgDuBD1TV0lM3kqQx2dhlUVUdovdE6eC+mwcuHwfeNtrRJElny3eoSlKDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNahT3JPsSHIiyXyS/adZ874kx5McS/LF0Y4pSVqJjcMWJNkAHADeBSwAR5PMVtXxgTXbgL8G3lZVzyR53WoNLEkarssj98uB+ao6WVXPAXcBu5as+SBwoKqeAaiqp0Y7piRpJbrEfTPw+MD2Qn/foEuAS5J8N8mRJDuWu6Ek+5LMJZlbXFw8u4klSUN1iXuW2VdLtjcC24ArgT3APyV5zUu+qepgVc1U1czU1NRKZ5UkddQl7gvA1oHtLcATy6z5alU9X1U/Ak7Qi70kaQK6xP0osC3JxUkuAHYDs0vWfAV4J0CSTfRO05wc5aCSpO6Gxr2qTgHXA4eBR4G7q+pYkluT7OwvOwz8NMlx4F7gr6rqp6s1tCTpzIa+FBKgqg4Bh5bsu3ngcgE39r8kSRPmO1QlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIa1CnuSXYkOZFkPsn+M6x7T5JKMjO6ESVJKzU07kk2AAeAa4DtwJ4k25dZdyHwl8D3Rz2kJGllujxyvxyYr6qTVfUccBewa5l1HwNuA345wvkkSWehS9w3A48PbC/0970oyWXA1qr62pluKMm+JHNJ5hYXF1c8rCSpmy5xzzL76sUrk5cBtwMfHXZDVXWwqmaqamZqaqr7lJKkFekS9wVg68D2FuCJge0LgUuBbyd5DLgCmPVJVUmanC5xPwpsS3JxkguA3cDsC1dW1bNVtamqpqtqGjgC7KyquVWZWJI01NC4V9Up4HrgMPAocHdVHUtya5Kdqz2gJGnlNnZZVFWHgENL9t18mrVXnvtYkqRz4TtUJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGtQp7kl2JDmRZD7J/mWuvzHJ8SQPJ/lmkotGP6okqauhcU+yATgAXANsB/Yk2b5k2QPATFX9DnAPcNuoB5UkddflkfvlwHxVnayq54C7gF2DC6rq3qr6RX/zCLBltGNKklaiS9w3A48PbC/0953OXuAby12RZF+SuSRzi4uL3aeUJK1Il7hnmX217MLkOmAG+ORy11fVwaqaqaqZqamp7lNKklZkY4c1C8DWge0twBNLFyW5GrgJeEdV/Wo040mSzkaXR+5HgW1JLk5yAbAbmB1ckOQy4LPAzqp6avRjSpJWYmjcq+oUcD1wGHgUuLuqjiW5NcnO/rJPAq8EvpzkwSSzp7k5SdIYdDktQ1UdAg4t2XfzwOWrRzyXJOkc+A5VSWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWpQp89zX2um9399Yvf92MffPbH7lqSufOQuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUoE5xT7IjyYkk80n2L3P9y5N8qX/995NMj3pQSVJ3Qz8VMskG4ADwLmABOJpktqqODyzbCzxTVb+VZDfwCeCPV2NgSaM3yU9a1ero8pG/lwPzVXUSIMldwC5gMO67gFv6l+8B/iFJqqpGOOuaMKk/Aj9qWNJKdIn7ZuDxge0F4K2nW1NVp5I8C/wG8JPBRUn2Afv6mz9PcuJshgY2Lb3t1uUT598xcx7+nPGYzwvn+Pd8UZdFXeKeZfYtfUTeZQ1VdRA42OE+zzxQMldVM+d6O+uJx3x+8JjPD+M45i5PqC4AWwe2twBPnG5Nko3Aq4GnRzGgJGnlusT9KLAtycVJLgB2A7NL1swCf9q//B7gWy2eb5ek9WLoaZn+OfTrgcPABuCOqjqW5FZgrqpmgX8GvpBknt4j9t2rOTQjOLWzDnnM5weP+fyw6sccH2BLUnt8h6okNci4S1KD1nTcz8ePPehwzDcmOZ7k4STfTNLpNa9r2bBjHlj3niSVZN2/bK7LMSd5X/9nfSzJF8c946h1+N1+Q5J7kzzQ//2+dhJzjkqSO5I8leSR01yfJJ/q//N4OMlbRjpAVa3JL3pP3v4n8EbgAuAhYPuSNX8BfKZ/eTfwpUnPPYZjfifw6/3LHz4fjrm/7kLgPuAIMDPpucfwc94GPAC8tr/9uknPPYZjPgh8uH95O/DYpOc+x2N+O/AW4JHTXH8t8A167xO6Avj+KO9/LT9yf/FjD6rqOeCFjz0YtAv4l/7le4Crkiz3hqr1YugxV9W9VfWL/uYReu87WM+6/JwBPgbcBvxynMOtki7H/EHgQFU9A1BVT415xlHrcswFvKp/+dW89P0060pV3ceZ3++zC/h89RwBXpPk9aO6/7Uc9+U+9mDz6dZU1SnghY89WK+6HPOgvfT+zb+eDT3mJJcBW6vqa+McbBV1+TlfAlyS5LtJjiTZMbbpVkeXY74FuC7JAnAIuGE8o03MSv/eV6TLxw9Mysg+9mAd6Xw8Sa4DZoB3rOpEq++Mx5zkZcDtwAfGNdAYdPk5b6R3auZKev919p0kl1bVz1Z5ttXS5Zj3AJ+rqr9L8vv03jtzaVX93+qPNxGr2q+1/Mj9fPzYgy7HTJKrgZuAnVX1qzHNtlqGHfOFwKXAt5M8Ru/c5Ow6f1K16+/2V6vq+ar6EXCCXuzXqy7HvBe4G6Cqvge8gt6HirWq09/72VrLcT8fP/Zg6DH3T1F8ll7Y1/t5WBhyzFX1bFVtqqrpqpqm9zzDzqqam8y4I9Hld/sr9J48J8kmeqdpTo51ytHqcsw/Bq4CSPJmenFfHOuU4zULvL//qpkrgGer6smR3fqkn1Ee8mzztcAP6T3LflN/3630/rih98P/MjAP/DvwxknPPIZj/jfgv4EH+1+zk555tY95ydpvs85fLdPx5xzg7+n9fxN+AOye9MxjOObtwHfpvZLmQeCPJj3zOR7vncCTwPP0HqXvBT4EfGjgZ3yg/8/jB6P+vfbjBySpQWv5tIwk6SwZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAb9P/eeGwd9YuPVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_probs_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 2, 0,\n",
       "         0],\n",
       "        [1, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0,\n",
       "         0]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled_ts[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 0, 0],\n",
       "       [2, 2, 2, 0, 0],\n",
       "       [2, 2, 2, 0, 0],\n",
       "       [2, 2, 2, 0, 0],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 0, 0],\n",
       "       [2, 2, 2, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [2, 2, 2, 0, 0],\n",
       "       [2, 2, 2, 0, 0],\n",
       "       [2, 2, 2, 0, 0],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 2, 2],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 2, 1],\n",
       "       [2, 2, 1, 2, 1],\n",
       "       [2, 2, 1, 2, 1],\n",
       "       [2, 2, 1, 2, 1],\n",
       "       [2, 2, 1, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 2],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 2, 2],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 1, 2, 1],\n",
       "       [1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1],\n",
       "       [2, 2, 2, 2, 1]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_everything_windows[1000000:1000100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 0., 0.],\n",
       "        [2., 2., 2., 0., 0.],\n",
       "        [2., 2., 2., 0., 0.],\n",
       "        [2., 2., 2., 0., 0.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 0., 0.],\n",
       "        [2., 2., 2., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [2., 2., 2., 0., 0.],\n",
       "        [2., 2., 2., 0., 0.],\n",
       "        [2., 2., 2., 0., 0.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 2., 2.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 2., 1.],\n",
       "        [2., 2., 1., 2., 1.],\n",
       "        [2., 2., 1., 2., 1.],\n",
       "        [2., 2., 1., 2., 1.],\n",
       "        [2., 2., 1., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 2., 2.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 1., 2., 1.],\n",
       "        [1., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.],\n",
       "        [2., 2., 2., 2., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled[1000000:1000100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005078098890923699\n",
      "0.0003913204089874833\n",
      "0.0014480455645232704\n",
      "0.0014508530056153157\n",
      "0.01354255598348486\n",
      "0.03253716003112722\n",
      "0.0007590468415920593\n",
      "0.0007431183237380234\n",
      "0.00028649486911341837\n",
      "0.002593912226161954\n",
      "0.0018647281320323117\n",
      "0.9991726876057854\n",
      "0.999791628469211\n",
      "0.000762786984841668\n",
      "0.019824447749028806\n",
      "0.0027112675165555378\n",
      "0.00030145259904595934\n",
      "0.00038404626502536177\n",
      "0.00037671071476036433\n",
      "0.004816053324366465\n",
      "3.9306247322476104e-05\n",
      "4.1861330792505175e-06\n",
      "2.826889425862865e-05\n",
      "0.0002153524397428743\n",
      "0.004812799428182904\n",
      "0.005065578929722744\n",
      "0.00039091473906242846\n",
      "0.0003835648675527964\n",
      "0.00022959204119903098\n",
      "0.001110630339805671\n",
      "3.9262731529425974e-05\n",
      "3.718266664914184e-07\n",
      "2.6777967627359225e-07\n",
      "3.945536968773676e-06\n",
      "0.0006109908980965464\n",
      "3.9262731529425974e-05\n",
      "3.718266664914184e-07\n",
      "2.6777967627359225e-07\n",
      "3.945536968773676e-06\n",
      "0.0006109908980965464\n",
      "3.9300865168037885e-05\n",
      "3.720164123257653e-07\n",
      "3.834042406382582e-06\n",
      "2.8692889826422102e-05\n",
      "0.004647517259257665\n",
      "0.0048748470861888285\n",
      "1.7782488880939136e-06\n",
      "3.7068132353418283e-07\n",
      "3.9475438949980175e-06\n",
      "0.0006117088675212537\n",
      "0.005439023266851084\n",
      "0.0002011560410850599\n",
      "0.004558647624114176\n",
      "0.9449195082138293\n",
      "0.9732738555944707\n",
      "0.003629615917041029\n",
      "0.0002036102448910171\n",
      "0.9712002733373772\n",
      "0.971450863061591\n",
      "0.0014236569757413164\n",
      "4.585937430092585e-05\n",
      "3.2754059282845416e-05\n",
      "0.00021184120744324048\n",
      "0.00037685196353167555\n",
      "0.004812834379213281\n",
      "0.0050697029924809955\n",
      "0.00039109505370593656\n",
      "0.00038389582430486957\n",
      "0.0003766149898664084\n",
      "0.004818546299406035\n",
      "0.005065578929722744\n",
      "0.00039091473906242846\n",
      "0.0003835648675527964\n",
      "0.00022959204119903098\n",
      "0.001110630339805671\n",
      "4.399503429630691e-05\n",
      "2.5620454194408282e-05\n",
      "0.004181742308283054\n",
      "0.8549958720640987\n",
      "0.8622625257127936\n",
      "7.747984051474643e-05\n",
      "3.377412738661545e-06\n",
      "4.895057701501115e-06\n",
      "2.515196104134476e-05\n",
      "0.0006283795792996899\n",
      "0.004907661210016401\n",
      "0.9732142141783626\n",
      "0.9731899165593703\n",
      "1.653410145918688e-05\n",
      "0.0009018258255905076\n",
      "4.228464948307503e-05\n",
      "2.3552442654457828e-05\n",
      "0.0038400105528121273\n",
      "0.5804414764915447\n",
      "0.6316183002340772\n",
      "3.9262731529425974e-05\n",
      "3.718266664914184e-07\n",
      "2.6777967627359225e-07\n",
      "3.945536968773676e-06\n",
      "0.0006109908980965464\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(R_pred_probs_per_frame[i + 1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.3420200e+05, 2.1149000e+04, 7.7870000e+03, 6.8100000e+03,\n",
       "        8.7200000e+03, 7.8980000e+03, 1.3076000e+04, 1.0069500e+05,\n",
       "        1.1969400e+05, 1.1230489e+07]),\n",
       " array([1.08796358e-09, 9.99999900e-02, 1.99999979e-01, 2.99999968e-01,\n",
       "        3.99999957e-01, 4.99999946e-01, 5.99999935e-01, 6.99999924e-01,\n",
       "        7.99999913e-01, 8.99999902e-01, 9.99999891e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_probs_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98128119, 0.98241826, 0.99931511, 0.9960668 , 0.74069486,\n",
       "       0.80678443, 0.99510206, 0.98100089, 0.91893201, 0.97323139])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred_probs_per_frame[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_nums = [\n",
    "    (video_id, intrvl.start, intrvl.end)\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows = [\n",
    "    (window_info, np.array([prediction, 1. - prediction]))\n",
    "    for window_info, prediction in zip(window_nums, R_pred_probs_per_frame)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we needed to cut the predictions to a multiple of T\n",
    "last_preds = []\n",
    "for window_info in window_nums[len(predictions_to_save_windows):]:\n",
    "    last_preds.append((window_info, np.array([0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows += last_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np_windows = np.array(predictions_to_save_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to disk\n",
    "with open('../../data/shot_detection_weak_labels/ts_weak_labels_all_windows_tuned.npy', 'wb') as f:\n",
    "    np.save(f, preds_np_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution compared to Metal LabelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(1, 0, 16), array([0., 1.])],\n",
       "       [(1, 8, 24), array([0., 1.])],\n",
       "       [(1, 16, 32), array([0., 1.])],\n",
       "       [(1, 24, 40), array([0., 1.])],\n",
       "       [(1, 32, 48), array([0., 1.])],\n",
       "       [(1, 40, 56), array([0.01515144, 0.98484856])],\n",
       "       [(1, 48, 64), array([0.98484856, 0.01515144])],\n",
       "       [(1, 56, 72), array([0.98484856, 0.01515144])],\n",
       "       [(1, 64, 80), array([0.01515144, 0.98484856])],\n",
       "       [(1, 72, 88), array([0.01515144, 0.98484856])]], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_np_windows[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12350523, 2)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_np_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/noisy_labels_all_windows.npy', 'rb') as f:\n",
    "    preds_np_windows_metal = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(1, 0, 16), array([0.0032828, 0.9967172])],\n",
       "       [(1, 8, 24), array([0.0032828, 0.9967172])],\n",
       "       [(1, 16, 32), array([0.0032828, 0.9967172])],\n",
       "       [(1, 24, 40), array([0.0032828, 0.9967172])],\n",
       "       [(1, 32, 48), array([0.0032828, 0.9967172])],\n",
       "       [(1, 40, 56), array([0.24933879, 0.75066121])],\n",
       "       [(1, 48, 64), array([0.98051349, 0.01948651])],\n",
       "       [(1, 56, 72), array([0.36412127, 0.63587873])],\n",
       "       [(1, 64, 80), array([0.0032828, 0.9967172])],\n",
       "       [(1, 72, 88), array([0.0032828, 0.9967172])]], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_np_windows_metal[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12350523, 2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_np_windows_metal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Distribution across all data, timeseries LabelModel')\n",
    "ax.hist([\n",
    "    i[1][0]\n",
    "    for i in preds_np_windows\n",
    "])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Distribution across all data, Metal LabelModel')\n",
    "ax.hist([\n",
    "    i[1][0]\n",
    "    for i in preds_np_windows_metal\n",
    "])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/majority_vote_labels_all_windows.npy', 'rb') as f:\n",
    "    preds_np_windows_mv = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Distribution across all data, Majority Vote')\n",
    "ax.hist([\n",
    "    i[1][0]\n",
    "    for i in preds_np_windows_mv\n",
    "])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff (bad class balance, wrong R_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.8 s, sys: 1.6 s, total: 43.4 s\n",
      "Wall time: 43.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "max_seed = 10\n",
    "temporal_models = [None,]*max_seed\n",
    "for seed in range(max_seed):\n",
    "    markov_model = DPLabelModel(m=m_per_task*T, \n",
    "                                T=T,\n",
    "                                edges=[(i,i+m_per_task) for i in range((T-1)*m_per_task)],\n",
    "                                coverage_sets=[[t,] for t in range(T) for _ in range(m_per_task)],\n",
    "                                mu_sharing=[[t*m_per_task+i for t in range(T)] for i in range(m_per_task)],\n",
    "                                phi_sharing=[[(t*m_per_task+i, (t+1)*m_per_task+i) for t in range(T-1)] for i in range(m_per_task)],\n",
    "                                device=device,\n",
    "                                # class_balance=MRI_data_temporal['class_balance'],\n",
    "                                seed=seed)\n",
    "    optimize(markov_model, L_hat=MRI_data_temporal['Li_train'], num_iter=10, lr=1e-5, momentum=0.8, clamp=True, \n",
    "             verbose=False, seed=seed)\n",
    "    temporal_models[seed] = markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7325,)\n",
      "Accuracy: 0.357\n",
      "F1: 0.090\n",
      "Recall: 0.182\n",
      "Precision: 0.060\n",
      "(7325,)\n",
      "Accuracy: 0.369\n",
      "F1: 0.184\n",
      "Recall: 0.404\n",
      "Precision: 0.119\n",
      "(7325,)\n",
      "Accuracy: 0.523\n",
      "F1: 0.218\n",
      "Recall: 0.377\n",
      "Precision: 0.153\n",
      "(7325,)\n",
      "Accuracy: 0.388\n",
      "F1: 0.225\n",
      "Recall: 0.506\n",
      "Precision: 0.145\n",
      "(7325,)\n",
      "Accuracy: 0.375\n",
      "F1: 0.234\n",
      "Recall: 0.542\n",
      "Precision: 0.149\n",
      "(7325,)\n",
      "Accuracy: 0.668\n",
      "F1: 0.344\n",
      "Recall: 0.494\n",
      "Precision: 0.263\n",
      "(7325,)\n",
      "Accuracy: 0.580\n",
      "F1: 0.086\n",
      "Recall: 0.112\n",
      "Precision: 0.069\n",
      "(7325,)\n",
      "Accuracy: 0.509\n",
      "F1: 0.154\n",
      "Recall: 0.254\n",
      "Precision: 0.111\n",
      "(7325,)\n",
      "Accuracy: 0.277\n",
      "F1: 0.024\n",
      "Recall: 0.050\n",
      "Precision: 0.015\n",
      "(7325,)\n",
      "Accuracy: 0.544\n",
      "F1: 0.279\n",
      "Recall: 0.500\n",
      "Precision: 0.193\n"
     ]
    }
   ],
   "source": [
    "for seed, model in enumerate(temporal_models):\n",
    "    Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "    R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "    #find sequence label config. with highest prob.\n",
    "    config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "    R_pred_config = model.feasible_y[config_index]\n",
    "    R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "    #for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "    R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "    for idx in range(R_pred_config.shape[0]):\n",
    "        R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "    R_pred_probs = R_pred_probs.numpy()\n",
    "    R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "\n",
    "    Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "    R_pred_frame = model.predict_proba(Li_dev.to(device)) #predict per sequence\n",
    "\n",
    "    #find sequence label config. with highest prob.\n",
    "    config_index = np.argmax(R_pred_frame.detach().cpu().numpy(), axis=1) \n",
    "    R_pred_config = model.feasible_y[config_index]\n",
    "    R_pred_max = torch.FloatTensor(np.max(R_pred_frame.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "    #for each 1 in config, multiply by prob. label for sequence (1-prob label otherwise)\n",
    "    R_pred_probs = torch.FloatTensor(R_pred_config.shape)\n",
    "    for idx in range(R_pred_config.shape[0]):\n",
    "        R_pred_probs[idx,:] = torch.LongTensor(R_pred_config[idx,:].cpu()).float()*R_pred_max[idx]\n",
    "\n",
    "    R_pred_probs = R_pred_probs.numpy()\n",
    "    R_pred_probs[R_pred_probs < 0] = 1+R_pred_probs[R_pred_probs < 0]\n",
    "    R_pred_frame_label = np.round(R_pred_probs.ravel())\n",
    "    R_pred_frame_label[R_pred_frame_label == 0.] = 2.\n",
    "\n",
    "    for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "        score = metric_score(Y_dev.cpu(), R_pred_frame_label, metric)\n",
    "        print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.207  F1=0.294  precision=0.175 recall=0.931 \n",
      "seed=1  accuracy=0.208  F1=0.270  precision=0.161 recall=0.823 \n",
      "seed=2  accuracy=0.730  F1=0.168  precision=0.185 recall=0.154 \n",
      "seed=3  accuracy=0.182  F1=0.300  precision=0.177 recall=0.988 \n",
      "seed=4  accuracy=0.196  F1=0.283  precision=0.168 recall=0.892 \n",
      "seed=5  accuracy=0.674  F1=0.143  precision=0.134 recall=0.154 \n",
      "seed=6  accuracy=0.812  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=7  accuracy=0.499  F1=0.193  precision=0.135 recall=0.338 \n",
      "seed=8  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=9  accuracy=0.639  F1=0.470  precision=0.318 recall=0.904 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=1 (20s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=2  accuracy=0.616  F1=0.228  precision=0.177 recall=0.319 \n",
      "seed=3  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=4  accuracy=0.174  F1=0.293  precision=0.173 recall=0.965 \n",
      "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=8  accuracy=0.739  F1=0.073  precision=0.098 recall=0.058 \n",
      "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=10 (40s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=2  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=3  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=4  accuracy=0.661  F1=0.347  precision=0.263 recall=0.508 \n",
      "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=8  accuracy=0.386  F1=0.334  precision=0.207 recall=0.865 \n",
      "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=50 (2 min 25s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=2  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=3  accuracy=0.621  F1=0.224  precision=0.176 recall=0.308 \n",
      "seed=4  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=8  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
      "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=100 (4 min 28s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=1  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=2  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=3  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=4  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=5  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=6  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=7  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=8  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
      "seed=9  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n"
     ]
    }
   ],
   "source": [
    "# num_iter=500 (22 min 2s wall time)\n",
    "for seed, model in enumerate(temporal_models):\n",
    "    R_pred = model.predict(MRI_data_temporal['Li_dev'])\n",
    "    F1 = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'f1')\n",
    "    accuracy = metric_score(MRI_data_temporal['R_dev'].cpu(), R_pred.cpu(), 'accuracy')\n",
    "    pre = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'precision')\n",
    "    rec = metric_score(MRI_data_temporal['R_dev'].cpu()>0, R_pred.cpu()>0, 'recall')\n",
    "    print(f\"seed={seed}  accuracy={accuracy:.3f}  F1={F1:.3f}  precision={pre:.3f} recall={rec:.3f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on runs\n",
    "\n",
    "```\n",
    "1 iteration:\n",
    "seed=0  accuracy=0.207  F1=0.294  precision=0.175 recall=0.931 \n",
    "seed=1  accuracy=0.208  F1=0.270  precision=0.161 recall=0.823 \n",
    "seed=2  accuracy=0.730  F1=0.168  precision=0.185 recall=0.154 \n",
    "seed=3  accuracy=0.182  F1=0.300  precision=0.177 recall=0.988 \n",
    "seed=4  accuracy=0.196  F1=0.283  precision=0.168 recall=0.892 \n",
    "seed=5  accuracy=0.674  F1=0.143  precision=0.134 recall=0.154 \n",
    "seed=6  accuracy=0.812  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=7  accuracy=0.499  F1=0.193  precision=0.135 recall=0.338 \n",
    "seed=8  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=9  accuracy=0.639  F1=0.470  precision=0.318 recall=0.904\n",
    "\n",
    "10 iterations:\n",
    "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=2  accuracy=0.616  F1=0.228  precision=0.177 recall=0.319 \n",
    "seed=3  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=4  accuracy=0.174  F1=0.293  precision=0.173 recall=0.965 \n",
    "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=8  accuracy=0.739  F1=0.073  precision=0.098 recall=0.058 \n",
    "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000\n",
    "\n",
    "50 iterations:\n",
    "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=2  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=3  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=4  accuracy=0.661  F1=0.347  precision=0.263 recall=0.508 \n",
    "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=8  accuracy=0.386  F1=0.334  precision=0.207 recall=0.865 \n",
    "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "\n",
    "100 iterations:\n",
    "seed=0  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=1  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=2  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=3  accuracy=0.621  F1=0.224  precision=0.176 recall=0.308 \n",
    "seed=4  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=5  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=6  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=7  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=8  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "seed=9  accuracy=0.177  F1=0.301  precision=0.177 recall=1.000 \n",
    "\n",
    "500 iterations:\n",
    "seed=0  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=1  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=2  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=3  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=4  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=5  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=6  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=7  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=8  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "seed=9  accuracy=0.823  F1=0.000  precision=0.000 recall=0.000 \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
