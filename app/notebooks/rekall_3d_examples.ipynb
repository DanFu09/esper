{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:09:02.236334Z",
     "start_time": "2019-02-22T01:09:02.189994Z"
    }
   },
   "outputs": [],
   "source": [
    "from esper.prelude import *\n",
    "from query.models import Shot\n",
    "from django.db.models import Value\n",
    "from django.db.models.fields import IntegerField\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "\n",
    "def convert_to_1d_collection(collection):\n",
    "    from rekall.interval_list import Interval\n",
    "    video_map = collection.get_allintervals()\n",
    "    return VideoIntervalCollection({vid: [Interval(\n",
    "        i.t[0], i.t[1], i.payload) for i in video_map[vid].get_intervals()] for vid in video_map})\n",
    "\n",
    "\n",
    "def display_result(collection_1d, display_payload=False):\n",
    "    from esper.rekall import intrvllists_to_result_bbox\n",
    "    from esper.rekall import intrvllists_to_result_with_objects\n",
    "    if display_payload:\n",
    "        results = intrvllists_to_result_with_objects(collection_1d.get_allintervals(), \n",
    "            lambda p, v: p, limit=1000, stride=1)\n",
    "    else:\n",
    "        results = intrvllists_to_result_with_objects(collection_1d.get_allintervals(), \n",
    "            lambda p, v:[], limit=1000, stride=1)\n",
    "    return esper_widget(results,\n",
    "            crop_bboxes=False, show_middle_frame=False, disable_captions=True,\n",
    "            results_per_page=25, jupyter_keybindings=True)\n",
    "\n",
    "def get_set(vid, collection):\n",
    "    from rekall.interval_set_3d import IntervalSet3D\n",
    "    return collection.get_allintervals().get(vid, IntervalSet3D([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hermione in the Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T07:26:12.237840Z",
     "start_time": "2019-02-21T07:25:56.477347Z"
    }
   },
   "outputs": [],
   "source": [
    "def hermione_in_the_middle():\n",
    "    from rekall.video_interval_collection_3d import VideoIntervalCollection3D\n",
    "    from rekall.interval_set_3d_utils import T,P,XY,or_preds,X\n",
    "    from rekall.temporal_predicates import before, overlaps_before\n",
    "    from rekall.bbox_predicates import height_at_least, same_value, same_height, left_of\n",
    "    from query.models import FaceCharacterActor\n",
    "    \n",
    "    MIN_FACE_HEIGHT = 0.25\n",
    "    EPSILON = 0.15\n",
    "    NAMES = [ 'ron weasley', 'hermione granger', 'harry potter' ]\n",
    "    \n",
    "    faces_with_character_actor_qs = FaceCharacterActor.objects.annotate(\n",
    "        min_frame=F('face__frame__number'),\n",
    "        max_frame=F('face__frame__number'),\n",
    "        video_id=F('face__frame__video_id'),\n",
    "        bbox_x1=F('face__bbox_x1'),\n",
    "        bbox_y1=F('face__bbox_y1'),\n",
    "        bbox_x2=F('face__bbox_x2'),\n",
    "        bbox_y2=F('face__bbox_y2'),\n",
    "        character_name=F('characteractor__character__name')\n",
    "    ).filter(face__frame__video__name__contains=\"harry potter\")\n",
    "    \n",
    "    total = faces_with_character_actor_qs.count()\n",
    "    \n",
    "    schema = VideoIntervalCollection3D.django_bbox_default_schema()\n",
    "    schema['payload'] = 'character_name'    \n",
    "    all_faces = VideoIntervalCollection3D.from_django_qs(faces_with_character_actor_qs, schema, progress=True,\n",
    "                                                    total=total)\n",
    "    frames_with_faces = all_faces.group_by_time()\n",
    "    \n",
    "    def name_is(name):\n",
    "        return lambda f: f.payload == name\n",
    "    def start_before():\n",
    "        return or_preds(overlaps_before(), before())\n",
    "    def in_order():\n",
    "        return lambda a,b,c: start_before()(a,b) and start_before()(b,c)\n",
    "    def rev_order():\n",
    "        return lambda *args: in_order()(*args[::-1])\n",
    "    \n",
    "    pattern = [\n",
    "        ([\"harry\"], [XY(height_at_least(MIN_FACE_HEIGHT)), name_is(NAMES[2])]),\n",
    "        ([\"ron\"], [XY(height_at_least(MIN_FACE_HEIGHT)), name_is(NAMES[0])]),\n",
    "        ([\"hermione\"], [XY(height_at_least(MIN_FACE_HEIGHT)), name_is(NAMES[1])]),\n",
    "        ([\"harry\", \"ron\"], [XY(same_value('y1', epsilon=EPSILON)), XY(same_height(epsilon=EPSILON))]),\n",
    "        ([\"harry\", \"hermione\"], [XY(same_value('y1', epsilon=EPSILON)), XY(same_height(epsilon=EPSILON))]),\n",
    "        ([\"ron\", \"hermione\"], [XY(same_value('y1', epsilon=EPSILON)), XY(same_height(epsilon=EPSILON))]),\n",
    "        ([\"harry\",\"hermione\", \"ron\"], [X(or_preds(in_order(), rev_order()))])\n",
    "    ]\n",
    "    \n",
    "    def matches_pattern(pattern, exact):\n",
    "        def pred(intervals):\n",
    "            return len(intervals.match(pattern, exact))>0\n",
    "        return pred\n",
    "    \n",
    "    # Frame_IS<Face_IS<character>>\n",
    "    final = frames_with_faces.filter(P(matches_pattern(pattern, exact=True)))\n",
    "    return final\n",
    "\n",
    "def payload_to_vgrid_objects(faces):\n",
    "    from query.models import Character\n",
    "    def intrvl_to_obj(face):\n",
    "        return {\n",
    "            'type': 'bbox',\n",
    "            'bbox_x1': face.x[0], 'bbox_x2': face.x[1],\n",
    "            'bbox_y1': face.y[0], 'bbox_y2': face.y[1],\n",
    "            'character_id': Character.objects.get(name=face.payload).id\n",
    "        }\n",
    "    def update(acc, face):\n",
    "        acc.append(intrvl_to_obj(face))\n",
    "        return acc\n",
    "    return faces.fold(update, [])\n",
    "    \n",
    "answer = hermione_in_the_middle()\n",
    "display_result(convert_to_1d_collection(answer.map_payload(payload_to_vgrid_objects)), display_payload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kissing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:08:47.894529Z",
     "start_time": "2019-02-14T07:08:02.922162Z"
    }
   },
   "outputs": [],
   "source": [
    "def kissing():\n",
    "    from query.models import Face\n",
    "    from rekall.interval_set_3d import Interval3D, IntervalSet3D\n",
    "    from rekall.video_interval_collection_3d import VideoIntervalCollection3D\n",
    "    from rekall.interval_set_3d_utils import T,P,XY,or_preds,X,Y\n",
    "    from rekall.merge_ops import payload_plus\n",
    "    from rekall.payload_predicates import payload_satisfies\n",
    "    from rekall.spatial_predicates import scene_graph\n",
    "    from rekall.temporal_predicates import overlaps, overlaps_before, before\n",
    "    from rekall.face_landmark_predicates import looking_left, looking_right\n",
    "    from rekall.bbox_predicates import height_at_least, same_height\n",
    "    import esper.face_landmarks_wrapper as flw\n",
    "    from esper.captions import get_all_segments\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    \n",
    "    MAX_MOUTH_DIFF = 0.12\n",
    "    MIN_FACE_CONFIDENCE = 0.8\n",
    "    MIN_FACE_HEIGHT = 0.4\n",
    "    MAX_FACE_HEIGHT_DIFF = 0.1\n",
    "    MIN_FACE_OVERLAP_X = 0.05\n",
    "    MIN_FACE_OVERLAP_Y = 0.2\n",
    "    MAX_FACE_OVERLAP_X_FRACTION = 0.7\n",
    "    MIN_FACE_ANGLE = 0.2\n",
    "    \n",
    "    # Cannot be parallelized??\n",
    "    def get_landmarks(faces):\n",
    "        ids = [face.payload for face in faces.get_intervals()]\n",
    "        landmarks = flw.get_from_face_ids(ids)\n",
    "        id_to_lm = {idx: lm for idx, lm in zip(ids, landmarks)}\n",
    "        return faces.map_payload(lambda idx : {\n",
    "            'id': idx,\n",
    "            'landmarks': id_to_lm[idx]\n",
    "        })\n",
    "    \n",
    "    def mouths_are_close(lm1, lm2):\n",
    "        select_outer=[2,3,4,8,9,10]\n",
    "        select_inner=[1,2,3,5,6,7]\n",
    "        mouth1 = np.concatenate((lm1.outer_lips()[select_outer], lm1.inner_lips()[select_inner]))\n",
    "        mouth2 = np.concatenate((lm2.outer_lips()[select_outer], lm2.inner_lips()[select_inner]))\n",
    "        mean1 = np.mean(mouth1, axis=0)\n",
    "        mean2 = np.mean(mouth2, axis=0)\n",
    "        return np.linalg.norm(mean1-mean2) <= MAX_MOUTH_DIFF\n",
    "    \n",
    "    # Line is ax+by+c=0\n",
    "    def project_point_to_line(pt, a, b, c):\n",
    "        x0,y0=pt[0], pt[1]\n",
    "        d=a*a+b*b\n",
    "        x=(b*(b*x0-a*y0)-a*c)/d\n",
    "        y=(a*(-b*x0+a*y0)-b*c)/d\n",
    "        return np.array([x,y])\n",
    "    \n",
    "    # Returns (a,b,c) which defines ax+by+c=0\n",
    "    def find_best_line_fit(xs, ys):\n",
    "        fit1 = np.polyfit(xs, ys, 1)\n",
    "        error1 = np.sum((np.poly1d(fit1)(xs)-ys)**2)\n",
    "        fit2 = np.polyfit(ys, xs, 1)\n",
    "        error2 = np.sum((np.poly1d(fit2)(ys)-xs)**2)\n",
    "        if error1 < error2:\n",
    "            # fit1[0]x+fit1[1]=y\n",
    "            return fit1[0], -1, fit1[1]\n",
    "        # fit2[0]y+fit2[1]=x\n",
    "        return -1, fit2[0], fit2[1]\n",
    "    \n",
    "    # Positive if facing left\n",
    "    def signed_face_angle(lm):\n",
    "        center_line_indices = [27,28, 32, 33,34, 51,62,66,57]\n",
    "        data = lm.landmarks[center_line_indices]\n",
    "        a, b, c = find_best_line_fit(data[:,0], data[:,1])\n",
    "        A = project_point_to_line(lm.landmarks[center_line_indices[0]], a, b, c)\n",
    "        B = project_point_to_line(lm.landmarks[center_line_indices[-1]], a, b, c)\n",
    "        AB = B-A\n",
    "        AB = AB / np.linalg.norm(AB)\n",
    "        C = np.mean(lm.nose_bridge()[2:4], axis=0)\n",
    "        AC = C-A\n",
    "        AC = AC / np.linalg.norm(AC)\n",
    "        return np.cross(AB, AC)\n",
    "\n",
    "    # Annotate face rows with start and end frames and the video ID\n",
    "    faces_qs = Face.objects.filter(\n",
    "        frame__regularly_sampled=True,\n",
    "        probability__gte=MIN_FACE_CONFIDENCE).annotate(\n",
    "        min_frame=F('frame__number'),\n",
    "        max_frame=F('frame__number'),\n",
    "        height = F('bbox_y2')-F('bbox_y1'),\n",
    "        video_id=F('frame__video_id')).filter(height__gte=MIN_FACE_HEIGHT, video_id__lte=25)\n",
    "    \n",
    "    total = faces_qs.count()\n",
    "    \n",
    "    # Frame_IS<Face_IS<face_id>>\n",
    "    frames_with_faces = VideoIntervalCollection3D.from_django_qs(\n",
    "        faces_qs, VideoIntervalCollection3D.django_bbox_default_schema(),\n",
    "        progress=True, total=total\n",
    "    ).group_by_time(profile=True)\n",
    "    \n",
    "    overlap_faces_pattern = [\n",
    "        ([\"left\",\"right\"], [\n",
    "            X(or_preds(before(), overlaps_before())), # Left face on the left\n",
    "            X(lambda f1,f2: f1.end - f2.start > MIN_FACE_OVERLAP_X), # Faces overlap\n",
    "            Y(lambda f1,f2: min(f1.end, f2.end)-max(f1.start, f2.start) > MIN_FACE_OVERLAP_Y), # No face is entirely above another\n",
    "            XY(same_height(MAX_FACE_HEIGHT_DIFF)),\n",
    "            X(lambda f1, f2: (f1.end-f2.start)/max(f1.length(), f2.length()) < MAX_FACE_OVERLAP_X_FRACTION),\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    def matches_pattern(pattern, exact):\n",
    "        def pred(intervals):\n",
    "            return len(intervals.match(pattern, exact))>0\n",
    "        return pred\n",
    "    \n",
    "    # Frame_IS<Face_IS<face_id>>\n",
    "    frames_with_overlapped_faces = frames_with_faces.filter(P(matches_pattern(overlap_faces_pattern, exact=True)), profile=True)\n",
    "    \n",
    "#     def get_landmark_map(frames_with_faces):\n",
    "#         def update(fids, frame):\n",
    "#             faces = frame.payload\n",
    "#             return fids + [face.payload for face in faces.get_intervals()]\n",
    "#         vid_to_fids = frames_with_faces.fold(update, [])\n",
    "#         fids = [fid for fids in vid_to_fids.values() for fid in fids]\n",
    "#         print(\"Getting landmarks for {0} faces\".format(len(fids)))\n",
    "#         landmarks = flw.get_from_face_ids(fids)\n",
    "#         print(\"Landmarks loaded\".format(len(fids)))\n",
    "#         return {idx: lm for idx, lm in zip(fids, landmarks)}\n",
    "    \n",
    "#     landmark_map = get_landmark_map(frames_with_overlapped_faces)\n",
    "    \n",
    "#     def fid_to_face_meta(lm_map):\n",
    "#         def map_fn(fid):\n",
    "#             return {\n",
    "#                 'id': fid,\n",
    "#                 'landmarks': lm_map[fid]\n",
    "#             }\n",
    "#         return map_fn       \n",
    "    \n",
    "#     # Frame_IS<Face_IS<FaceMeta>>\n",
    "#     frames_with_landmarks = frames_with_overlapped_faces.map_payload(\n",
    "#         lambda faces: faces.map_payload(fid_to_face_meta(landmark_map)))\n",
    "    \n",
    "    # Frame_IS<Face_IS<FaceMeta>>\n",
    "    frames_with_landmarks = frames_with_overlapped_faces.map_payload(get_landmarks, profile=True, parallel=True)\n",
    "    opposing_face_pattern = [\n",
    "        (['left'], [P(lambda f: signed_face_angle(f['landmarks']) < -MIN_FACE_ANGLE)]),\n",
    "        (['right'], [P(lambda f: signed_face_angle(f['landmarks']) > MIN_FACE_ANGLE)]),\n",
    "        (['left','right'], [P(lambda l, r: mouths_are_close(l['landmarks'], r['landmarks']))])\n",
    "    ]\n",
    "    \n",
    "    # Frame_IS<Face_IS<FaceMeta>>\n",
    "    frames_with_opposing_faces = frames_with_landmarks.filter(P(matches_pattern(opposing_face_pattern, exact=True)), profile=True)\n",
    "    \n",
    "    vids = frames_with_opposing_faces.get_allintervals().keys()\n",
    "    \n",
    "    # Merge with shots\n",
    "    shots_qs = Shot.objects.filter(\n",
    "        video_id__in = vids,\n",
    "        cinematic = True,\n",
    "    )\n",
    "    total = shots_qs.count()\n",
    "    # Shot_IS<>\n",
    "    shots = VideoIntervalCollection3D.from_django_qs(\n",
    "        shots_qs,\n",
    "        progress=True, total=total\n",
    "    )\n",
    "    # Shot_IS<Frame_IS<Face_IS<FaceMeta>>>\n",
    "    kissing_shots = shots.collect_by_interval(\n",
    "        frames_with_opposing_faces,\n",
    "        T(overlaps()),\n",
    "        time_window=1, profile=True\n",
    "    ).map_payload(lambda p:p[1]).map(\n",
    "        # Take the start of the kissing as the start of the shot\n",
    "        lambda shot: Interval3D((shot.payload.get_intervals()[0].t[0], shot.t[1]), payload=shot.payload))\n",
    "    \n",
    "    # Get faces in shots\n",
    "    faces_qs2 = Face.objects.filter(\n",
    "         frame__regularly_sampled=True,\n",
    "         frame__video_id__in=vids,probability__gte=MIN_FACE_CONFIDENCE).annotate(\n",
    "             min_frame=F('frame__number'),\n",
    "             max_frame=F('frame__number'),\n",
    "             video_id=F('frame__video_id')       \n",
    "    )\n",
    "    total = faces_qs2.count()\n",
    "    # Frame_IS<Face_IS>\n",
    "    frames_with_faces2 = VideoIntervalCollection3D.from_django_qs(\n",
    "        faces_qs2, VideoIntervalCollection3D.django_bbox_default_schema(),\n",
    "        progress=True, total=total\n",
    "    ).group_by_time(profile=True)\n",
    "    \n",
    "    def both_faces_are_high(faces):\n",
    "        def update(result, face):\n",
    "            if face.height() < MIN_FACE_HEIGHT:\n",
    "                return False\n",
    "            return result\n",
    "        return faces.fold(update, True)\n",
    "    \n",
    "    # Frame_IS<Face_IS>\n",
    "    frames_with_two_faces = frames_with_faces2.filter(\n",
    "        P(lambda faces: faces.size()==2)).filter(P(both_faces_are_high), profile=True)\n",
    "    \n",
    "    # Collect frames with two faces into kissing shots, and clips the shot to the last frame with two faces\n",
    "    def clip_to_last_frame(intrvl):\n",
    "        frames = intrvl.payload[1]\n",
    "        if frames.empty():\n",
    "            return intrvl.copy()\n",
    "        return Interval3D((intrvl.t[0], frames.get_intervals()[-1].t[1]), payload=intrvl.payload)\n",
    "    \n",
    "    # Shot_IS<(Frame_IS<Face_IS>, Frame_IS<Face_IS>)>\n",
    "    clipped_kissing_shots = kissing_shots.collect_by_interval(\n",
    "        frames_with_two_faces, T(overlaps()), time_window=1, filter_empty=False, profile=True\n",
    "    ).map(clip_to_last_frame).filter_size(min_size=12)\n",
    "    \n",
    "    \n",
    "    results = get_all_segments(vids)\n",
    "    fps_map = dict((i, Video.objects.get(id=i).fps) for i in vids)\n",
    "    # Word_IS<>\n",
    "    caption_results = VideoIntervalCollection3D({\n",
    "        video_id: IntervalSet3D([Interval3D((\n",
    "            word[0] * fps_map[video_id], # start frame\n",
    "            word[1] * fps_map[video_id]))\n",
    "            for word in words])\n",
    "        for video_id, words in tqdm(results)\n",
    "    })\n",
    "    \n",
    "    kissing_without_words = clipped_kissing_shots.minus(caption_results, profile=True)\n",
    "    kissing_final = kissing_without_words.temporal_coalesce(epsilon=0.5, profile=True).map(\n",
    "        lambda i: Interval3D((int(i.t[0]), int(i.t[1])), payload=i.payload)\n",
    "    ).filter_size(min_size=12)\n",
    "    \n",
    "    return kissing_final\n",
    "\n",
    "def payload_to_vgrid_objects(payload):\n",
    "    # Frame_IS<Face_IS<FaceMeta>>\n",
    "    frames_with_opposing_overlapped_faces, _ = payload\n",
    "    def face_to_objects(face):\n",
    "        from esper.stdlib import face_landmarks_to_dict\n",
    "        return [{\n",
    "            'type': 'bbox',\n",
    "            'bbox_x1': face.x[0], 'bbox_x2': face.x[1],\n",
    "            'bbox_y1': face.y[0], 'bbox_y2': face.y[1]\n",
    "        }, face_landmarks_to_dict(face.payload['landmarks'])]\n",
    "    def update(acc, frame):\n",
    "        def accumulate_faces(a, face):\n",
    "            return a+face_to_objects(face)\n",
    "        return acc + frame.payload.fold(accumulate_faces, [])\n",
    "    return frames_with_opposing_overlapped_faces.fold(update, [])\n",
    "\n",
    "answer = kissing()\n",
    "print(\"Query finished. Preparing VGrid.\")\n",
    "display_result(convert_to_1d_collection(answer.map_payload(payload_to_vgrid_objects)), display_payload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T09:06:01.786277Z",
     "start_time": "2019-02-08T09:06:01.756131Z"
    }
   },
   "source": [
    "# Action Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T01:11:02.451099Z",
     "start_time": "2019-02-20T01:10:06.121906Z"
    }
   },
   "outputs": [],
   "source": [
    "def action_shots():\n",
    "    from query.models import Shot\n",
    "    from rekall.video_interval_collection_3d import VideoIntervalCollection3D\n",
    "    from rekall.interval_set_3d import IntervalSet3D, Interval3D\n",
    "    from rekall.temporal_predicates import meets_before, overlaps, equal\n",
    "    from rekall.interval_set_3d_utils import T,P,XY,or_preds,X,Y\n",
    "    from django.db.models import ExpressionWrapper, FloatField\n",
    "    from esper.captions import get_all_segments\n",
    "    from rekall.merge_ops import payload_first, payload_plus, merge_named_payload\n",
    "    import numpy as np\n",
    "\n",
    "    NUM_SHOTS=5\n",
    "    MAX_SHOT_DURATION=0.8\n",
    "    BRIGHTNESS_THRESHOLD = 20.0\n",
    "    MAX_NUM_WORDS_PER_SECOND = 1.0\n",
    "    \n",
    "    shots_qs = Shot.objects.annotate(\n",
    "        duration = ExpressionWrapper((F('max_frame') - F('min_frame')) / F('video__fps'), output_field=FloatField())\n",
    "    ).filter(\n",
    "        duration__lt=MAX_SHOT_DURATION,\n",
    "        duration__gt=0.,\n",
    "        cinematic=True,\n",
    "        video__ignore_film=False\n",
    "    )\n",
    "    total=shots_qs.count()\n",
    "    \n",
    "    # Shot_IS<>\n",
    "    short_shots = VideoIntervalCollection3D.from_django_qs(shots_qs, progress=True, total=total)\n",
    "    \n",
    "    def get_all_frames(short_shots):\n",
    "        def update(frames, shot):\n",
    "            return frames + list(range(shot.t[0], shot.t[1]+1))\n",
    "        return short_shots.fold(update, [])\n",
    "    \n",
    "    video_to_frame_numbers = get_all_frames(short_shots)\n",
    "    all_frames = VideoIntervalCollection3D({})\n",
    "    for video, frames in tqdm(video_to_frame_numbers.items()):\n",
    "        # Getting brightness\n",
    "        qs = Frame.objects.filter(video_id=video, number__in=frames, regularly_sampled=True).order_by('number')\n",
    "        all_frames = all_frames.union(VideoIntervalCollection3D.from_django_qs(\n",
    "            qs, schema={'t1':'number', 't2': 'number', 'payload': 'brightness'}), profile=False)\n",
    "    \n",
    "    def select_second(p):\n",
    "        return p[1]\n",
    "    \n",
    "    # Shot_IS<Frame_IS>\n",
    "    shots_with_brightness = short_shots.collect_by_interval(\n",
    "        all_frames, T(overlaps()), time_window=0, filter_empty=False\n",
    "    ).map_payload(select_second)\n",
    "    \n",
    "    # Sequence_IS<Shot_IS<Frame_IS>>\n",
    "    one_shots = shots_with_brightness.collect_by_interval(\n",
    "        shots_with_brightness, T(equal()), time_window=0).map_payload(select_second)\n",
    "    n_shots = one_shots\n",
    "    for n in range(2, NUM_SHOTS+1):\n",
    "        print(\"Constructing {} consecutive short shots\".format(n))\n",
    "        n_shots = n_shots.merge(\n",
    "            one_shots,\n",
    "            T(meets_before(epsilon=1)),\n",
    "            payload_merge_op = IntervalSet3D.union,            \n",
    "            time_window=1, profile=False)\n",
    "        print('There are {} videos with {} consecutive short shots'.format(\n",
    "                 len(n_shots.get_allintervals()), n))\n",
    "        \n",
    "    def merge_shots(seq1, seq2):\n",
    "        return seq1.union(seq2.minus(seq1))\n",
    "    coalesced_n_shots = n_shots.temporal_coalesce(payload_merge_op=merge_shots)\n",
    "    \n",
    "    def bright_enough(shots):\n",
    "        # Check if any shots is above mean brightness threshold\n",
    "        def compute_avg_brightness(frames):\n",
    "            ret = frames.fold(lambda acc, f: acc+f.payload, 0)\n",
    "            if not frames.empty():\n",
    "                ret = ret / frames.size()\n",
    "            return ret\n",
    "        return shots.map_payload(compute_avg_brightness).fold(\n",
    "            lambda acc, shot: acc or shot.payload > BRIGHTNESS_THRESHOLD)\n",
    "            \n",
    "    n_bright_shots = n_shots.filter(P(bright_enough))\n",
    "    \n",
    "    vids = n_bright_shots.get_allintervals().keys()\n",
    "    results = get_all_segments(vids)\n",
    "    fps_map = dict((i, Video.objects.get(id=i).fps) for i in vids)\n",
    "    \n",
    "    # Word_IS<fps>\n",
    "    caption_results = VideoIntervalCollection3D({\n",
    "        video_id: IntervalSet3D([Interval3D(\n",
    "            (word[0] * fps_map[video_id], word[1] * fps_map[video_id]),\n",
    "            payload = fps_map[video_id])\n",
    "            for word in words])\n",
    "        for video_id, words in results\n",
    "    })\n",
    "    \n",
    "    def has_few_words(seq):\n",
    "        _, words = seq.payload\n",
    "        n_words = words.size()\n",
    "        if n_words == 0:\n",
    "            return True\n",
    "        time = seq.length() / words.get_intervals()[0].payload\n",
    "        return n_words / time <= MAX_NUM_WORDS_PER_SECOND\n",
    "    \n",
    "    # Seq_IS<(Shot_IS<Frame_IS>, Word_IS)>\n",
    "    n_bright_shots_with_few_words = n_bright_shots.collect_by_interval(\n",
    "        caption_results,\n",
    "        T(overlaps()),\n",
    "        time_window=0,\n",
    "        filter_empty=False).filter(has_few_words)\n",
    "    \n",
    "    # Seq_IS<Shot_IS<Frame_IS>>\n",
    "    action_shots = coalesced_n_shots.filter_against(\n",
    "        n_bright_shots_with_few_words,\n",
    "        T(overlaps()),\n",
    "        time_window=0)\n",
    "    \n",
    "    return action_shots\n",
    "\n",
    "answer = action_shots()\n",
    "display_result(convert_to_1d_collection(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T09:25:48.264643Z",
     "start_time": "2019-02-09T09:25:48.229274Z"
    }
   },
   "source": [
    "# Conversations with Identity Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T00:55:19.567309Z",
     "start_time": "2019-02-22T00:55:19.523129Z"
    }
   },
   "outputs": [],
   "source": [
    "def coalesce(self, predicate, payload_merge_op):\n",
    "    from rekall.interval_set_3d import IntervalSet3D\n",
    "    # State is (new, current)\n",
    "    def update(state, interval):\n",
    "        new, current = state\n",
    "        updated_current = []\n",
    "        size = len(new)+len(current)\n",
    "        for cur in current:\n",
    "            # No more intervals will overlap with cur\n",
    "            if cur.t[1] < interval.t[0]:\n",
    "                new.append(cur)\n",
    "            else:\n",
    "                updated_current.append(cur)\n",
    "        matched = None\n",
    "        for i, cur in enumerate(updated_current):\n",
    "            if predicate(cur, interval):\n",
    "                matched = i\n",
    "                break\n",
    "        if matched is None:\n",
    "            updated_current.append(interval)\n",
    "        else:\n",
    "            updated_current[matched] = updated_current[matched].merge(interval, payload_merge_op)\n",
    "        return new, updated_current\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        old = self.size(profile=False)\n",
    "        self = self.fold_to_set(update, ([],[]), acc_to_set=lambda state: IntervalSet3D(state[0]+state[1]),\n",
    "                                profile=False)\n",
    "        converged = old == self.size(profile=False)\n",
    "        # print(old, self.size(profile=False))\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T00:55:20.184892Z",
     "start_time": "2019-02-22T00:55:20.050279Z"
    }
   },
   "outputs": [],
   "source": [
    "def conversationsq(vids, progress=True):\n",
    "    from query.models import FaceCharacterActor, Shot\n",
    "    from rekall.video_interval_collection_3d import VideoIntervalCollection3D\n",
    "    from rekall.interval_set_3d import IntervalSet3D, Interval3D\n",
    "    from rekall.temporal_predicates import before, overlaps, equal\n",
    "    from rekall.interval_set_3d_utils import T,P,XY,or_preds,X,Y\n",
    "    \n",
    "    faces_with_character_actor_qs = FaceCharacterActor.objects.annotate(\n",
    "        min_frame=F('face__frame__number'),\n",
    "        max_frame=F('face__frame__number'),\n",
    "        video_id=F('face__frame__video_id'),\n",
    "        bbox_x1=F('face__bbox_x1'),\n",
    "        bbox_y1=F('face__bbox_y1'),\n",
    "        bbox_x2=F('face__bbox_x2'),\n",
    "        bbox_y2=F('face__bbox_y2'),\n",
    "        character_name=F('characteractor__character__name')\n",
    "    ).filter(face__frame__video_id__in=vids)\n",
    "    \n",
    "    total = faces_with_character_actor_qs.count()\n",
    "    \n",
    "    schema = VideoIntervalCollection3D.django_bbox_default_schema()\n",
    "    schema['payload'] = 'character_name'    \n",
    "    all_faces = VideoIntervalCollection3D.from_django_qs(faces_with_character_actor_qs, schema, progress=progress,\n",
    "                                                    total=total)\n",
    "    \n",
    "    vids = all_faces.get_allintervals().keys()\n",
    "    if len(vids) == 0:\n",
    "        return VideoIntervalCollection3D({})\n",
    "    \n",
    "    shots_qs = Shot.objects.filter(\n",
    "        video_id__in = vids,\n",
    "        cinematic = True,\n",
    "    )\n",
    "    total = shots_qs.count()\n",
    "    shots = VideoIntervalCollection3D.from_django_qs(\n",
    "        shots_qs,\n",
    "        progress=progress, total=total\n",
    "    )\n",
    "    \n",
    "    def select_second(p):\n",
    "        return p[1]\n",
    "    \n",
    "    # Shot_IS<Face_IS>\n",
    "    shots_with_frames = shots.collect_by_interval(\n",
    "        all_faces, T(overlaps()), time_window=0, filter_empty=True).map_payload(select_second)\n",
    "    \n",
    "    def group_characters(faces):\n",
    "        def get_char(face):\n",
    "            return face.payload\n",
    "        def merge(char, faces):\n",
    "            merged_interval = faces.fold(Interval3D.merge)\n",
    "            merged_interval.payload = (char, faces)\n",
    "            return merged_interval\n",
    "        return faces.group_by(get_char, merge)\n",
    "    \n",
    "    # Shot_IS<Char_IS<(char, Face_IS)>>\n",
    "    shots_with_chars = shots_with_frames.map_payload(group_characters)\n",
    "    \n",
    "    def cross_product_chars(chars1, chars2):\n",
    "        def get_chars(chars):\n",
    "            def update(acc, char):\n",
    "                acc.append(char.payload[0])\n",
    "                return acc\n",
    "            return chars.fold(update, [])\n",
    "        chars_in_1 = get_chars(chars1)\n",
    "        chars_in_2 = get_chars(chars2)\n",
    "        result = []\n",
    "        for charA in chars_in_1:\n",
    "            for charB in chars_in_2:\n",
    "                if charA != charB:\n",
    "                    result.append((charA, charB))\n",
    "        return result\n",
    "            \n",
    "    # Seq_IS<[(char, char)]>\n",
    "    two_shots = shots_with_chars.merge(\n",
    "        shots_with_chars,\n",
    "        T(before(max_dist=1)),\n",
    "        payload_merge_op=cross_product_chars,\n",
    "        time_window=1\n",
    "    )\n",
    "    \n",
    "    def sequences_share_face_pair(list1, list2):\n",
    "        for A1, B1 in list1:\n",
    "            for A2, B2 in list2:\n",
    "                if ((A1==A2 and B1==B2) or\n",
    "                    (A1==B2 and B1==A2)):\n",
    "                    return True\n",
    "        return False\n",
    "    def merge_face_pairs(list1, list2):\n",
    "        return list1+list2\n",
    "    \n",
    "    conv_candidates = coalesce(two_shots, P(sequences_share_face_pair), merge_face_pairs)\n",
    "    \n",
    "    def num_shots_at_least(n):\n",
    "        def pred(shots):\n",
    "            return shots.size() >= n\n",
    "        return pred\n",
    "    \n",
    "    # Conv_IS<Shot_IS<Char_IS<(char, Face_IS)>>>\n",
    "    convs = conv_candidates.collect_by_interval(\n",
    "        shots_with_chars, T(overlaps()), time_window=0, filter_empty=True\n",
    "    ).map_payload(select_second\n",
    "    ).filter(P(num_shots_at_least(3)))\n",
    "    \n",
    "    \n",
    "    return convs\n",
    "\n",
    "#answer = conversationsq([380])\n",
    "#display_result(convert_to_1d_collection(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShotScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T00:55:21.518619Z",
     "start_time": "2019-02-22T00:55:21.200157Z"
    }
   },
   "outputs": [],
   "source": [
    "def shot_scale_q(vids, progress=True):\n",
    "    from query.models import Face, Pose, Shot\n",
    "    from rekall.video_interval_collection_3d import VideoIntervalCollection3D\n",
    "    from rekall.interval_set_3d import IntervalSet3D, Interval3D\n",
    "    from rekall.temporal_predicates import before, overlaps, equal\n",
    "    from rekall.interval_set_3d_utils import T,P,XY,or_preds,X,Y\n",
    "    from esper import pose_wrapper as pw\n",
    "    from esper import shot_scale\n",
    "    \n",
    "    shots_qs = Shot.objects.filter(video_id__in=vids, cinematic=True)\n",
    "    total = shots_qs.count()\n",
    "    # Shot_IS\n",
    "    shots = VideoIntervalCollection3D.from_django_qs(shots_qs, progress=progress, total=total)\n",
    "    \n",
    "    faces_qs= Face.objects.filter(frame__video_id__in=vids, frame__regularly_sampled=True\n",
    "                                 ).annotate(video_id=F('frame__video__id'),\n",
    "                                            number=F('frame__number'))\n",
    "    poses_qs = PoseMeta.objects.filter(frame__video_id__in=vids, frame__regularly_sampled=True\n",
    "                                 ).annotate(video_id=F('frame__video__id'),\n",
    "                                            number=F('frame__number'))\n",
    "    \n",
    "    total = faces_qs.count()\n",
    "    all_faces = VideoIntervalCollection3D.from_django_qs(faces_qs, schema={\n",
    "        't1':'number', 't2':'number', 'x1':'bbox_x1', 'x2':'bbox_x2', 'y1':'bbox_y1', 'y2':'bbox_y2'\n",
    "    }, progress=progress, total=total)\n",
    "    \n",
    "    total = poses_qs.count()\n",
    "    all_poses = VideoIntervalCollection3D.from_django_qs(poses_qs, schema={\n",
    "        't1':'number', 't2':'number', 'payload':'id'\n",
    "    }, progress=progress, total=total)\n",
    "    \n",
    "    # print(\"Loading {0} Poses\".format(total))\n",
    "    \n",
    "    def get_pose_map(poses_qs):\n",
    "        poses = pw.get(poses_qs)\n",
    "        # print(\"Poses loaded\")\n",
    "        return {pose.id: pose for pose in poses}\n",
    "    \n",
    "    def get_pose(pose_map):\n",
    "        def map_fn(payload):\n",
    "            return pose_map[payload]\n",
    "        return map_fn\n",
    "    \n",
    "    all_poses = all_poses.map_payload(get_pose(get_pose_map(poses_qs)), profile=progress)\n",
    "    \n",
    "    def add_scale_to_face(face):\n",
    "        face.payload = shot_scale.face_height_to_shot_scale(face.height())\n",
    "        return face\n",
    "    \n",
    "    # Face_IS<Scale>\n",
    "    all_faces_with_scale = all_faces.map(add_scale_to_face, profile=progress)\n",
    "    \n",
    "    def add_scale_to_pose(pose):\n",
    "        return {\n",
    "            'pose': pose,\n",
    "            'scale': shot_scale.pose_keypoints_to_shot_scale(pose.pose_keypoints())\n",
    "        }\n",
    "    # Pose_IS<{'pose','scale'}>\n",
    "    all_poses_with_scale = all_poses.map_payload(add_scale_to_pose, profile=progress)\n",
    "    \n",
    "    # Frame_IS\n",
    "    all_frames_with_signal = all_faces.union(all_poses).group_by_time(profile=progress)\n",
    "    \n",
    "    def select_second(p):\n",
    "        return p[1]\n",
    "    \n",
    "    # Frame_IS<(Face_IS, Pose_IS)>\n",
    "    frames_with_faces_and_poses = all_frames_with_signal.collect_by_interval(\n",
    "        all_faces_with_scale,\n",
    "        T(overlaps()),\n",
    "        filter_empty=False,\n",
    "        time_window=0, profile=progress,\n",
    "    ).map_payload(select_second).collect_by_interval(\n",
    "        all_poses_with_scale,\n",
    "        T(overlaps()),\n",
    "        filter_empty=False,\n",
    "        time_window=0, profile=progress\n",
    "    )\n",
    "    \n",
    "    def add_scale_to_frame(payload):\n",
    "        faces, poses = payload\n",
    "        def take_max_face(acc, face):\n",
    "            return max(acc, face.payload)\n",
    "        def take_max_pose(acc, pose):\n",
    "            return max(acc, pose.payload['scale'])\n",
    "        max_face_scale = faces.fold(take_max_face, shot_scale.ShotScale.UNKNOWN)\n",
    "        max_pose_scale = poses.fold(take_max_pose, shot_scale.ShotScale.UNKNOWN)\n",
    "        return (max(max_face_scale, max_pose_scale), faces, poses)\n",
    "    \n",
    "    # Frame_IS<(Scale, Face_IS, Pose_IS)>\n",
    "    frames_with_scale = frames_with_faces_and_poses.map_payload(add_scale_to_frame, profile=progress)\n",
    "    \n",
    "    # Shot_IS<Frame_IS<(Scale, Face_IS, Pose_IS)>>\n",
    "    shots_with_frames = shots.collect_by_interval(\n",
    "        frames_with_scale,\n",
    "        T(overlaps()),\n",
    "        filter_empty=False,\n",
    "        time_window=0, profile=progress,\n",
    "    ).map_payload(select_second)\n",
    "    \n",
    "    def get_mode(scales):\n",
    "        count = {}\n",
    "        for s in shot_scale.ShotScale:\n",
    "            count[s] = 0\n",
    "        for s in scales:\n",
    "            count[s]+=1\n",
    "        best_count = 0\n",
    "        best_scale=shot_scale.ShotScale.UNKNOWN\n",
    "        for s in shot_scale.ShotScale:\n",
    "            if count[s]>=best_count:\n",
    "                best_count = count[s]\n",
    "                best_scale = s\n",
    "        return best_scale\n",
    "    \n",
    "    def add_scale_to_shot(frames):\n",
    "        def update(acc, frame):\n",
    "            acc.append(frame.payload[0])\n",
    "            return acc\n",
    "        scales = frames.fold(update, [])\n",
    "        mode = get_mode(scales)\n",
    "        return mode, frames\n",
    "    \n",
    "    # Shot_IS<(Scale, Frame_IS<(Scale, Face_IS<Scale>, Pose_IS<(Scale, PoseKeypoints)>)>)>\n",
    "    shots_with_scale = shots_with_frames.map_payload(add_scale_to_shot, profile=progress)\n",
    "    return shots_with_scale\n",
    "\n",
    "def payload_to_vgrid_objects(payload):\n",
    "    from rekall.interval_set_3d_utils import P\n",
    "    # Frame_IS<(Scale, Face_IS, Pose_IS)>\n",
    "    scale, frames = payload\n",
    "    def face_to_object(face):\n",
    "        return {\n",
    "            'type': 'bbox',\n",
    "            'bbox_x1': face.x[0], 'bbox_x2': face.x[1],\n",
    "            'bbox_y1': face.y[0], 'bbox_y2': face.y[1]\n",
    "        }\n",
    "    \n",
    "    def pose_to_object(pose):\n",
    "        from esper.stdlib import pose_to_dict\n",
    "        return pose_to_dict(pose.payload['pose'])\n",
    "    \n",
    "    def face_objects_at_scale(faces, scale):\n",
    "        faces = faces.filter(P(lambda p:p==scale))\n",
    "        def update(acc, face):\n",
    "            acc.append(face_to_object(face))\n",
    "            return acc\n",
    "        return faces.fold(update, [])\n",
    "    \n",
    "    def pose_objects_at_scale(poses, scale):\n",
    "        poses = poses.filter(P(lambda p:p['scale']==scale))\n",
    "        def update(acc, pose):\n",
    "            acc.append(pose_to_object(pose))\n",
    "            return acc\n",
    "        return poses.fold(update, [])\n",
    "    \n",
    "    frames = frames.filter(P(lambda p:p[0]==scale))\n",
    "    def update(acc, frame):\n",
    "        _, faces, poses = frame.payload\n",
    "        return acc + face_objects_at_scale(faces, scale) + pose_objects_at_scale(poses, scale)\n",
    "    return frames.fold(update, [])\n",
    "\n",
    "#answer = shot_scale_q([1])\n",
    "#display_result(convert_to_1d_collection(answer.map_payload(payload_to_vgrid_objects)), display_payload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:05:21.717255Z",
     "start_time": "2019-02-22T01:05:19.654967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query in Runtime starts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                               | 0/642 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [378]\n",
      "Processing [614]\n",
      "Processing [327]\n",
      "Processing [323]\n",
      "Processing [615]\n",
      "Processing [439]\n",
      "Processing [249]\n",
      "Processing [312]\n",
      "Processing [446]\n",
      "Processing [591]\n",
      "Processing [148]\n",
      "Processing [140]\n",
      "Processing [41]\n",
      "Processing [182]\n",
      "Processing [132]\n",
      "Processing [109]\n",
      "Processing [601]\n",
      "Processing [229]\n",
      "Processing [234]\n",
      "Processing [501]\n",
      "Processing [445]\n",
      "Processing [47]\n",
      "Processing [173]\n",
      "Processing [16]\n",
      "Processing [264]\n",
      "Processing [338]\n",
      "Processing [642]\n",
      "Processing [146]\n",
      "Processing [369]\n",
      "Processing [172]\n",
      "Processing [479]\n",
      "Processing [590]\n",
      "Processing [274]\n",
      "Processing [301]\n",
      "Processing [103]\n",
      "Processing [452]\n",
      "Processing [537]\n",
      "Processing [293]\n",
      "Processing [436]\n",
      "Processing [582]\n",
      "Processing [18]\n",
      "Processing [412]\n",
      "Processing [423]\n",
      "Processing [343]\n",
      "Processing [277]\n",
      "Processing [52]\n",
      "Processing [283]\n",
      "Processing [185]\n",
      "Processing [334]\n",
      "Processing [371]\n",
      "Processing [574]\n",
      "Processing [226]\n",
      "Processing [152]\n",
      "Processing [483]\n",
      "Processing [113]\n",
      "Processing [596]\n",
      "Processing [225]\n",
      "Processing [44]\n",
      "Processing [565]\n",
      "Processing [120]\n",
      "Processing [85]\n",
      "Processing [168]\n",
      "Processing [358]\n",
      "Processing [310]\n",
      "Processing [243]\n",
      "Processing [551]\n",
      "Processing [569]\n",
      "Processing [533]\n",
      "Processing [480]\n",
      "Processing [23]\n",
      "Processing [107]\n",
      "Processing [303]\n",
      "Processing [237]\n",
      "Processing [101]\n",
      "Processing [363]\n",
      "Processing [265]\n",
      "Processing [526]\n",
      "Processing [55]\n",
      "Processing [496]\n",
      "Processing [4]\n",
      "Processing [628]\n",
      "Processing [572]\n",
      "Processing [578]\n",
      "Processing [91]\n",
      "Processing [48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████▎                                                                        | 93/642 [00:00<00:00, 917.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [409]\n",
      "Processing [175]\n",
      "Processing [368]\n",
      "Processing [561]\n",
      "Processing [279]\n",
      "Processing [442]\n",
      "Processing [521]\n",
      "Processing [307]\n",
      "Processing [311]\n",
      "Processing [431]\n",
      "Processing [389]\n",
      "Processing [461]\n",
      "Processing [603]\n",
      "Processing [340]\n",
      "Processing [511]\n",
      "Processing [14]\n",
      "Processing [291]\n",
      "Processing [416]\n",
      "Processing [51]\n",
      "Processing [3]\n",
      "Processing [534]\n",
      "Processing [463]\n",
      "Processing [459]\n",
      "Processing [286]\n",
      "Processing [401]\n",
      "Processing [208]\n",
      "Processing [353]\n",
      "Processing [622]\n",
      "Processing [434]\n",
      "Processing [542]\n",
      "Processing [78]\n",
      "Processing [200]\n",
      "Processing [383]\n",
      "Processing [106]\n",
      "Processing [210]\n",
      "Processing [356]\n",
      "Processing [558]\n",
      "Processing [313]\n",
      "Processing [269]\n",
      "Processing [207]\n",
      "Processing [438]\n",
      "Processing [228]\n",
      "Processing [539]\n",
      "Processing [441]\n",
      "Processing [449]\n",
      "Processing [42]\n",
      "Processing [230]\n",
      "Processing [272]\n",
      "Processing [470]\n",
      "Processing [139]\n",
      "Processing [576]\n",
      "Processing [336]\n",
      "Processing [457]\n",
      "Processing [151]\n",
      "Processing [157]\n",
      "Processing [261]\n",
      "Processing [196]\n",
      "Processing [497]\n",
      "Processing [522]\n",
      "Processing [214]\n",
      "Processing [294]\n",
      "Processing [179]\n",
      "Processing [253]\n",
      "Processing [329]\n",
      "Processing [621]\n",
      "Processing [464]\n",
      "Processing [270]\n",
      "Processing [355]\n",
      "Processing [198]\n",
      "Processing [360]\n",
      "Processing [404]\n",
      "Processing [278]\n",
      "Processing [38]\n",
      "Processing [189]\n",
      "Processing [586]\n",
      "Processing [585]\n",
      "Processing [637]\n",
      "Processing [433]\n",
      "Processing [254]\n",
      "Processing [372]\n",
      "Processing [428]\n",
      "Processing [250]\n",
      "Processing [188]\n",
      "Processing [93]\n",
      "Processing [367]\n",
      "Processing [485]\n",
      "Processing [171]\n",
      "Processing [121]\n",
      "Processing [246]\n",
      "Processing [19]\n",
      "Processing [599]\n",
      "Processing [493]\n",
      "Processing [216]\n",
      "Processing [193]\n",
      "Processing [211]\n",
      "Processing [102]\n",
      "Processing [379]\n",
      "Processing [282]\n",
      "Processing [554]\n",
      "Processing [231]\n",
      "Processing [382]\n",
      "Processing [20]\n",
      "Processing [468]\n",
      "Processing [317]\n",
      "Processing [391]\n",
      "Processing [454]\n",
      "Processing [221]\n",
      "Processing [248]\n",
      "Processing [300]\n",
      "Processing [275]\n",
      "Processing [538]\n",
      "Processing [616]\n",
      "Processing [589]\n",
      "Processing [156]\n",
      "Processing [515]\n",
      "Processing [429]\n",
      "Processing [608]\n",
      "Processing [440]\n",
      "Processing [39]\n",
      "Processing [388]\n",
      "Processing [260]\n",
      "Processing [125]\n",
      "Processing [437]\n",
      "Processing [567]\n",
      "Processing [604]\n",
      "Processing [143]\n",
      "Processing [427]\n",
      "Processing [373]\n",
      "Processing [640]\n",
      "Processing [64]\n",
      "Processing [9]\n",
      "Processing [525]\n",
      "Processing [25]\n",
      "Processing [529]\n",
      "Processing [29]\n",
      "Processing [342]\n",
      "Processing [597]\n",
      "Processing [413]\n",
      "Processing [133]\n",
      "Processing [240]\n",
      "Processing [268]\n",
      "Processing [527]\n",
      "Processing [8]\n",
      "Processing [535]\n",
      "Processing [417]\n",
      "Processing [24]\n",
      "Processing [499]\n",
      "Processing [73]\n",
      "Processing [328]\n",
      "Processing [351]\n",
      "Processing [377]\n",
      "Processing [451]\n",
      "Processing [563]\n",
      "Processing [518]\n",
      "Processing [547]\n",
      "Processing [505]\n",
      "Processing [219]\n",
      "Processing [100]\n",
      "Processing [374]\n",
      "Processing [82]\n",
      "Processing [127]\n",
      "Processing [549]\n",
      "Processing [492]\n",
      "Processing [30]\n",
      "Processing [545]\n",
      "Processing [458]\n",
      "Processing [60]\n",
      "Processing [1]\n",
      "Processing [233]\n",
      "Processing [147]\n",
      "Processing [502]\n",
      "Processing [32]\n",
      "Processing [298]\n",
      "Processing [632]\n",
      "Processing [84]\n",
      "Processing [443]\n",
      "Processing [186]\n",
      "Processing [45]\n",
      "Processing [53]\n",
      "Processing [524]\n",
      "Processing [61]\n",
      "Processing [72]\n",
      "Processing [174]\n",
      "Processing [164]\n",
      "Processing [13]\n",
      "Processing [460]\n",
      "Processing [516]\n",
      "Processing [387]\n",
      "Processing [124]\n",
      "Processing [399]\n",
      "Processing [56]\n",
      "Processing [17]\n",
      "Processing [627]\n",
      "Processing [263]\n",
      "Processing [104]\n",
      "Processing [407]\n",
      "Processing [453]\n",
      "Processing [71]\n",
      "Processing [495]\n",
      "Processing [74]\n",
      "Processing [202]\n",
      "Processing [227]\n",
      "Processing [630]\n",
      "Processing [69]\n",
      "Processing [415]\n",
      "Processing [27]\n",
      "Processing [266]\n",
      "Processing [514]\n",
      "Processing [66]\n",
      "Processing [424]\n",
      "Processing [180]\n",
      "Processing [98]\n",
      "Processing [600]\n",
      "Processing [7]\n",
      "Processing [22]\n",
      "Processing [105]\n",
      "Processing [94]\n",
      "Processing [15]\n",
      "Processing [287]\n",
      "Processing [573]\n",
      "Processing [153]\n",
      "Processing [92]\n",
      "Processing [138]\n",
      "Processing [309]\n",
      "Processing [76]\n",
      "Processing [564]\n",
      "Processing [594]\n",
      "Processing [244]\n",
      "Processing [197]\n",
      "Processing [160]\n",
      "Processing [385]\n",
      "Processing [256]\n",
      "Processing [364]\n",
      "Processing [341]\n",
      "Processing [281]\n",
      "Processing [252]\n",
      "Processing [239]\n",
      "Processing [117]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|██████████████████████████████████████████▋                                        | 330/642 [00:00<00:00, 1123.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [123]\n",
      "Processing [487]\n",
      "Processing [68]\n",
      "Processing [584]\n",
      "Processing [595]\n",
      "Processing [405]\n",
      "Processing [613]\n",
      "Processing [471]\n",
      "Processing [555]\n",
      "Processing [116]\n",
      "Processing [325]\n",
      "Processing [472]\n",
      "Processing [376]\n",
      "Processing [319]\n",
      "Processing [548]\n",
      "Processing [28]\n",
      "Processing [556]\n",
      "Processing [297]\n",
      "Processing [284]\n",
      "Processing [190]\n",
      "Processing [144]\n",
      "Processing [295]\n",
      "Processing [419]\n",
      "Processing [448]\n",
      "Processing [99]\n",
      "Processing [191]\n",
      "Processing [235]\n",
      "Processing [482]\n",
      "Processing [484]\n",
      "Processing [177]\n",
      "Processing [26]\n",
      "Processing [324]\n",
      "Processing [635]\n",
      "Processing [163]\n",
      "Processing [170]\n",
      "Processing [262]\n",
      "Processing [35]\n",
      "Processing [432]\n",
      "Processing [236]\n",
      "Processing [288]\n",
      "Processing [169]\n",
      "Processing [154]\n",
      "Processing [444]\n",
      "Processing [639]\n",
      "Processing [579]\n",
      "Processing [605]\n",
      "Processing [541]\n",
      "Processing [398]\n",
      "Processing [178]\n",
      "Processing [507]\n",
      "Executing in workers starts.\n",
      "Dispatching tasks starts.\n",
      "Dispatching tasks ends after 0.04 seconds\n",
      "Processing [305]\n",
      "Processing [199]\n",
      "Processing [111]\n",
      "Processing [347]\n",
      "Processing [612]\n",
      "Processing [536]\n",
      "Processing [456]\n",
      "Processing [467]\n",
      "Processing [70]\n",
      "Processing [610]\n",
      "Processing [63]\n",
      "Processing [380]\n",
      "Processing [553]\n",
      "Processing [506]\n",
      "Processing [337]\n",
      "Processing [392]\n",
      "Processing [79]\n",
      "Processing [142]\n",
      "Processing [267]\n",
      "Processing [187]\n",
      "Processing [617]\n",
      "Processing [359]\n",
      "Processing [308]\n",
      "Processing [34]\n",
      "Processing [136]\n",
      "Processing [531]\n",
      "Processing [162]\n",
      "Processing [10]\n",
      "Processing [90]\n",
      "Processing [131]\n",
      "Processing [81]\n",
      "Processing [192]\n",
      "Processing [330]\n",
      "Processing [500]\n",
      "Processing [145]\n",
      "Processing [280]\n",
      "Processing [215]\n",
      "Processing [201]\n",
      "Processing [67]\n",
      "Processing [384]\n",
      "Processing [258]\n",
      "Processing [633]\n",
      "Processing [625]\n",
      "Processing [634]\n",
      "Processing [65]\n",
      "Processing [326]\n",
      "Processing [523]\n",
      "Processing [59]\n",
      "Processing [49]\n",
      "Processing [224]\n",
      "Processing [141]\n",
      "Processing [31]\n",
      "Processing [491]\n",
      "Processing [306]\n",
      "Processing [350]\n",
      "Processing [489]\n",
      "Processing [508]\n",
      "Processing [626]\n",
      "Processing [577]\n",
      "Processing [450]\n",
      "Processing [607]\n",
      "Processing [629]\n",
      "Processing [159]\n",
      "Processing [557]\n",
      "Processing [503]\n",
      "Processing [410]\n",
      "Processing [89]\n",
      "Processing [641]\n",
      "Processing [425]\n",
      "Processing [636]\n",
      "Processing [314]\n",
      "Processing [618]\n",
      "Processing [37]\n",
      "Processing [112]\n",
      "Processing [583]\n",
      "Processing [273]\n",
      "Processing [361]\n",
      "Processing [114]\n",
      "Processing [126]\n",
      "Processing [488]\n",
      "Processing [195]\n",
      "Processing [6]\n",
      "Processing [581]\n",
      "Processing [559]\n",
      "Processing [322]\n",
      "Processing [532]\n",
      "Processing [426]\n",
      "Processing [149]\n",
      "Processing [403]\n",
      "Processing [242]\n",
      "Processing [519]\n",
      "Processing [176]\n",
      "Processing [455]\n",
      "Processing [320]\n",
      "Processing [166]\n",
      "Processing [349]\n",
      "Processing [566]\n",
      "Processing [411]\n",
      "Processing [420]\n",
      "Processing [478]\n",
      "Processing [510]\n",
      "Processing [135]\n",
      "Processing [21]\n",
      "Processing [395]\n",
      "Processing [213]\n",
      "Processing [571]\n",
      "Processing [435]\n",
      "Processing [212]\n",
      "Processing [352]\n",
      "Processing [638]\n",
      "Processing [184]\n",
      "Processing [241]\n",
      "Processing [203]\n",
      "Processing [606]\n",
      "Processing [509]\n",
      "Processing [155]\n",
      "Processing [562]\n",
      "Processing [393]\n",
      "Processing [57]\n",
      "Processing [575]\n",
      "Processing [466]\n",
      "Processing [181]\n",
      "Processing [183]\n",
      "Processing [530]\n",
      "Processing [115]\n",
      "Processing [12]\n",
      "Processing [58]\n",
      "Processing [33]\n",
      "Processing [5]\n",
      "Processing [396]\n",
      "Processing [381]\n",
      "Processing [88]\n",
      "Processing [422]\n",
      "Processing [137]\n",
      "Processing [304]\n",
      "Processing [128]\n",
      "Processing [209]\n",
      "Processing [414]\n",
      "Processing [631]\n",
      "Processing [580]\n",
      "Processing [552]\n",
      "Processing [43]\n",
      "Processing [80]\n",
      "Processing [339]\n",
      "Processing [504]\n",
      "Processing [513]\n",
      "Processing [150]\n",
      "Processing [129]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|██████████████████████████████████████████████████████████████████████▋             | 540/642 [00:00<00:00, 980.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [158]\n",
      "Processing [418]\n",
      "Processing [486]\n",
      "Processing [592]\n",
      "Processing [400]\n",
      "Processing [520]\n",
      "Processing [321]\n",
      "Processing [447]\n",
      "Processing [62]\n",
      "Processing [587]\n",
      "Processing [205]\n",
      "Processing [194]\n",
      "Processing [619]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 642/642 [00:00<00:00, 1286.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [512]\n",
      "Processing [475]\n",
      "Processing [86]\n",
      "Processing [540]\n",
      "Processing [623]\n",
      "Processing [289]\n",
      "Processing [83]\n",
      "Processing [50]\n",
      "Processing [11]\n",
      "Processing [290]\n",
      "Processing [494]\n",
      "Processing [465]\n",
      "Processing [517]\n",
      "Processing [316]\n",
      "Processing [40]\n",
      "Processing [406]\n",
      "Processing [421]\n",
      "Processing [344]\n",
      "Processing [593]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [624]\n",
      "Processing [130]\n",
      "Processing [251]\n",
      "Processing [218]\n",
      "Processing [474]\n",
      "Processing [365]\n",
      "Processing [345]\n",
      "Processing [96]\n",
      "Processing [602]\n",
      "Processing [87]\n",
      "Processing [161]\n",
      "Processing [204]\n",
      "Processing [333]\n",
      "Processing [276]\n",
      "Processing [134]\n",
      "Processing [245]\n",
      "Processing [370]\n",
      "Processing [119]\n",
      "Processing [167]\n",
      "Processing [570]\n",
      "Processing [476]\n",
      "Processing [348]\n",
      "Processing [110]\n",
      "Processing [598]\n",
      "Processing [357]\n",
      "Processing [473]\n",
      "Processing [75]\n",
      "Processing [481]\n",
      "Processing [366]\n",
      "Processing [315]\n",
      "Processing [54]\n",
      "Processing [259]\n",
      "Processing [408]\n",
      "Processing [335]\n",
      "Processing [238]\n",
      "Processing [232]\n",
      "Processing [108]\n",
      "Processing [222]\n",
      "Processing [477]\n",
      "Processing [362]\n",
      "Processing [469]\n",
      "Processing [122]\n",
      "Processing [46]\n",
      "Processing [620]\n",
      "Processing [217]\n",
      "Processing [560]\n",
      "Processing [332]\n",
      "Processing [609]\n",
      "Processing [165]\n",
      "Processing [331]\n",
      "Processing [247]\n",
      "Processing [95]\n",
      "Processing [285]\n",
      "Processing [302]\n",
      "Processing [257]\n",
      "Processing [402]\n",
      "Processing [375]\n",
      "Processing [255]\n",
      "Processing [77]\n",
      "Processing [568]\n",
      "Processing [528]\n",
      "Processing [462]\n",
      "Processing [386]\n",
      "Processing [299]\n",
      "Processing [394]\n",
      "Processing [271]\n",
      "Processing [611]\n",
      "Processing [588]\n",
      "Processing [296]\n",
      "Processing [498]\n",
      "Processing [543]\n",
      "Processing [206]\n",
      "Processing [546]\n",
      "Processing [2]\n",
      "Processing [36]\n",
      "Processing [292]\n",
      "Processing [390]\n",
      "Processing [544]\n",
      "Processing [223]\n",
      "Processing [346]\n",
      "Processing [318]\n",
      "Processing [550]\n",
      "Processing [118]\n",
      "Processing [354]\n",
      "Processing [220]\n",
      "Processing [490]\n",
      "Processing [430]\n",
      "Processing [97]\n",
      "Processing [397]\n",
      "Executing in workers ends after 0.49 seconds\n",
      "Combining results from workers starts.\n",
      "Combining results from workers ends after 0.00 seconds\n",
      "Executing query in Runtime ends after 0.88 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbee720c23394dd98169b997204463d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(jsglobals={'schema': [['Identity', ['id', 'name']], ['Genre', ['id', 'name']], ['Director', ['id',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dummy_func(vids):\n",
    "    from rekall.interval_set_3d import IntervalSet3D, Interval3D\n",
    "    from rekall.video_interval_collection_3d import VideoIntervalCollection3D\n",
    "    from query.models import Video\n",
    "    \n",
    "    print(\"Processing\", vids)\n",
    "    vs = Video.objects.filter(id__in=vids)\n",
    "    return VideoIntervalCollection3D({\n",
    "        v.id: IntervalSet3D([Interval3D((0, v.num_frames-1))]) for v in vs})\n",
    "\n",
    "def run_dummy():\n",
    "    from esper.rekall_parallel import get_runtime_for_jupyter\n",
    "    from query.models import Video\n",
    "    \n",
    "    vids = [v.id for v in Video.objects.all()]\n",
    "    rt = get_runtime_for_jupyter()\n",
    "    results,_ = rt.run(dummy_func, vids, profile=True, progress=True)\n",
    "    return results\n",
    "\n",
    "display_result(convert_to_1d_collection(run_dummy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hermione in the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:05:50.034990Z",
     "start_time": "2019-02-22T01:05:40.396552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query in Runtime starts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                 | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing in workers starts.\n",
      "Dispatching tasks starts.\n",
      "Dispatching tasks ends after 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:07<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing in workers ends after 7.88 seconds\n",
      "Combining results from workers starts.\n",
      "Combining results from workers ends after 0.00 seconds\n",
      "Executing query in Runtime ends after 8.11 seconds\n",
      "Answer computed. Preparing for VGrid.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bc3e92cbcd4e438ff1a66629b115bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(jsglobals={'schema': [['Identity', ['id', 'name']], ['Genre', ['id', 'name']], ['Director', ['id',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hermione_in_the_middle_for_vid(vid):\n",
    "    from rekall.video_interval_collection_3d import VideoIntervalCollection3D\n",
    "    from rekall.interval_set_3d_utils import T,P,XY,or_preds,X\n",
    "    from rekall.temporal_predicates import before, overlaps_before\n",
    "    from rekall.bbox_predicates import height_at_least, same_value, same_height, left_of\n",
    "    from query.models import FaceCharacterActor\n",
    "    \n",
    "    MIN_FACE_HEIGHT = 0.25\n",
    "    EPSILON = 0.15\n",
    "    NAMES = [ 'ron weasley', 'hermione granger', 'harry potter' ]\n",
    "    \n",
    "    faces_with_character_actor_qs = FaceCharacterActor.objects.annotate(\n",
    "        min_frame=F('face__frame__number'),\n",
    "        max_frame=F('face__frame__number'),\n",
    "        video_id=F('face__frame__video_id'),\n",
    "        bbox_x1=F('face__bbox_x1'),\n",
    "        bbox_y1=F('face__bbox_y1'),\n",
    "        bbox_x2=F('face__bbox_x2'),\n",
    "        bbox_y2=F('face__bbox_y2'),\n",
    "        character_name=F('characteractor__character__name')\n",
    "    ).filter(face__frame__video_id=vid)\n",
    "    \n",
    "    schema = VideoIntervalCollection3D.django_bbox_default_schema()\n",
    "    schema['payload'] = 'character_name'    \n",
    "    all_faces = VideoIntervalCollection3D.from_django_qs(faces_with_character_actor_qs, schema).get_allintervals()[vid]\n",
    "    frames_with_faces = all_faces.group_by_time()\n",
    "    \n",
    "    def name_is(name):\n",
    "        return lambda f: f.payload == name\n",
    "    def start_before():\n",
    "        return or_preds(overlaps_before(), before())\n",
    "    def in_order():\n",
    "        return lambda a,b,c: start_before()(a,b) and start_before()(b,c)\n",
    "    def rev_order():\n",
    "        return lambda *args: in_order()(*args[::-1])\n",
    "    \n",
    "    pattern = [\n",
    "        ([\"harry\"], [XY(height_at_least(MIN_FACE_HEIGHT)), name_is(NAMES[2])]),\n",
    "        ([\"ron\"], [XY(height_at_least(MIN_FACE_HEIGHT)), name_is(NAMES[0])]),\n",
    "        ([\"hermione\"], [XY(height_at_least(MIN_FACE_HEIGHT)), name_is(NAMES[1])]),\n",
    "        ([\"harry\", \"ron\"], [XY(same_value('y1', epsilon=EPSILON)), XY(same_height(epsilon=EPSILON))]),\n",
    "        ([\"harry\", \"hermione\"], [XY(same_value('y1', epsilon=EPSILON)), XY(same_height(epsilon=EPSILON))]),\n",
    "        ([\"ron\", \"hermione\"], [XY(same_value('y1', epsilon=EPSILON)), XY(same_height(epsilon=EPSILON))]),\n",
    "        ([\"harry\",\"hermione\", \"ron\"], [X(or_preds(in_order(), rev_order()))])\n",
    "    ]\n",
    "    \n",
    "    def matches_pattern(pattern, exact):\n",
    "        def pred(intervals):\n",
    "            return len(intervals.match(pattern, exact))>0\n",
    "        return pred\n",
    "    \n",
    "    # Frame_IS<Face_IS<character>>\n",
    "    final = frames_with_faces.filter(P(matches_pattern(pattern, exact=True)))\n",
    "    return final\n",
    "\n",
    "def run_hp():\n",
    "    from esper.rekall_parallel import get_runtime_for_jupyter\n",
    "    from rekall.runtime import wrap_interval_set\n",
    "    from query.models import Video\n",
    "    \n",
    "    vids = [v.id for v in Video.objects.filter(name__contains=\"harry potter\")]\n",
    "    rt = get_runtime_for_jupyter(num_workers=8)\n",
    "    return rt.run(wrap_interval_set(hermione_in_the_middle_for_vid), vids, profile=True, progress=True)[0]\n",
    "\n",
    "def payload_to_vgrid_objects(faces):\n",
    "    from query.models import Character\n",
    "    def intrvl_to_obj(face):\n",
    "        return {\n",
    "            'type': 'bbox',\n",
    "            'bbox_x1': face.x[0], 'bbox_x2': face.x[1],\n",
    "            'bbox_y1': face.y[0], 'bbox_y2': face.y[1],\n",
    "            'character_id': Character.objects.get(name=face.payload).id\n",
    "        }\n",
    "    def update(acc, face):\n",
    "        acc.append(intrvl_to_obj(face))\n",
    "        return acc\n",
    "    return faces.fold(update, [])\n",
    "    \n",
    "answer = run_hp()\n",
    "print(\"Answer computed. Preparing for VGrid.\")\n",
    "display_result(convert_to_1d_collection(answer.map_payload(payload_to_vgrid_objects)), display_payload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kissing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:13:58.941841Z",
     "start_time": "2019-02-22T01:09:23.830069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query in Runtime starts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                               | 0/642 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing in workers starts.\n",
      "Dispatching tasks starts.\n",
      "Dispatching tasks ends after 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the document list and lexicon\n",
      "Loading the document list and lexicon\n",
      "Loading the document list and lexicon\n",
      "Loading the document list and lexicon\n",
      "Loading the document list and lexicon\n",
      "Loading the document list and lexicon\n",
      "Loading the document list and lexicon\n",
      "Loading the document list and lexicon\n",
      "Loading the document list and lexicon\n",
      "Loading the document list and lexicon\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "Matched 571 documents to videos\n",
      "0 documents have no videos\n",
      "71 videos have no documents\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 642/642 [04:29<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing in workers ends after 269.82 seconds\n",
      "Combining results from workers starts.\n",
      "Combining results from workers ends after 0.00 seconds\n",
      "Executing query in Runtime ends after 270.18 seconds\n",
      "Query finished. Preparing VGrid.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2600f865e7d04965bdc8b743f14aff04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(jsglobals={'schema': [['Identity', ['id', 'name']], ['Genre', ['id', 'name']], ['Director', ['id',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def kissing_for_vid(vid):\n",
    "    from query.models import Face\n",
    "    from rekall.interval_set_3d import Interval3D, IntervalSet3D\n",
    "    from rekall.video_interval_collection_3d import VideoIntervalCollection3D\n",
    "    from rekall.interval_set_3d_utils import T,P,XY,or_preds,X,Y\n",
    "    from rekall.merge_ops import payload_plus\n",
    "    from rekall.payload_predicates import payload_satisfies\n",
    "    from rekall.spatial_predicates import scene_graph\n",
    "    from rekall.temporal_predicates import overlaps, overlaps_before, before\n",
    "    from rekall.face_landmark_predicates import looking_left, looking_right\n",
    "    from rekall.bbox_predicates import height_at_least, same_height\n",
    "    import esper.face_landmarks_wrapper as flw\n",
    "    from esper.captions import get_all_segments\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    \n",
    "    MAX_MOUTH_DIFF = 0.12\n",
    "    MIN_FACE_CONFIDENCE = 0.8\n",
    "    MIN_FACE_HEIGHT = 0.4\n",
    "    MAX_FACE_HEIGHT_DIFF = 0.1\n",
    "    MIN_FACE_OVERLAP_X = 0.05\n",
    "    MIN_FACE_OVERLAP_Y = 0.2\n",
    "    MAX_FACE_OVERLAP_X_FRACTION = 0.7\n",
    "    MIN_FACE_ANGLE = 0.2\n",
    "    \n",
    "    def get_landmarks(faces):\n",
    "        ids = [face.payload for face in faces.get_intervals()]\n",
    "        landmarks = flw.get_from_face_ids(ids)\n",
    "        id_to_lm = {idx: lm for idx, lm in zip(ids, landmarks)}\n",
    "        return faces.map_payload(lambda idx : {\n",
    "            'id': idx,\n",
    "            'landmarks': id_to_lm[idx]\n",
    "        })\n",
    "    \n",
    "    def mouths_are_close(lm1, lm2):\n",
    "        select_outer=[2,3,4,8,9,10]\n",
    "        select_inner=[1,2,3,5,6,7]\n",
    "        mouth1 = np.concatenate((lm1.outer_lips()[select_outer], lm1.inner_lips()[select_inner]))\n",
    "        mouth2 = np.concatenate((lm2.outer_lips()[select_outer], lm2.inner_lips()[select_inner]))\n",
    "        mean1 = np.mean(mouth1, axis=0)\n",
    "        mean2 = np.mean(mouth2, axis=0)\n",
    "        return np.linalg.norm(mean1-mean2) <= MAX_MOUTH_DIFF\n",
    "    \n",
    "    # Line is ax+by+c=0\n",
    "    def project_point_to_line(pt, a, b, c):\n",
    "        x0,y0=pt[0], pt[1]\n",
    "        d=a*a+b*b\n",
    "        x=(b*(b*x0-a*y0)-a*c)/d\n",
    "        y=(a*(-b*x0+a*y0)-b*c)/d\n",
    "        return np.array([x,y])\n",
    "    \n",
    "    # Returns (a,b,c) which defines ax+by+c=0\n",
    "    def find_best_line_fit(xs, ys):\n",
    "        fit1 = np.polyfit(xs, ys, 1)\n",
    "        error1 = np.sum((np.poly1d(fit1)(xs)-ys)**2)\n",
    "        fit2 = np.polyfit(ys, xs, 1)\n",
    "        error2 = np.sum((np.poly1d(fit2)(ys)-xs)**2)\n",
    "        if error1 < error2:\n",
    "            # fit1[0]x+fit1[1]=y\n",
    "            return fit1[0], -1, fit1[1]\n",
    "        # fit2[0]y+fit2[1]=x\n",
    "        return -1, fit2[0], fit2[1]\n",
    "    \n",
    "    # Positive if facing left\n",
    "    def signed_face_angle(lm):\n",
    "        center_line_indices = [27,28, 32, 33,34, 51,62,66,57]\n",
    "        data = lm.landmarks[center_line_indices]\n",
    "        a, b, c = find_best_line_fit(data[:,0], data[:,1])\n",
    "        A = project_point_to_line(lm.landmarks[center_line_indices[0]], a, b, c)\n",
    "        B = project_point_to_line(lm.landmarks[center_line_indices[-1]], a, b, c)\n",
    "        AB = B-A\n",
    "        AB = AB / np.linalg.norm(AB)\n",
    "        C = np.mean(lm.nose_bridge()[2:4], axis=0)\n",
    "        AC = C-A\n",
    "        AC = AC / np.linalg.norm(AC)\n",
    "        return np.cross(AB, AC)\n",
    "\n",
    "    # Annotate face rows with start and end frames and the video ID\n",
    "    faces_qs = Face.objects.filter(\n",
    "        frame__regularly_sampled=True,\n",
    "        probability__gte=MIN_FACE_CONFIDENCE).annotate(\n",
    "        min_frame=F('frame__number'),\n",
    "        max_frame=F('frame__number'),\n",
    "        height = F('bbox_y2')-F('bbox_y1'),\n",
    "        video_id=F('frame__video_id')).filter(height__gte=MIN_FACE_HEIGHT, video_id=vid)\n",
    "    \n",
    "    # Frame_IS<Face_IS<face_id>>\n",
    "    frames_with_faces = get_set(vid, VideoIntervalCollection3D.from_django_qs(\n",
    "        faces_qs, VideoIntervalCollection3D.django_bbox_default_schema(),\n",
    "    )).group_by_time()\n",
    "    \n",
    "    overlap_faces_pattern = [\n",
    "        ([\"left\",\"right\"], [\n",
    "            X(or_preds(before(), overlaps_before())), # Left face on the left\n",
    "            X(lambda f1,f2: f1.end - f2.start > MIN_FACE_OVERLAP_X), # Faces overlap\n",
    "            Y(lambda f1,f2: min(f1.end, f2.end)-max(f1.start, f2.start) > MIN_FACE_OVERLAP_Y), # No face is entirely above another\n",
    "            XY(same_height(MAX_FACE_HEIGHT_DIFF)),\n",
    "            X(lambda f1, f2: (f1.end-f2.start)/max(f1.length(), f2.length()) < MAX_FACE_OVERLAP_X_FRACTION),\n",
    "        ])\n",
    "    ]\n",
    "    \n",
    "    def matches_pattern(pattern, exact):\n",
    "        def pred(intervals):\n",
    "            return len(intervals.match(pattern, exact))>0\n",
    "        return pred\n",
    "    \n",
    "    # Frame_IS<Face_IS<face_id>>\n",
    "    frames_with_overlapped_faces = frames_with_faces.filter(P(matches_pattern(overlap_faces_pattern, exact=True)))\n",
    "    \n",
    "    # Frame_IS<Face_IS<FaceMeta>>\n",
    "    frames_with_landmarks = frames_with_overlapped_faces.map_payload(get_landmarks)\n",
    "    opposing_face_pattern = [\n",
    "        (['left'], [P(lambda f: signed_face_angle(f['landmarks']) < -MIN_FACE_ANGLE)]),\n",
    "        (['right'], [P(lambda f: signed_face_angle(f['landmarks']) > MIN_FACE_ANGLE)]),\n",
    "        (['left','right'], [P(lambda l, r: mouths_are_close(l['landmarks'], r['landmarks']))])\n",
    "    ]\n",
    "    \n",
    "    # Frame_IS<Face_IS<FaceMeta>>\n",
    "    frames_with_opposing_faces = frames_with_landmarks.filter(P(matches_pattern(opposing_face_pattern, exact=True)))\n",
    "    \n",
    "    # Merge with shots\n",
    "    shots_qs = Shot.objects.filter(\n",
    "        video_id = vid,\n",
    "        cinematic = True,\n",
    "    )\n",
    "    # Shot_IS<>\n",
    "    shots = get_set(vid, VideoIntervalCollection3D.from_django_qs(\n",
    "        shots_qs,\n",
    "    ))\n",
    "    # Shot_IS<Frame_IS<Face_IS<FaceMeta>>>\n",
    "    kissing_shots = shots.collect_by_interval(\n",
    "        frames_with_opposing_faces,\n",
    "        T(overlaps()),\n",
    "        time_window=1,\n",
    "    ).map_payload(lambda p:p[1]).map(\n",
    "        # Take the start of the kissing as the start of the shot\n",
    "        lambda shot: Interval3D((shot.payload.get_intervals()[0].t[0], shot.t[1]), payload=shot.payload))\n",
    "    \n",
    "    # Get faces in shots\n",
    "    faces_qs2 = Face.objects.filter(\n",
    "         frame__regularly_sampled=True,\n",
    "         frame__video_id=vid,\n",
    "         probability__gte=MIN_FACE_CONFIDENCE).annotate(\n",
    "             min_frame=F('frame__number'),\n",
    "             max_frame=F('frame__number'),\n",
    "             video_id=F('frame__video_id')       \n",
    "    )\n",
    "    # Frame_IS<Face_IS>\n",
    "    frames_with_faces2 = get_set(vid, VideoIntervalCollection3D.from_django_qs(\n",
    "        faces_qs2, VideoIntervalCollection3D.django_bbox_default_schema(),\n",
    "    )).group_by_time()\n",
    "    \n",
    "    def both_faces_are_high(faces):\n",
    "        def update(result, face):\n",
    "            if face.height() < MIN_FACE_HEIGHT:\n",
    "                return False\n",
    "            return result\n",
    "        return faces.fold(update, True)\n",
    "    \n",
    "    # Frame_IS<Face_IS>\n",
    "    frames_with_two_faces = frames_with_faces2.filter(\n",
    "        P(lambda faces: faces.size()==2)).filter(P(both_faces_are_high))\n",
    "    \n",
    "    # Collect frames with two faces into kissing shots, and clips the shot to the last frame with two faces\n",
    "    def clip_to_last_frame(intrvl):\n",
    "        frames = intrvl.payload[1]\n",
    "        if frames.empty():\n",
    "            return intrvl.copy()\n",
    "        return Interval3D((intrvl.t[0], frames.get_intervals()[-1].t[1]), payload=intrvl.payload)\n",
    "    \n",
    "    # Shot_IS<(Frame_IS<Face_IS>, Frame_IS<Face_IS>)>\n",
    "    clipped_kissing_shots = kissing_shots.collect_by_interval(\n",
    "        frames_with_two_faces, T(overlaps()), time_window=1, filter_empty=False\n",
    "    ).map(clip_to_last_frame).filter_size(min_size=12)\n",
    "    \n",
    "    \n",
    "    _, words = next(get_all_segments([vid]))\n",
    "    fps = Video.objects.get(id=vid).fps\n",
    "    # Word_IS<>\n",
    "    caption_results = IntervalSet3D([Interval3D((\n",
    "            word[0] * fps, # start frame\n",
    "            word[1] * fps))\n",
    "            for word in words])\n",
    "    \n",
    "    kissing_without_words = clipped_kissing_shots.minus(caption_results)\n",
    "    kissing_final = kissing_without_words.temporal_coalesce(epsilon=0.5).map(\n",
    "        lambda i: Interval3D((int(i.t[0]), int(i.t[1])), payload=i.payload)\n",
    "    ).filter_size(min_size=12)\n",
    "    \n",
    "    return kissing_final\n",
    "\n",
    "def payload_to_vgrid_objects(payload):\n",
    "    # Frame_IS<Face_IS<FaceMeta>>\n",
    "    frames_with_opposing_overlapped_faces, _ = payload\n",
    "    def face_to_objects(face):\n",
    "        from esper.stdlib import face_landmarks_to_dict\n",
    "        return [{\n",
    "            'type': 'bbox',\n",
    "            'bbox_x1': face.x[0], 'bbox_x2': face.x[1],\n",
    "            'bbox_y1': face.y[0], 'bbox_y2': face.y[1]\n",
    "        }, face_landmarks_to_dict(face.payload['landmarks'])]\n",
    "    def update(acc, frame):\n",
    "        def accumulate_faces(a, face):\n",
    "            return a+face_to_objects(face)\n",
    "        return acc + frame.payload.fold(accumulate_faces, [])\n",
    "    return frames_with_opposing_overlapped_faces.fold(update, [])\n",
    "\n",
    "def run_kissing():\n",
    "    from esper.rekall_parallel import get_worker_pool_factory_for_jupyter, WorkerPoolWithStorageFactory\n",
    "    from rekall.runtime import Runtime, wrap_interval_set\n",
    "    from query.models import Video\n",
    "    \n",
    "    vids = [v.id for v in Video.objects.all()]\n",
    "    output_dir = \"/app/data/kissing\"\n",
    "    rt = Runtime(WorkerPoolWithStorageFactory(output_dir, get_worker_pool_factory_for_jupyter(num_workers=10)))\n",
    "    return rt.run(wrap_interval_set(kissing_for_vid), vids, profile=True, progress=True, chunksize=5, randomize=False)[0]\n",
    "\n",
    "answer = run_kissing()\n",
    "print(\"Query finished. Preparing VGrid.\")\n",
    "display_result(convert_to_1d_collection(answer.map_payload(payload_to_vgrid_objects)), display_payload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:57:40.708382Z",
     "start_time": "2019-02-22T01:57:23.286062Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                               | 0/642 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query in Runtime starts.\n",
      "Executing in workers starts.\n",
      "Dispatching tasks starts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                      | 4/642 [00:01<11:02,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatching tasks ends after 1.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 642/642 [00:16<00:00, 39.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing in workers ends after 16.29 seconds\n",
      "Combining results from workers starts.\n",
      "Combining results from workers ends after 0.00 seconds\n",
      "Executing query in Runtime ends after 16.30 seconds\n",
      "Query finished. Preparing VGrid.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354db166d5514f7c9813daf15bb16a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(jsglobals={'schema': [['Identity', ['id', 'name']], ['Genre', ['id', 'name']], ['Director', ['id',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def action_shots_for_vid(vid):\n",
    "    from query.models import Shot, Frame, Video\n",
    "    from rekall.video_interval_collection_3d import VideoIntervalCollection3D\n",
    "    from rekall.interval_set_3d import IntervalSet3D, Interval3D\n",
    "    from rekall.temporal_predicates import meets_before, overlaps, equal\n",
    "    from rekall.interval_set_3d_utils import T,P,XY,or_preds,X,Y\n",
    "    from django.db.models import ExpressionWrapper, FloatField, F\n",
    "    from esper.captions import get_all_segments\n",
    "    from rekall.merge_ops import payload_first, payload_plus, merge_named_payload\n",
    "    import numpy as np\n",
    "    \n",
    "    def get_set(vid, collection):\n",
    "        from rekall.interval_set_3d import IntervalSet3D\n",
    "        return collection.get_allintervals().get(vid, IntervalSet3D([]))\n",
    "\n",
    "    NUM_SHOTS=5\n",
    "    MAX_SHOT_DURATION=0.8\n",
    "    BRIGHTNESS_THRESHOLD = 20.0\n",
    "    MAX_NUM_WORDS_PER_SECOND = 1.0\n",
    "    \n",
    "    shots_qs = Shot.objects.annotate(\n",
    "        duration = ExpressionWrapper((F('max_frame') - F('min_frame')) / F('video__fps'), output_field=FloatField())\n",
    "    ).filter(\n",
    "        duration__lt=MAX_SHOT_DURATION,\n",
    "        duration__gt=0.,\n",
    "        cinematic=True,\n",
    "        video_id=vid,\n",
    "    )\n",
    "    \n",
    "    # Shot_IS<>\n",
    "    short_shots = get_set(vid, VideoIntervalCollection3D.from_django_qs(shots_qs))\n",
    "    \n",
    "    def get_all_frames(short_shots):\n",
    "        def update(frames, shot):\n",
    "            return frames + list(range(shot.t[0], shot.t[1]+1))\n",
    "        return short_shots.fold(update, [])\n",
    "    \n",
    "    frame_numbers = get_all_frames(short_shots)\n",
    "    frames_qs = Frame.objects.filter(video_id=vid, number__in=frame_numbers, regularly_sampled=True).order_by('number')\n",
    "    all_frames = get_set(vid, VideoIntervalCollection3D.from_django_qs(\n",
    "            frames_qs, schema={'t1':'number', 't2': 'number', 'payload': 'brightness'}))\n",
    "\n",
    "    def select_second(p):\n",
    "        return p[1]\n",
    "    \n",
    "    # Shot_IS<Frame_IS>\n",
    "    shots_with_brightness = short_shots.collect_by_interval(\n",
    "        all_frames, T(overlaps()), time_window=0, filter_empty=False\n",
    "    ).map_payload(select_second)\n",
    "    \n",
    "    # Sequence_IS<Shot_IS<Frame_IS>>\n",
    "    one_shots = shots_with_brightness.collect_by_interval(\n",
    "        shots_with_brightness, T(equal()), time_window=0).map_payload(select_second)\n",
    "    n_shots = one_shots\n",
    "    for n in range(2, NUM_SHOTS+1):\n",
    "        n_shots = n_shots.merge(\n",
    "            one_shots,\n",
    "            T(meets_before(epsilon=1)),\n",
    "            payload_merge_op = IntervalSet3D.union,            \n",
    "            time_window=1)\n",
    "        \n",
    "    def merge_shots(seq1, seq2):\n",
    "        return seq1.union(seq2.minus(seq1))\n",
    "    coalesced_n_shots = n_shots.temporal_coalesce(payload_merge_op=merge_shots)\n",
    "    \n",
    "    def bright_enough(shots):\n",
    "        # Check if any shots is above mean brightness threshold\n",
    "        def compute_avg_brightness(frames):\n",
    "            ret = frames.fold(lambda acc, f: acc+f.payload, 0)\n",
    "            if not frames.empty():\n",
    "                ret = ret / frames.size()\n",
    "            return ret\n",
    "        return shots.map_payload(compute_avg_brightness).fold(\n",
    "            lambda acc, shot: acc or shot.payload > BRIGHTNESS_THRESHOLD)\n",
    "            \n",
    "    n_bright_shots = n_shots.filter(P(bright_enough))\n",
    "    \n",
    "    _, words = next(get_all_segments([vid]))\n",
    "    fps = Video.objects.get(id=vid).fps\n",
    "    \n",
    "    # Word_IS<>\n",
    "    caption_results = IntervalSet3D([Interval3D(\n",
    "            (word[0] * fps, word[1] * fps))\n",
    "            for word in words])\n",
    "    \n",
    "    def has_few_words(seq):\n",
    "        _, words = seq.payload\n",
    "        n_words = words.size()\n",
    "        if n_words == 0:\n",
    "            return True\n",
    "        time = seq.length() / fps\n",
    "        return n_words / time <= MAX_NUM_WORDS_PER_SECOND\n",
    "    \n",
    "    # Seq_IS<(Shot_IS<Frame_IS>, Word_IS)>\n",
    "    n_bright_shots_with_few_words = n_bright_shots.collect_by_interval(\n",
    "        caption_results,\n",
    "        T(overlaps()),\n",
    "        time_window=0,\n",
    "        filter_empty=False).filter(has_few_words)\n",
    "    \n",
    "    # Seq_IS<Shot_IS<Frame_IS>>\n",
    "    action_shots = coalesced_n_shots.filter_against(\n",
    "        n_bright_shots_with_few_words,\n",
    "        T(overlaps()),\n",
    "        time_window=0)\n",
    "    \n",
    "    return action_shots\n",
    "\n",
    "def run_action_shots():\n",
    "    import ipyparallel as ipp\n",
    "    from esper.rekall_parallel import get_runtime_for_ipython_cluster, get_runtime_for_jupyter\n",
    "    from rekall.runtime import wrap_interval_set\n",
    "    from query.models import Video\n",
    "    \n",
    "    vids = [v.id for v in Video.objects.all()]\n",
    "    rt = get_runtime_for_ipython_cluster(ipp.Client(profile=\"local\"))\n",
    "    return rt.run(wrap_interval_set(action_shots_for_vid), vids, profile=True, progress=True)[0]\n",
    "\n",
    "answer = run_action_shots()\n",
    "print(\"Query finished. Preparing VGrid.\")\n",
    "display_result(convert_to_1d_collection(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversations with Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:21:49.577298Z",
     "start_time": "2019-02-22T01:15:21.395659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query in Runtime starts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                               | 0/642 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing in workers starts.\n",
      "Dispatching tasks starts.\n",
      "Dispatching tasks ends after 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 642/642 [06:25<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing in workers ends after 385.23 seconds\n",
      "Combining results from workers starts.\n",
      "Combining results from workers ends after 0.00 seconds\n",
      "Executing query in Runtime ends after 385.84 seconds\n",
      "Query finished. Preparing VGrid.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dabe15bea74db4a1ef9f0758d39509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(jsglobals={'schema': [['Identity', ['id', 'name']], ['Genre', ['id', 'name']], ['Director', ['id',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_conversations():\n",
    "    from esper.rekall_parallel import get_runtime_for_jupyter\n",
    "    from query.models import Video, FaceCharacterActor\n",
    "    \n",
    "    vids = [v.id for v in Video.objects.all()]\n",
    "    rt = get_runtime_for_jupyter()\n",
    "    def query(vids):\n",
    "        return conversationsq(vids, progress=False)\n",
    "    return rt.run(query, vids, profile=True, progress=True, chunksize=10)[0]\n",
    "\n",
    "answer = run_conversations()\n",
    "print(\"Query finished. Preparing VGrid.\")\n",
    "display_result(convert_to_1d_collection(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShotScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:32:30.091666Z",
     "start_time": "2019-02-22T01:28:50.660666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query in Runtime starts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing in workers starts.\n",
      "Dispatching tasks starts.\n",
      "Dispatching tasks ends after 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:07<00:00, 10.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing in workers ends after 127.37 seconds\n",
      "Combining results from workers starts.\n",
      "Combining results from workers ends after 0.00 seconds\n",
      "Executing query in Runtime ends after 128.71 seconds\n",
      "Query finished. Preparing VGrid.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f9d3b22039441c9d1079141338cf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(jsglobals={'schema': [['Identity', ['id', 'name']], ['Genre', ['id', 'name']], ['Director', ['id',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_shot_scale():\n",
    "    from esper.rekall_parallel import get_runtime_for_jupyter\n",
    "    from query.models import Video\n",
    "    \n",
    "    vids = [v.id for v in Video.objects.filter(id__lte=10)]\n",
    "    rt = get_runtime_for_jupyter()\n",
    "    def query(vids):\n",
    "        return shot_scale_q(vids, progress=False)\n",
    "    return rt.run(query, vids, profile=True, progress=True, chunksize=1)[0]\n",
    "\n",
    "def payload_to_vgrid_objects(payload):\n",
    "    from rekall.interval_set_3d_utils import P\n",
    "    # Frame_IS<(Scale, Face_IS, Pose_IS)>\n",
    "    scale, frames = payload\n",
    "    def face_to_object(face):\n",
    "        return {\n",
    "            'type': 'bbox',\n",
    "            'bbox_x1': face.x[0], 'bbox_x2': face.x[1],\n",
    "            'bbox_y1': face.y[0], 'bbox_y2': face.y[1]\n",
    "        }\n",
    "    \n",
    "    def pose_to_object(pose):\n",
    "        from esper.stdlib import pose_to_dict\n",
    "        return pose_to_dict(pose.payload['pose'])\n",
    "    \n",
    "    def face_objects_at_scale(faces, scale):\n",
    "        faces = faces.filter(P(lambda p:p==scale))\n",
    "        def update(acc, face):\n",
    "            acc.append(face_to_object(face))\n",
    "            return acc\n",
    "        return faces.fold(update, [])\n",
    "    \n",
    "    def pose_objects_at_scale(poses, scale):\n",
    "        poses = poses.filter(P(lambda p:p['scale']==scale))\n",
    "        def update(acc, pose):\n",
    "            acc.append(pose_to_object(pose))\n",
    "            return acc\n",
    "        return poses.fold(update, [])\n",
    "    \n",
    "    frames = frames.filter(P(lambda p:p[0]==scale))\n",
    "    def update(acc, frame):\n",
    "        _, faces, poses = frame.payload\n",
    "        return acc + face_objects_at_scale(faces, scale) + pose_objects_at_scale(poses, scale)\n",
    "    return frames.fold(update, [])\n",
    "\n",
    "answer = run_shot_scale()\n",
    "print(\"Query finished. Preparing VGrid.\")\n",
    "display_result(convert_to_1d_collection(answer.map_payload(payload_to_vgrid_objects)), display_payload=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T00:24:01.993498Z",
     "start_time": "2019-02-10T00:24:01.953664Z"
    }
   },
   "outputs": [],
   "source": [
    "answer.filter(lambda i:i.t[0]==120401).get_allintervals()[380].get_intervals()[0].payload.map_payload(lambda char: char.fold(lambda acc, c: acc+[c.payload[0]], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T19:39:36.083555Z",
     "start_time": "2019-02-11T19:39:36.051577Z"
    }
   },
   "outputs": [],
   "source": [
    "answer.get_allintervals()[32].get_intervals()[0].payload[0].get_intervals()[0].payload.get_intervals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T22:33:45.964208Z",
     "start_time": "2019-02-11T22:33:19.090364Z"
    }
   },
   "outputs": [],
   "source": [
    "def payload_to_vgrid_objects(payload):\n",
    "    from rekall.interval_set_3d_utils import P\n",
    "    # Frame_IS<(Scale, Face_IS, Pose_IS)>\n",
    "    scale, frames = payload\n",
    "    def face_to_object(face):\n",
    "        return {\n",
    "            'type': 'bbox',\n",
    "            'bbox_x1': face.x[0], 'bbox_x2': face.x[1],\n",
    "            'bbox_y1': face.y[0], 'bbox_y2': face.y[1]\n",
    "        }\n",
    "    \n",
    "    def pose_to_object(pose):\n",
    "        from esper.stdlib import pose_to_dict\n",
    "        return pose_to_dict(pose.payload['pose'])\n",
    "    \n",
    "    def face_objects_at_scale(faces, scale):\n",
    "        faces = faces.filter(P(lambda p:p==scale))\n",
    "        def update(acc, face):\n",
    "            acc.append(face_to_object(face))\n",
    "            return acc\n",
    "        return faces.fold(update, [])\n",
    "    \n",
    "    def pose_objects_at_scale(poses, scale):\n",
    "        poses = poses.filter(P(lambda p:p['scale']==scale))\n",
    "        def update(acc, pose):\n",
    "            acc.append(pose_to_object(pose))\n",
    "            return acc\n",
    "        return poses.fold(update, [])\n",
    "    \n",
    "    frames = frames.filter(P(lambda p:p[0]==scale))\n",
    "    def update(acc, frame):\n",
    "        _, faces, poses = frame.payload\n",
    "        return acc + face_objects_at_scale(faces, scale) + pose_objects_at_scale(poses, scale)\n",
    "    return frames.fold(update, [])\n",
    "\n",
    "display_result(convert_to_1d_collection(answer.map_payload(payload_to_vgrid_objects)), display_payload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T05:42:15.510233Z",
     "start_time": "2019-02-14T05:42:15.436150Z"
    }
   },
   "outputs": [],
   "source": [
    "qs=Face.objects.all()[:3]\n",
    "[q.id for q in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T05:33:35.694562Z",
     "start_time": "2019-02-14T05:33:35.497049Z"
    }
   },
   "outputs": [],
   "source": [
    "import esper.face_landmarks_wrapper as flw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T05:33:55.830761Z",
     "start_time": "2019-02-14T05:33:55.797716Z"
    }
   },
   "outputs": [],
   "source": [
    "flw.get_from_face_ids([3,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T07:36:54.585992Z",
     "start_time": "2019-02-14T07:36:39.400298Z"
    }
   },
   "outputs": [],
   "source": [
    "from django.db.models import Count\n",
    "from query.models import Face\n",
    "Face.objects.values(\"frame__video_id\").annotate(num_faces=Count('id')).order_by('num_faces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T21:50:08.195355Z",
     "start_time": "2019-02-19T21:50:08.155611Z"
    }
   },
   "outputs": [],
   "source": [
    "from query.models import Frame, Video\n",
    "v = Video.objects.get(id=1)\n",
    "Frame.objects.filter(video=v, number=v.num_frames-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T01:46:42.160154Z",
     "start_time": "2019-02-20T01:46:41.064610Z"
    }
   },
   "outputs": [],
   "source": [
    "from esper.shot_scale import ShotScale as ShotScaleEnum\n",
    "from rekall.interval_set_3d_utils import P\n",
    "display_result(convert_to_1d_collection(answer.filter(P(lambda p: p==ShotScaleEnum.EXTREME_LONG))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T08:42:24.161519Z",
     "start_time": "2019-02-21T08:37:46.756154Z"
    }
   },
   "outputs": [],
   "source": [
    "display_result(convert_to_1d_collection(answer.map_payload(payload_to_vgrid_objects, profile=True, parallel=True)), display_payload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "1219px",
    "left": "0px",
    "right": "2348px",
    "top": "110px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
