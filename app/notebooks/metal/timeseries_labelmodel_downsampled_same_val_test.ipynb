{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch, pickle, csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "sys.path.append('/lfs/1/danfu/metal')\n",
    "sys.path.append('/lfs/1/danfu/sequential_ws')\n",
    "from metal.metrics import metric_score\n",
    "from torch.nn.functional import normalize\n",
    "from DP.label_model import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_train_100_windows_downsampled.npz'\n",
    "L_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_val_windows_downsampled_same_val_test.npz'\n",
    "Y_dev_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_val_windows_downsampled_same_val_test.npy'\n",
    "L_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/L_test_windows_downsampled_same_val_test.npz'\n",
    "Y_test_path = '/lfs/1/danfu/esper/app/data/shot_detection_weak_labels/Y_test_windows_downsampled_same_val_test.npy'\n",
    "\n",
    "stride = 1\n",
    "L_train_raw = sp.sparse.load_npz(L_train_path).todense()[::stride]\n",
    "L_dev_raw = sp.sparse.load_npz(L_dev_path).todense()\n",
    "Y_dev_raw = np.load(Y_dev_path)\n",
    "L_test_raw = sp.sparse.load_npz(L_test_path).todense()\n",
    "Y_test_raw = np.load(Y_test_path)\n",
    "\n",
    "T = 5\n",
    "\n",
    "L_train = torch.FloatTensor(L_train_raw[:L_train_raw.shape[0] - (L_train_raw.shape[0] % T)]).to(device)\n",
    "L_dev = torch.FloatTensor(L_dev_raw[:L_dev_raw.shape[0] - (L_dev_raw.shape[0] % T)]).to(device)\n",
    "Y_dev = torch.FloatTensor(Y_dev_raw[:Y_dev_raw.shape[0] - (Y_dev_raw.shape[0] % T)]).to(device)\n",
    "L_test = torch.FloatTensor(L_test_raw[:L_test_raw.shape[0] - (L_test_raw.shape[0] % T)]).to(device)\n",
    "Y_test = torch.FloatTensor(Y_test_raw[:Y_test_raw.shape[0] - (Y_test_raw.shape[0] % T)]).to(device)\n",
    "m_per_task = L_train.size(1)\n",
    "n_frames_train = L_train.size(0)\n",
    "n_patients_train = n_frames_train//T\n",
    "n_frames_dev = L_dev.size(0)\n",
    "n_patients_dev = n_frames_dev//T\n",
    "n_frames_test = L_test.size(0)\n",
    "n_patients_test = n_frames_test//T\n",
    "\n",
    "# MRI_data_naive = {'Li_train': (L_train.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'Li_dev': (L_dev.unsqueeze(2) == torch.FloatTensor([-1,1,0]).to(device).unsqueeze(0).unsqueeze(0)).argmax(2),\n",
    "#                   'R_dev': (Y_dev.unsqueeze(1) == torch.FloatTensor([-1,1]).to(device).unsqueeze(0)).argmax(1),\n",
    "#                   'm':m_per_task, 'T':1,\n",
    "#                  }\n",
    "\n",
    "# don't need to transform the raw data\n",
    "MRI_data_naive = {'Li_train': L_train.long().to(device),\n",
    "                  'Li_dev': L_dev.long().to(device),\n",
    "                  'R_dev': Y_dev.long().to(device),\n",
    "                  'Li_test': L_test.long().to(device),\n",
    "                  'R_test': Y_test.long().to(device),\n",
    "                  'm':m_per_task, 'T':1,\n",
    "                 }\n",
    "MRI_data_naive['class_balance'] = normalize((MRI_data_naive['R_dev'].unsqueeze(1)==torch.arange(2, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                            dim=0, p=1)\n",
    "MRI_data_temporal = {'Li_train': MRI_data_naive['Li_train'].view(n_patients_train, (m_per_task*T)),\n",
    "                     'Li_dev': MRI_data_naive['Li_dev'].view(n_patients_dev, (m_per_task*T)),\n",
    "                     'R_dev': MRI_data_naive['R_dev']*(2**T-1),\n",
    "                     'Li_test': MRI_data_naive['Li_test'].view(n_patients_test, (m_per_task*T)),\n",
    "                     'R_test': MRI_data_naive['R_test']*(2**T-1),\n",
    "                     'm': m_per_task * T, 'T': T,\n",
    "                    } \n",
    "MRI_data_temporal['class_balance'] = normalize((MRI_data_temporal['R_dev'].unsqueeze(1)==torch.arange(2**T, device=device).unsqueeze(0)).sum(0).float(), \n",
    "                                                dim=0, p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MRI_data_naive['class_balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0 loss=50.6702766418457\n",
      "iteration=300 loss=8.80715274810791\n",
      "iteration=600 loss=3.66485595703125\n",
      "iteration=900 loss=1.8281009197235107\n",
      "iteration=1200 loss=0.9653668999671936\n",
      "iteration=1500 loss=0.6120548844337463\n",
      "iteration=1800 loss=0.4767417907714844\n",
      "iteration=2100 loss=0.40450260043144226\n",
      "iteration=2400 loss=0.3607012629508972\n",
      "iteration=2700 loss=0.3310929536819458\n",
      "iteration=2999 loss=0.30959445238113403\n"
     ]
    }
   ],
   "source": [
    "naive_model = DPLabelModel(m=m_per_task, \n",
    "                           T=1,\n",
    "                           edges=[],\n",
    "                           coverage_sets=[[0,]]*m_per_task,\n",
    "                           mu_sharing=[[i,] for i in range(m_per_task)],\n",
    "                           phi_sharing=[],\n",
    "                           device=device,\n",
    "                           # class_balance=MRI_data_naive['class_balance'], \n",
    "                           seed=0)\n",
    "optimize(naive_model, L_hat=MRI_data_naive['Li_train'], num_iter=3000, lr=4.087885261759692e-05,\n",
    "         momentum=0.9, clamp=True, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.125\n",
      "F1: 0.389\n",
      "Recall: 0.712\n",
      "Precision: 0.268\n"
     ]
    }
   ],
   "source": [
    "R_pred = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipping Parameters\n",
      "Accuracy: 0.051\n",
      "F1: 0.143\n",
      "Recall: 0.288\n",
      "Precision: 0.095\n"
     ]
    }
   ],
   "source": [
    "# Flipping params\n",
    "print(\"Flipping Parameters\")\n",
    "naive_model.flip_params()\n",
    "R_pred_flipped = naive_model.predict(MRI_data_naive['Li_dev'])\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(MRI_data_naive['R_dev'].cpu(), R_pred_flipped.cpu(), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b37ee3d16836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalid_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m feasible_y = np.array([[-1, -1, -1, -1, -1],\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_dev' is not defined"
     ]
    }
   ],
   "source": [
    "valid_target = Y_dev.long()\n",
    "T = 5\n",
    "\n",
    "feasible_y = np.array([[-1, -1, -1, -1, -1],\n",
    "        [-1,  1, -1, -1, -1],\n",
    "        [ 1, -1, -1, -1, -1],\n",
    "        [ 1,  1, -1, -1, -1],\n",
    "        [-1, -1,  1, -1, -1],\n",
    "        [-1,  1,  1, -1, -1],\n",
    "        [ 1, -1,  1, -1, -1],\n",
    "        [ 1,  1,  1, -1, -1],\n",
    "        [-1, -1, -1,  1, -1],\n",
    "        [-1,  1, -1,  1, -1],\n",
    "        [ 1, -1, -1,  1, -1],\n",
    "        [ 1,  1, -1,  1, -1],\n",
    "        [-1, -1,  1,  1, -1],\n",
    "        [-1,  1,  1,  1, -1],\n",
    "        [ 1, -1,  1,  1, -1],\n",
    "        [ 1,  1,  1,  1, -1],\n",
    "        [-1, -1, -1, -1,  1],\n",
    "        [-1,  1, -1, -1,  1],\n",
    "        [ 1, -1, -1, -1,  1],\n",
    "        [ 1,  1, -1, -1,  1],\n",
    "        [-1, -1,  1, -1,  1],\n",
    "        [-1,  1,  1, -1,  1],\n",
    "        [ 1, -1,  1, -1,  1],\n",
    "        [ 1,  1,  1, -1,  1],\n",
    "        [-1, -1, -1,  1,  1],\n",
    "        [-1,  1, -1,  1,  1],\n",
    "        [ 1, -1, -1,  1,  1],\n",
    "        [ 1,  1, -1,  1,  1],\n",
    "        [-1, -1,  1,  1,  1],\n",
    "        [-1,  1,  1,  1,  1],\n",
    "        [ 1, -1,  1,  1,  1],\n",
    "        [ 1,  1,  1,  1,  1]])\n",
    "\n",
    "feasible_y[feasible_y==-1] = 0\n",
    "feasible_y = feasible_y.tolist()\n",
    "possibilities = list(map(lambda l : ''.join(map(str,l)), feasible_y))\n",
    "\n",
    "class_balance = np.empty(2 ** T)\n",
    "#compute class balance from dev set and use laplace smoothing\n",
    "\n",
    "valid_target_copy = np.copy(valid_target)\n",
    "valid_target_copy[valid_target_copy == 2] = 0\n",
    "\n",
    "assert len(valid_target_copy) % T == 0\n",
    "num_windows = len(valid_target_copy) / T\n",
    "\n",
    "freq = {}\n",
    "for i in range(0, len(valid_target_copy), T):\n",
    "    s = ''.join(map(str,valid_target_copy[i:i+T]))\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "for i in range(len(class_balance)):\n",
    "    if possibilities[i] in freq and freq[possibilities[i]] > 5:\n",
    "        class_balance[i] = (freq[possibilities[i]] + 1) / (num_windows + len(possibilities))\n",
    "    else:\n",
    "        class_balance[i] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0001\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.057\n",
      "F1: 0.443\n",
      "Recall: 0.299\n",
      "Precision: 0.851\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.003\n",
      "Precision: 0.012\n",
      "\n",
      "Accuracy: 0.029\n",
      "F1: 0.233\n",
      "Recall: 0.151\n",
      "Precision: 0.510\n",
      "Accuracy: 0.036\n",
      "F1: 0.202\n",
      "Recall: 0.189\n",
      "Precision: 0.217\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.379\n",
      "Recall: 0.384\n",
      "Precision: 0.375\n",
      "Accuracy: 0.010\n",
      "F1: 0.072\n",
      "Recall: 0.052\n",
      "Precision: 0.115\n",
      "\n",
      "Accuracy: 0.075\n",
      "F1: 0.532\n",
      "Recall: 0.395\n",
      "Precision: 0.814\n",
      "Accuracy: 0.017\n",
      "F1: 0.070\n",
      "Recall: 0.090\n",
      "Precision: 0.057\n",
      "\n",
      "Accuracy: 0.052\n",
      "F1: 0.363\n",
      "Recall: 0.276\n",
      "Precision: 0.528\n",
      "Accuracy: 0.002\n",
      "F1: 0.023\n",
      "Recall: 0.012\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.033\n",
      "Recall: 0.035\n",
      "Precision: 0.031\n",
      "Accuracy: 0.127\n",
      "F1: 0.762\n",
      "Recall: 0.669\n",
      "Precision: 0.885\n",
      "\n",
      "Accuracy: 0.009\n",
      "F1: 0.050\n",
      "Recall: 0.049\n",
      "Precision: 0.050\n",
      "Accuracy: 0.139\n",
      "F1: 0.775\n",
      "Recall: 0.733\n",
      "Precision: 0.824\n",
      "\n",
      "Accuracy: 0.012\n",
      "F1: 0.046\n",
      "Recall: 0.061\n",
      "Precision: 0.037\n",
      "Accuracy: 0.053\n",
      "F1: 0.434\n",
      "Recall: 0.282\n",
      "Precision: 0.942\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.010\n",
      "Recall: 0.012\n",
      "Precision: 0.009\n",
      "Accuracy: 0.130\n",
      "F1: 0.716\n",
      "Recall: 0.686\n",
      "Precision: 0.749\n",
      "\n",
      "Accuracy: 0.077\n",
      "F1: 0.526\n",
      "Recall: 0.404\n",
      "Precision: 0.751\n",
      "Accuracy: 0.016\n",
      "F1: 0.081\n",
      "Recall: 0.084\n",
      "Precision: 0.078\n",
      "\n",
      "1 1e-05\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.057\n",
      "F1: 0.443\n",
      "Recall: 0.299\n",
      "Precision: 0.851\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.003\n",
      "Precision: 0.012\n",
      "\n",
      "Accuracy: 0.029\n",
      "F1: 0.233\n",
      "Recall: 0.151\n",
      "Precision: 0.510\n",
      "Accuracy: 0.036\n",
      "F1: 0.202\n",
      "Recall: 0.189\n",
      "Precision: 0.217\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.379\n",
      "Recall: 0.384\n",
      "Precision: 0.375\n",
      "Accuracy: 0.010\n",
      "F1: 0.072\n",
      "Recall: 0.052\n",
      "Precision: 0.115\n",
      "\n",
      "Accuracy: 0.075\n",
      "F1: 0.532\n",
      "Recall: 0.395\n",
      "Precision: 0.814\n",
      "Accuracy: 0.017\n",
      "F1: 0.070\n",
      "Recall: 0.090\n",
      "Precision: 0.057\n",
      "\n",
      "Accuracy: 0.052\n",
      "F1: 0.363\n",
      "Recall: 0.276\n",
      "Precision: 0.528\n",
      "Accuracy: 0.002\n",
      "F1: 0.023\n",
      "Recall: 0.012\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.033\n",
      "Recall: 0.035\n",
      "Precision: 0.031\n",
      "Accuracy: 0.127\n",
      "F1: 0.762\n",
      "Recall: 0.669\n",
      "Precision: 0.885\n",
      "\n",
      "Accuracy: 0.009\n",
      "F1: 0.050\n",
      "Recall: 0.049\n",
      "Precision: 0.050\n",
      "Accuracy: 0.139\n",
      "F1: 0.775\n",
      "Recall: 0.733\n",
      "Precision: 0.824\n",
      "\n",
      "Accuracy: 0.012\n",
      "F1: 0.046\n",
      "Recall: 0.061\n",
      "Precision: 0.037\n",
      "Accuracy: 0.053\n",
      "F1: 0.434\n",
      "Recall: 0.282\n",
      "Precision: 0.942\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.010\n",
      "Recall: 0.012\n",
      "Precision: 0.009\n",
      "Accuracy: 0.130\n",
      "F1: 0.716\n",
      "Recall: 0.686\n",
      "Precision: 0.749\n",
      "\n",
      "Accuracy: 0.077\n",
      "F1: 0.526\n",
      "Recall: 0.404\n",
      "Precision: 0.751\n",
      "Accuracy: 0.016\n",
      "F1: 0.081\n",
      "Recall: 0.084\n",
      "Precision: 0.078\n",
      "\n",
      "1 1e-06\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.057\n",
      "F1: 0.443\n",
      "Recall: 0.299\n",
      "Precision: 0.851\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.003\n",
      "Precision: 0.012\n",
      "\n",
      "Accuracy: 0.029\n",
      "F1: 0.233\n",
      "Recall: 0.151\n",
      "Precision: 0.510\n",
      "Accuracy: 0.036\n",
      "F1: 0.202\n",
      "Recall: 0.189\n",
      "Precision: 0.217\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.379\n",
      "Recall: 0.384\n",
      "Precision: 0.375\n",
      "Accuracy: 0.010\n",
      "F1: 0.072\n",
      "Recall: 0.052\n",
      "Precision: 0.115\n",
      "\n",
      "Accuracy: 0.075\n",
      "F1: 0.532\n",
      "Recall: 0.395\n",
      "Precision: 0.814\n",
      "Accuracy: 0.017\n",
      "F1: 0.070\n",
      "Recall: 0.090\n",
      "Precision: 0.057\n",
      "\n",
      "Accuracy: 0.052\n",
      "F1: 0.363\n",
      "Recall: 0.276\n",
      "Precision: 0.528\n",
      "Accuracy: 0.002\n",
      "F1: 0.023\n",
      "Recall: 0.012\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.033\n",
      "Recall: 0.035\n",
      "Precision: 0.031\n",
      "Accuracy: 0.127\n",
      "F1: 0.762\n",
      "Recall: 0.669\n",
      "Precision: 0.885\n",
      "\n",
      "Accuracy: 0.009\n",
      "F1: 0.050\n",
      "Recall: 0.049\n",
      "Precision: 0.050\n",
      "Accuracy: 0.139\n",
      "F1: 0.775\n",
      "Recall: 0.733\n",
      "Precision: 0.824\n",
      "\n",
      "Accuracy: 0.012\n",
      "F1: 0.046\n",
      "Recall: 0.061\n",
      "Precision: 0.037\n",
      "Accuracy: 0.053\n",
      "F1: 0.434\n",
      "Recall: 0.282\n",
      "Precision: 0.942\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.010\n",
      "Recall: 0.012\n",
      "Precision: 0.009\n",
      "Accuracy: 0.130\n",
      "F1: 0.716\n",
      "Recall: 0.686\n",
      "Precision: 0.749\n",
      "\n",
      "Accuracy: 0.077\n",
      "F1: 0.526\n",
      "Recall: 0.404\n",
      "Precision: 0.751\n",
      "Accuracy: 0.016\n",
      "F1: 0.081\n",
      "Recall: 0.084\n",
      "Precision: 0.078\n",
      "\n",
      "1 1e-07\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.057\n",
      "F1: 0.443\n",
      "Recall: 0.299\n",
      "Precision: 0.851\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.003\n",
      "Precision: 0.012\n",
      "\n",
      "Accuracy: 0.029\n",
      "F1: 0.233\n",
      "Recall: 0.151\n",
      "Precision: 0.510\n",
      "Accuracy: 0.036\n",
      "F1: 0.202\n",
      "Recall: 0.189\n",
      "Precision: 0.217\n",
      "\n",
      "Accuracy: 0.073\n",
      "F1: 0.379\n",
      "Recall: 0.384\n",
      "Precision: 0.375\n",
      "Accuracy: 0.010\n",
      "F1: 0.072\n",
      "Recall: 0.052\n",
      "Precision: 0.115\n",
      "\n",
      "Accuracy: 0.075\n",
      "F1: 0.532\n",
      "Recall: 0.395\n",
      "Precision: 0.814\n",
      "Accuracy: 0.017\n",
      "F1: 0.070\n",
      "Recall: 0.090\n",
      "Precision: 0.057\n",
      "\n",
      "Accuracy: 0.052\n",
      "F1: 0.363\n",
      "Recall: 0.276\n",
      "Precision: 0.528\n",
      "Accuracy: 0.002\n",
      "F1: 0.023\n",
      "Recall: 0.012\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.007\n",
      "F1: 0.033\n",
      "Recall: 0.035\n",
      "Precision: 0.031\n",
      "Accuracy: 0.127\n",
      "F1: 0.762\n",
      "Recall: 0.669\n",
      "Precision: 0.885\n",
      "\n",
      "Accuracy: 0.009\n",
      "F1: 0.050\n",
      "Recall: 0.049\n",
      "Precision: 0.050\n",
      "Accuracy: 0.139\n",
      "F1: 0.775\n",
      "Recall: 0.733\n",
      "Precision: 0.824\n",
      "\n",
      "Accuracy: 0.012\n",
      "F1: 0.046\n",
      "Recall: 0.061\n",
      "Precision: 0.037\n",
      "Accuracy: 0.053\n",
      "F1: 0.434\n",
      "Recall: 0.282\n",
      "Precision: 0.942\n",
      "\n",
      "Accuracy: 0.002\n",
      "F1: 0.010\n",
      "Recall: 0.012\n",
      "Precision: 0.009\n",
      "Accuracy: 0.130\n",
      "F1: 0.716\n",
      "Recall: 0.686\n",
      "Precision: 0.749\n",
      "\n",
      "Accuracy: 0.077\n",
      "F1: 0.526\n",
      "Recall: 0.404\n",
      "Precision: 0.751\n",
      "Accuracy: 0.016\n",
      "F1: 0.081\n",
      "Recall: 0.084\n",
      "Precision: 0.078\n",
      "\n",
      "5 0.0001\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.071\n",
      "F1: 0.510\n",
      "Recall: 0.372\n",
      "Precision: 0.810\n",
      "Accuracy: 0.005\n",
      "F1: 0.036\n",
      "Recall: 0.026\n",
      "Precision: 0.056\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.678\n",
      "Recall: 0.567\n",
      "Precision: 0.844\n",
      "Accuracy: 0.022\n",
      "F1: 0.121\n",
      "Recall: 0.116\n",
      "Precision: 0.127\n",
      "\n",
      "Accuracy: 0.066\n",
      "F1: 0.338\n",
      "Recall: 0.349\n",
      "Precision: 0.328\n",
      "Accuracy: 0.021\n",
      "F1: 0.139\n",
      "Recall: 0.113\n",
      "Precision: 0.181\n",
      "\n",
      "Accuracy: 0.099\n",
      "F1: 0.647\n",
      "Recall: 0.520\n",
      "Precision: 0.856\n",
      "Accuracy: 0.015\n",
      "F1: 0.060\n",
      "Recall: 0.078\n",
      "Precision: 0.048\n",
      "\n",
      "Accuracy: 0.069\n",
      "F1: 0.508\n",
      "Recall: 0.363\n",
      "Precision: 0.845\n",
      "Accuracy: 0.001\n",
      "F1: 0.010\n",
      "Recall: 0.006\n",
      "Precision: 0.029\n",
      "\n",
      "Accuracy: 0.010\n",
      "F1: 0.078\n",
      "Recall: 0.052\n",
      "Precision: 0.151\n",
      "Accuracy: 0.088\n",
      "F1: 0.587\n",
      "Recall: 0.462\n",
      "Precision: 0.803\n",
      "\n",
      "Accuracy: 0.008\n",
      "F1: 0.049\n",
      "Recall: 0.044\n",
      "Precision: 0.056\n",
      "Accuracy: 0.116\n",
      "F1: 0.588\n",
      "Recall: 0.613\n",
      "Precision: 0.564\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.430\n",
      "Recall: 0.465\n",
      "Precision: 0.400\n",
      "Accuracy: 0.008\n",
      "F1: 0.080\n",
      "Recall: 0.044\n",
      "Precision: 0.455\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.031\n",
      "Recall: 0.026\n",
      "Precision: 0.038\n",
      "Accuracy: 0.107\n",
      "F1: 0.593\n",
      "Recall: 0.567\n",
      "Precision: 0.621\n",
      "\n",
      "Accuracy: 0.143\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n",
      "Accuracy: 0.009\n",
      "F1: 0.036\n",
      "Recall: 0.047\n",
      "Precision: 0.029\n",
      "\n",
      "5 1e-05\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.071\n",
      "F1: 0.510\n",
      "Recall: 0.372\n",
      "Precision: 0.810\n",
      "Accuracy: 0.005\n",
      "F1: 0.036\n",
      "Recall: 0.026\n",
      "Precision: 0.056\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.678\n",
      "Recall: 0.567\n",
      "Precision: 0.844\n",
      "Accuracy: 0.022\n",
      "F1: 0.121\n",
      "Recall: 0.116\n",
      "Precision: 0.127\n",
      "\n",
      "Accuracy: 0.066\n",
      "F1: 0.338\n",
      "Recall: 0.349\n",
      "Precision: 0.328\n",
      "Accuracy: 0.021\n",
      "F1: 0.139\n",
      "Recall: 0.113\n",
      "Precision: 0.181\n",
      "\n",
      "Accuracy: 0.099\n",
      "F1: 0.647\n",
      "Recall: 0.520\n",
      "Precision: 0.856\n",
      "Accuracy: 0.015\n",
      "F1: 0.060\n",
      "Recall: 0.078\n",
      "Precision: 0.048\n",
      "\n",
      "Accuracy: 0.069\n",
      "F1: 0.508\n",
      "Recall: 0.363\n",
      "Precision: 0.845\n",
      "Accuracy: 0.001\n",
      "F1: 0.010\n",
      "Recall: 0.006\n",
      "Precision: 0.029\n",
      "\n",
      "Accuracy: 0.010\n",
      "F1: 0.078\n",
      "Recall: 0.052\n",
      "Precision: 0.151\n",
      "Accuracy: 0.088\n",
      "F1: 0.587\n",
      "Recall: 0.462\n",
      "Precision: 0.803\n",
      "\n",
      "Accuracy: 0.008\n",
      "F1: 0.049\n",
      "Recall: 0.044\n",
      "Precision: 0.056\n",
      "Accuracy: 0.116\n",
      "F1: 0.588\n",
      "Recall: 0.613\n",
      "Precision: 0.564\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.430\n",
      "Recall: 0.465\n",
      "Precision: 0.400\n",
      "Accuracy: 0.008\n",
      "F1: 0.080\n",
      "Recall: 0.044\n",
      "Precision: 0.455\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.031\n",
      "Recall: 0.026\n",
      "Precision: 0.038\n",
      "Accuracy: 0.107\n",
      "F1: 0.593\n",
      "Recall: 0.567\n",
      "Precision: 0.621\n",
      "\n",
      "Accuracy: 0.143\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n",
      "Accuracy: 0.009\n",
      "F1: 0.036\n",
      "Recall: 0.047\n",
      "Precision: 0.029\n",
      "\n",
      "5 1e-06\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.071\n",
      "F1: 0.510\n",
      "Recall: 0.372\n",
      "Precision: 0.810\n",
      "Accuracy: 0.005\n",
      "F1: 0.036\n",
      "Recall: 0.026\n",
      "Precision: 0.056\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.678\n",
      "Recall: 0.567\n",
      "Precision: 0.844\n",
      "Accuracy: 0.022\n",
      "F1: 0.121\n",
      "Recall: 0.116\n",
      "Precision: 0.127\n",
      "\n",
      "Accuracy: 0.066\n",
      "F1: 0.338\n",
      "Recall: 0.349\n",
      "Precision: 0.328\n",
      "Accuracy: 0.021\n",
      "F1: 0.139\n",
      "Recall: 0.113\n",
      "Precision: 0.181\n",
      "\n",
      "Accuracy: 0.099\n",
      "F1: 0.647\n",
      "Recall: 0.520\n",
      "Precision: 0.856\n",
      "Accuracy: 0.015\n",
      "F1: 0.060\n",
      "Recall: 0.078\n",
      "Precision: 0.048\n",
      "\n",
      "Accuracy: 0.069\n",
      "F1: 0.508\n",
      "Recall: 0.363\n",
      "Precision: 0.845\n",
      "Accuracy: 0.001\n",
      "F1: 0.010\n",
      "Recall: 0.006\n",
      "Precision: 0.029\n",
      "\n",
      "Accuracy: 0.010\n",
      "F1: 0.078\n",
      "Recall: 0.052\n",
      "Precision: 0.151\n",
      "Accuracy: 0.088\n",
      "F1: 0.587\n",
      "Recall: 0.462\n",
      "Precision: 0.803\n",
      "\n",
      "Accuracy: 0.008\n",
      "F1: 0.049\n",
      "Recall: 0.044\n",
      "Precision: 0.056\n",
      "Accuracy: 0.116\n",
      "F1: 0.588\n",
      "Recall: 0.613\n",
      "Precision: 0.564\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.430\n",
      "Recall: 0.465\n",
      "Precision: 0.400\n",
      "Accuracy: 0.008\n",
      "F1: 0.080\n",
      "Recall: 0.044\n",
      "Precision: 0.455\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.031\n",
      "Recall: 0.026\n",
      "Precision: 0.038\n",
      "Accuracy: 0.107\n",
      "F1: 0.593\n",
      "Recall: 0.567\n",
      "Precision: 0.621\n",
      "\n",
      "Accuracy: 0.143\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n",
      "Accuracy: 0.009\n",
      "F1: 0.036\n",
      "Recall: 0.047\n",
      "Precision: 0.029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 1e-07\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Accuracy: 0.071\n",
      "F1: 0.510\n",
      "Recall: 0.372\n",
      "Precision: 0.810\n",
      "Accuracy: 0.005\n",
      "F1: 0.036\n",
      "Recall: 0.026\n",
      "Precision: 0.056\n",
      "\n",
      "Accuracy: 0.107\n",
      "F1: 0.678\n",
      "Recall: 0.567\n",
      "Precision: 0.844\n",
      "Accuracy: 0.022\n",
      "F1: 0.121\n",
      "Recall: 0.116\n",
      "Precision: 0.127\n",
      "\n",
      "Accuracy: 0.066\n",
      "F1: 0.338\n",
      "Recall: 0.349\n",
      "Precision: 0.328\n",
      "Accuracy: 0.021\n",
      "F1: 0.139\n",
      "Recall: 0.113\n",
      "Precision: 0.181\n",
      "\n",
      "Accuracy: 0.099\n",
      "F1: 0.647\n",
      "Recall: 0.520\n",
      "Precision: 0.856\n",
      "Accuracy: 0.015\n",
      "F1: 0.060\n",
      "Recall: 0.078\n",
      "Precision: 0.048\n",
      "\n",
      "Accuracy: 0.069\n",
      "F1: 0.508\n",
      "Recall: 0.363\n",
      "Precision: 0.845\n",
      "Accuracy: 0.001\n",
      "F1: 0.010\n",
      "Recall: 0.006\n",
      "Precision: 0.029\n",
      "\n",
      "Accuracy: 0.010\n",
      "F1: 0.078\n",
      "Recall: 0.052\n",
      "Precision: 0.151\n",
      "Accuracy: 0.088\n",
      "F1: 0.587\n",
      "Recall: 0.462\n",
      "Precision: 0.803\n",
      "\n",
      "Accuracy: 0.008\n",
      "F1: 0.049\n",
      "Recall: 0.044\n",
      "Precision: 0.056\n",
      "Accuracy: 0.116\n",
      "F1: 0.588\n",
      "Recall: 0.613\n",
      "Precision: 0.564\n",
      "\n",
      "Accuracy: 0.088\n",
      "F1: 0.430\n",
      "Recall: 0.465\n",
      "Precision: 0.400\n",
      "Accuracy: 0.008\n",
      "F1: 0.080\n",
      "Recall: 0.044\n",
      "Precision: 0.455\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.031\n",
      "Recall: 0.026\n",
      "Precision: 0.038\n",
      "Accuracy: 0.107\n",
      "F1: 0.593\n",
      "Recall: 0.567\n",
      "Precision: 0.621\n",
      "\n",
      "Accuracy: 0.143\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n",
      "Accuracy: 0.009\n",
      "F1: 0.036\n",
      "Recall: 0.047\n",
      "Precision: 0.029\n",
      "\n",
      "10 0.0001\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=1 loss=4928.9775390625\n",
      "iteration=2 loss=4217.1865234375\n",
      "iteration=3 loss=3606.879150390625\n",
      "iteration=4 loss=3179.128662109375\n",
      "iteration=5 loss=2916.369873046875\n",
      "iteration=6 loss=2773.415771484375\n",
      "iteration=7 loss=2702.24609375\n",
      "iteration=8 loss=2649.3662109375\n",
      "iteration=9 loss=2558.78369140625\n",
      "iteration=9 loss=2558.78369140625\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=1 loss=5143.34228515625\n",
      "iteration=2 loss=4018.0478515625\n",
      "iteration=3 loss=3237.42138671875\n",
      "iteration=4 loss=2795.703369140625\n",
      "iteration=5 loss=2583.005126953125\n",
      "iteration=6 loss=2503.117919921875\n",
      "iteration=7 loss=2480.36669921875\n",
      "iteration=8 loss=2407.119384765625\n",
      "iteration=9 loss=2311.55517578125\n",
      "iteration=9 loss=2311.55517578125\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=1 loss=5289.08642578125\n",
      "iteration=2 loss=4410.51953125\n",
      "iteration=3 loss=3701.227294921875\n",
      "iteration=4 loss=3216.73193359375\n",
      "iteration=5 loss=2919.48681640625\n",
      "iteration=6 loss=2761.02197265625\n",
      "iteration=7 loss=2693.2255859375\n",
      "iteration=8 loss=2665.072265625\n",
      "iteration=9 loss=2628.21142578125\n",
      "iteration=9 loss=2628.21142578125\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=1 loss=3921.1005859375\n",
      "iteration=2 loss=3544.753173828125\n",
      "iteration=3 loss=3205.98828125\n",
      "iteration=4 loss=2954.3408203125\n",
      "iteration=5 loss=2788.410888671875\n",
      "iteration=6 loss=2687.43212890625\n",
      "iteration=7 loss=2625.00732421875\n",
      "iteration=8 loss=2575.043212890625\n",
      "iteration=9 loss=2516.919189453125\n",
      "iteration=9 loss=2516.919189453125\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=1 loss=4997.9970703125\n",
      "iteration=2 loss=4011.04052734375\n",
      "iteration=3 loss=3327.263427734375\n",
      "iteration=4 loss=2939.2890625\n",
      "iteration=5 loss=2758.822509765625\n",
      "iteration=6 loss=2707.71533203125\n",
      "iteration=7 loss=2714.410400390625\n",
      "iteration=8 loss=2709.37939453125\n",
      "iteration=9 loss=2644.3837890625\n",
      "iteration=9 loss=2644.3837890625\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=1 loss=5031.20068359375\n",
      "iteration=2 loss=4062.97607421875\n",
      "iteration=3 loss=3370.138427734375\n",
      "iteration=4 loss=2958.609619140625\n",
      "iteration=5 loss=2742.236572265625\n",
      "iteration=6 loss=2642.4736328125\n",
      "iteration=7 loss=2598.89453125\n",
      "iteration=8 loss=2565.739990234375\n",
      "iteration=9 loss=2513.0263671875\n",
      "iteration=9 loss=2513.0263671875\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=1 loss=5998.1923828125\n",
      "iteration=2 loss=4221.92578125\n",
      "iteration=3 loss=3303.88330078125\n",
      "iteration=4 loss=2907.792724609375\n",
      "iteration=5 loss=2792.9462890625\n",
      "iteration=6 loss=2821.293701171875\n",
      "iteration=7 loss=2883.26806640625\n",
      "iteration=8 loss=2854.052734375\n",
      "iteration=9 loss=2759.062744140625\n",
      "iteration=9 loss=2759.062744140625\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=1 loss=6405.28564453125\n",
      "iteration=2 loss=4114.892578125\n",
      "iteration=3 loss=3167.1005859375\n",
      "iteration=4 loss=2847.634765625\n",
      "iteration=5 loss=2825.556640625\n",
      "iteration=6 loss=2935.91259765625\n",
      "iteration=7 loss=3045.556884765625\n",
      "iteration=8 loss=3015.14453125\n",
      "iteration=9 loss=2852.78759765625\n",
      "iteration=9 loss=2852.78759765625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=1 loss=7058.30029296875\n",
      "iteration=2 loss=4665.5537109375\n",
      "iteration=3 loss=3581.22705078125\n",
      "iteration=4 loss=3126.6162109375\n",
      "iteration=5 loss=2969.663330078125\n",
      "iteration=6 loss=2955.490234375\n",
      "iteration=7 loss=2959.5341796875\n",
      "iteration=8 loss=2868.6474609375\n",
      "iteration=9 loss=2723.16455078125\n",
      "iteration=9 loss=2723.16455078125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=1 loss=4937.01318359375\n",
      "iteration=2 loss=3951.827392578125\n",
      "iteration=3 loss=3250.3740234375\n",
      "iteration=4 loss=2815.603515625\n",
      "iteration=5 loss=2562.093505859375\n",
      "iteration=6 loss=2424.966064453125\n",
      "iteration=7 loss=2357.103271484375\n",
      "iteration=8 loss=2321.52734375\n",
      "iteration=9 loss=2274.587158203125\n",
      "iteration=9 loss=2274.587158203125\n",
      "Accuracy: 0.104\n",
      "F1: 0.657\n",
      "Recall: 0.547\n",
      "Precision: 0.825\n",
      "Accuracy: 0.007\n",
      "F1: 0.053\n",
      "Recall: 0.035\n",
      "Precision: 0.107\n",
      "\n",
      "Accuracy: 0.108\n",
      "F1: 0.666\n",
      "Recall: 0.570\n",
      "Precision: 0.800\n",
      "Accuracy: 0.009\n",
      "F1: 0.050\n",
      "Recall: 0.049\n",
      "Precision: 0.051\n",
      "\n",
      "Accuracy: 0.104\n",
      "F1: 0.497\n",
      "Recall: 0.549\n",
      "Precision: 0.454\n",
      "Accuracy: 0.011\n",
      "F1: 0.078\n",
      "Recall: 0.058\n",
      "Precision: 0.118\n",
      "\n",
      "Accuracy: 0.127\n",
      "F1: 0.756\n",
      "Recall: 0.672\n",
      "Precision: 0.865\n",
      "Accuracy: 0.011\n",
      "F1: 0.048\n",
      "Recall: 0.058\n",
      "Precision: 0.040\n",
      "\n",
      "Accuracy: 0.131\n",
      "F1: 0.725\n",
      "Recall: 0.692\n",
      "Precision: 0.760\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.134\n",
      "F1: 0.733\n",
      "Recall: 0.706\n",
      "Precision: 0.762\n",
      "Accuracy: 0.007\n",
      "F1: 0.051\n",
      "Recall: 0.035\n",
      "Precision: 0.098\n",
      "\n",
      "Accuracy: 0.102\n",
      "F1: 0.575\n",
      "Recall: 0.541\n",
      "Precision: 0.614\n",
      "Accuracy: 0.025\n",
      "F1: 0.130\n",
      "Recall: 0.131\n",
      "Precision: 0.129\n",
      "\n",
      "Accuracy: 0.112\n",
      "F1: 0.578\n",
      "Recall: 0.593\n",
      "Precision: 0.564\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.003\n",
      "Precision: 0.028\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.568\n",
      "Recall: 0.616\n",
      "Precision: 0.526\n",
      "Accuracy: 0.013\n",
      "F1: 0.077\n",
      "Recall: 0.067\n",
      "Precision: 0.092\n",
      "\n",
      "Accuracy: 0.097\n",
      "F1: 0.626\n",
      "Recall: 0.512\n",
      "Precision: 0.807\n",
      "Accuracy: 0.003\n",
      "F1: 0.014\n",
      "Recall: 0.017\n",
      "Precision: 0.012\n",
      "\n",
      "10 1e-05\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=1 loss=4928.9775390625\n",
      "iteration=2 loss=4217.1865234375\n",
      "iteration=3 loss=3606.879150390625\n",
      "iteration=4 loss=3179.128662109375\n",
      "iteration=5 loss=2916.369873046875\n",
      "iteration=6 loss=2773.415771484375\n",
      "iteration=7 loss=2702.24609375\n",
      "iteration=8 loss=2649.3662109375\n",
      "iteration=9 loss=2558.78369140625\n",
      "iteration=9 loss=2558.78369140625\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=1 loss=5143.34228515625\n",
      "iteration=2 loss=4018.0478515625\n",
      "iteration=3 loss=3237.42138671875\n",
      "iteration=4 loss=2795.703369140625\n",
      "iteration=5 loss=2583.005126953125\n",
      "iteration=6 loss=2503.117919921875\n",
      "iteration=7 loss=2480.36669921875\n",
      "iteration=8 loss=2407.119384765625\n",
      "iteration=9 loss=2311.55517578125\n",
      "iteration=9 loss=2311.55517578125\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=1 loss=5289.08642578125\n",
      "iteration=2 loss=4410.51953125\n",
      "iteration=3 loss=3701.227294921875\n",
      "iteration=4 loss=3216.73193359375\n",
      "iteration=5 loss=2919.48681640625\n",
      "iteration=6 loss=2761.02197265625\n",
      "iteration=7 loss=2693.2255859375\n",
      "iteration=8 loss=2665.072265625\n",
      "iteration=9 loss=2628.21142578125\n",
      "iteration=9 loss=2628.21142578125\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=1 loss=3921.1005859375\n",
      "iteration=2 loss=3544.753173828125\n",
      "iteration=3 loss=3205.98828125\n",
      "iteration=4 loss=2954.3408203125\n",
      "iteration=5 loss=2788.410888671875\n",
      "iteration=6 loss=2687.43212890625\n",
      "iteration=7 loss=2625.00732421875\n",
      "iteration=8 loss=2575.043212890625\n",
      "iteration=9 loss=2516.919189453125\n",
      "iteration=9 loss=2516.919189453125\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=1 loss=4997.9970703125\n",
      "iteration=2 loss=4011.04052734375\n",
      "iteration=3 loss=3327.263427734375\n",
      "iteration=4 loss=2939.2890625\n",
      "iteration=5 loss=2758.822509765625\n",
      "iteration=6 loss=2707.71533203125\n",
      "iteration=7 loss=2714.410400390625\n",
      "iteration=8 loss=2709.37939453125\n",
      "iteration=9 loss=2644.3837890625\n",
      "iteration=9 loss=2644.3837890625\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=1 loss=5031.20068359375\n",
      "iteration=2 loss=4062.97607421875\n",
      "iteration=3 loss=3370.138427734375\n",
      "iteration=4 loss=2958.609619140625\n",
      "iteration=5 loss=2742.236572265625\n",
      "iteration=6 loss=2642.4736328125\n",
      "iteration=7 loss=2598.89453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=8 loss=2565.739990234375\n",
      "iteration=9 loss=2513.0263671875\n",
      "iteration=9 loss=2513.0263671875\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=1 loss=5998.1923828125\n",
      "iteration=2 loss=4221.92578125\n",
      "iteration=3 loss=3303.88330078125\n",
      "iteration=4 loss=2907.792724609375\n",
      "iteration=5 loss=2792.9462890625\n",
      "iteration=6 loss=2821.293701171875\n",
      "iteration=7 loss=2883.26806640625\n",
      "iteration=8 loss=2854.052734375\n",
      "iteration=9 loss=2759.062744140625\n",
      "iteration=9 loss=2759.062744140625\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=1 loss=6405.28564453125\n",
      "iteration=2 loss=4114.892578125\n",
      "iteration=3 loss=3167.1005859375\n",
      "iteration=4 loss=2847.634765625\n",
      "iteration=5 loss=2825.556640625\n",
      "iteration=6 loss=2935.91259765625\n",
      "iteration=7 loss=3045.556884765625\n",
      "iteration=8 loss=3015.14453125\n",
      "iteration=9 loss=2852.78759765625\n",
      "iteration=9 loss=2852.78759765625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=1 loss=7058.30029296875\n",
      "iteration=2 loss=4665.5537109375\n",
      "iteration=3 loss=3581.22705078125\n",
      "iteration=4 loss=3126.6162109375\n",
      "iteration=5 loss=2969.663330078125\n",
      "iteration=6 loss=2955.490234375\n",
      "iteration=7 loss=2959.5341796875\n",
      "iteration=8 loss=2868.6474609375\n",
      "iteration=9 loss=2723.16455078125\n",
      "iteration=9 loss=2723.16455078125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=1 loss=4937.01318359375\n",
      "iteration=2 loss=3951.827392578125\n",
      "iteration=3 loss=3250.3740234375\n",
      "iteration=4 loss=2815.603515625\n",
      "iteration=5 loss=2562.093505859375\n",
      "iteration=6 loss=2424.966064453125\n",
      "iteration=7 loss=2357.103271484375\n",
      "iteration=8 loss=2321.52734375\n",
      "iteration=9 loss=2274.587158203125\n",
      "iteration=9 loss=2274.587158203125\n",
      "Accuracy: 0.104\n",
      "F1: 0.657\n",
      "Recall: 0.547\n",
      "Precision: 0.825\n",
      "Accuracy: 0.007\n",
      "F1: 0.053\n",
      "Recall: 0.035\n",
      "Precision: 0.107\n",
      "\n",
      "Accuracy: 0.108\n",
      "F1: 0.666\n",
      "Recall: 0.570\n",
      "Precision: 0.800\n",
      "Accuracy: 0.009\n",
      "F1: 0.050\n",
      "Recall: 0.049\n",
      "Precision: 0.051\n",
      "\n",
      "Accuracy: 0.104\n",
      "F1: 0.497\n",
      "Recall: 0.549\n",
      "Precision: 0.454\n",
      "Accuracy: 0.011\n",
      "F1: 0.078\n",
      "Recall: 0.058\n",
      "Precision: 0.118\n",
      "\n",
      "Accuracy: 0.127\n",
      "F1: 0.756\n",
      "Recall: 0.672\n",
      "Precision: 0.865\n",
      "Accuracy: 0.011\n",
      "F1: 0.048\n",
      "Recall: 0.058\n",
      "Precision: 0.040\n",
      "\n",
      "Accuracy: 0.131\n",
      "F1: 0.725\n",
      "Recall: 0.692\n",
      "Precision: 0.760\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.134\n",
      "F1: 0.733\n",
      "Recall: 0.706\n",
      "Precision: 0.762\n",
      "Accuracy: 0.007\n",
      "F1: 0.051\n",
      "Recall: 0.035\n",
      "Precision: 0.098\n",
      "\n",
      "Accuracy: 0.102\n",
      "F1: 0.575\n",
      "Recall: 0.541\n",
      "Precision: 0.614\n",
      "Accuracy: 0.025\n",
      "F1: 0.130\n",
      "Recall: 0.131\n",
      "Precision: 0.129\n",
      "\n",
      "Accuracy: 0.112\n",
      "F1: 0.578\n",
      "Recall: 0.593\n",
      "Precision: 0.564\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.003\n",
      "Precision: 0.028\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.568\n",
      "Recall: 0.616\n",
      "Precision: 0.526\n",
      "Accuracy: 0.013\n",
      "F1: 0.077\n",
      "Recall: 0.067\n",
      "Precision: 0.092\n",
      "\n",
      "Accuracy: 0.097\n",
      "F1: 0.626\n",
      "Recall: 0.512\n",
      "Precision: 0.807\n",
      "Accuracy: 0.003\n",
      "F1: 0.014\n",
      "Recall: 0.017\n",
      "Precision: 0.012\n",
      "\n",
      "10 1e-06\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=1 loss=4928.9775390625\n",
      "iteration=2 loss=4217.1865234375\n",
      "iteration=3 loss=3606.879150390625\n",
      "iteration=4 loss=3179.128662109375\n",
      "iteration=5 loss=2916.369873046875\n",
      "iteration=6 loss=2773.415771484375\n",
      "iteration=7 loss=2702.24609375\n",
      "iteration=8 loss=2649.3662109375\n",
      "iteration=9 loss=2558.78369140625\n",
      "iteration=9 loss=2558.78369140625\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=1 loss=5143.34228515625\n",
      "iteration=2 loss=4018.0478515625\n",
      "iteration=3 loss=3237.42138671875\n",
      "iteration=4 loss=2795.703369140625\n",
      "iteration=5 loss=2583.005126953125\n",
      "iteration=6 loss=2503.117919921875\n",
      "iteration=7 loss=2480.36669921875\n",
      "iteration=8 loss=2407.119384765625\n",
      "iteration=9 loss=2311.55517578125\n",
      "iteration=9 loss=2311.55517578125\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=1 loss=5289.08642578125\n",
      "iteration=2 loss=4410.51953125\n",
      "iteration=3 loss=3701.227294921875\n",
      "iteration=4 loss=3216.73193359375\n",
      "iteration=5 loss=2919.48681640625\n",
      "iteration=6 loss=2761.02197265625\n",
      "iteration=7 loss=2693.2255859375\n",
      "iteration=8 loss=2665.072265625\n",
      "iteration=9 loss=2628.21142578125\n",
      "iteration=9 loss=2628.21142578125\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=1 loss=3921.1005859375\n",
      "iteration=2 loss=3544.753173828125\n",
      "iteration=3 loss=3205.98828125\n",
      "iteration=4 loss=2954.3408203125\n",
      "iteration=5 loss=2788.410888671875\n",
      "iteration=6 loss=2687.43212890625\n",
      "iteration=7 loss=2625.00732421875\n",
      "iteration=8 loss=2575.043212890625\n",
      "iteration=9 loss=2516.919189453125\n",
      "iteration=9 loss=2516.919189453125\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=1 loss=4997.9970703125\n",
      "iteration=2 loss=4011.04052734375\n",
      "iteration=3 loss=3327.263427734375\n",
      "iteration=4 loss=2939.2890625\n",
      "iteration=5 loss=2758.822509765625\n",
      "iteration=6 loss=2707.71533203125\n",
      "iteration=7 loss=2714.410400390625\n",
      "iteration=8 loss=2709.37939453125\n",
      "iteration=9 loss=2644.3837890625\n",
      "iteration=9 loss=2644.3837890625\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=1 loss=5031.20068359375\n",
      "iteration=2 loss=4062.97607421875\n",
      "iteration=3 loss=3370.138427734375\n",
      "iteration=4 loss=2958.609619140625\n",
      "iteration=5 loss=2742.236572265625\n",
      "iteration=6 loss=2642.4736328125\n",
      "iteration=7 loss=2598.89453125\n",
      "iteration=8 loss=2565.739990234375\n",
      "iteration=9 loss=2513.0263671875\n",
      "iteration=9 loss=2513.0263671875\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=1 loss=5998.1923828125\n",
      "iteration=2 loss=4221.92578125\n",
      "iteration=3 loss=3303.88330078125\n",
      "iteration=4 loss=2907.792724609375\n",
      "iteration=5 loss=2792.9462890625\n",
      "iteration=6 loss=2821.293701171875\n",
      "iteration=7 loss=2883.26806640625\n",
      "iteration=8 loss=2854.052734375\n",
      "iteration=9 loss=2759.062744140625\n",
      "iteration=9 loss=2759.062744140625\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=1 loss=6405.28564453125\n",
      "iteration=2 loss=4114.892578125\n",
      "iteration=3 loss=3167.1005859375\n",
      "iteration=4 loss=2847.634765625\n",
      "iteration=5 loss=2825.556640625\n",
      "iteration=6 loss=2935.91259765625\n",
      "iteration=7 loss=3045.556884765625\n",
      "iteration=8 loss=3015.14453125\n",
      "iteration=9 loss=2852.78759765625\n",
      "iteration=9 loss=2852.78759765625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=1 loss=7058.30029296875\n",
      "iteration=2 loss=4665.5537109375\n",
      "iteration=3 loss=3581.22705078125\n",
      "iteration=4 loss=3126.6162109375\n",
      "iteration=5 loss=2969.663330078125\n",
      "iteration=6 loss=2955.490234375\n",
      "iteration=7 loss=2959.5341796875\n",
      "iteration=8 loss=2868.6474609375\n",
      "iteration=9 loss=2723.16455078125\n",
      "iteration=9 loss=2723.16455078125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=1 loss=4937.01318359375\n",
      "iteration=2 loss=3951.827392578125\n",
      "iteration=3 loss=3250.3740234375\n",
      "iteration=4 loss=2815.603515625\n",
      "iteration=5 loss=2562.093505859375\n",
      "iteration=6 loss=2424.966064453125\n",
      "iteration=7 loss=2357.103271484375\n",
      "iteration=8 loss=2321.52734375\n",
      "iteration=9 loss=2274.587158203125\n",
      "iteration=9 loss=2274.587158203125\n",
      "Accuracy: 0.104\n",
      "F1: 0.657\n",
      "Recall: 0.547\n",
      "Precision: 0.825\n",
      "Accuracy: 0.007\n",
      "F1: 0.053\n",
      "Recall: 0.035\n",
      "Precision: 0.107\n",
      "\n",
      "Accuracy: 0.108\n",
      "F1: 0.666\n",
      "Recall: 0.570\n",
      "Precision: 0.800\n",
      "Accuracy: 0.009\n",
      "F1: 0.050\n",
      "Recall: 0.049\n",
      "Precision: 0.051\n",
      "\n",
      "Accuracy: 0.104\n",
      "F1: 0.497\n",
      "Recall: 0.549\n",
      "Precision: 0.454\n",
      "Accuracy: 0.011\n",
      "F1: 0.078\n",
      "Recall: 0.058\n",
      "Precision: 0.118\n",
      "\n",
      "Accuracy: 0.127\n",
      "F1: 0.756\n",
      "Recall: 0.672\n",
      "Precision: 0.865\n",
      "Accuracy: 0.011\n",
      "F1: 0.048\n",
      "Recall: 0.058\n",
      "Precision: 0.040\n",
      "\n",
      "Accuracy: 0.131\n",
      "F1: 0.725\n",
      "Recall: 0.692\n",
      "Precision: 0.760\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.134\n",
      "F1: 0.733\n",
      "Recall: 0.706\n",
      "Precision: 0.762\n",
      "Accuracy: 0.007\n",
      "F1: 0.051\n",
      "Recall: 0.035\n",
      "Precision: 0.098\n",
      "\n",
      "Accuracy: 0.102\n",
      "F1: 0.575\n",
      "Recall: 0.541\n",
      "Precision: 0.614\n",
      "Accuracy: 0.025\n",
      "F1: 0.130\n",
      "Recall: 0.131\n",
      "Precision: 0.129\n",
      "\n",
      "Accuracy: 0.112\n",
      "F1: 0.578\n",
      "Recall: 0.593\n",
      "Precision: 0.564\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.003\n",
      "Precision: 0.028\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.568\n",
      "Recall: 0.616\n",
      "Precision: 0.526\n",
      "Accuracy: 0.013\n",
      "F1: 0.077\n",
      "Recall: 0.067\n",
      "Precision: 0.092\n",
      "\n",
      "Accuracy: 0.097\n",
      "F1: 0.626\n",
      "Recall: 0.512\n",
      "Precision: 0.807\n",
      "Accuracy: 0.003\n",
      "F1: 0.014\n",
      "Recall: 0.017\n",
      "Precision: 0.012\n",
      "\n",
      "10 1e-07\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=1 loss=4928.9775390625\n",
      "iteration=2 loss=4217.1865234375\n",
      "iteration=3 loss=3606.879150390625\n",
      "iteration=4 loss=3179.128662109375\n",
      "iteration=5 loss=2916.369873046875\n",
      "iteration=6 loss=2773.415771484375\n",
      "iteration=7 loss=2702.24609375\n",
      "iteration=8 loss=2649.3662109375\n",
      "iteration=9 loss=2558.78369140625\n",
      "iteration=9 loss=2558.78369140625\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=1 loss=5143.34228515625\n",
      "iteration=2 loss=4018.0478515625\n",
      "iteration=3 loss=3237.42138671875\n",
      "iteration=4 loss=2795.703369140625\n",
      "iteration=5 loss=2583.005126953125\n",
      "iteration=6 loss=2503.117919921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=7 loss=2480.36669921875\n",
      "iteration=8 loss=2407.119384765625\n",
      "iteration=9 loss=2311.55517578125\n",
      "iteration=9 loss=2311.55517578125\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=1 loss=5289.08642578125\n",
      "iteration=2 loss=4410.51953125\n",
      "iteration=3 loss=3701.227294921875\n",
      "iteration=4 loss=3216.73193359375\n",
      "iteration=5 loss=2919.48681640625\n",
      "iteration=6 loss=2761.02197265625\n",
      "iteration=7 loss=2693.2255859375\n",
      "iteration=8 loss=2665.072265625\n",
      "iteration=9 loss=2628.21142578125\n",
      "iteration=9 loss=2628.21142578125\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=1 loss=3921.1005859375\n",
      "iteration=2 loss=3544.753173828125\n",
      "iteration=3 loss=3205.98828125\n",
      "iteration=4 loss=2954.3408203125\n",
      "iteration=5 loss=2788.410888671875\n",
      "iteration=6 loss=2687.43212890625\n",
      "iteration=7 loss=2625.00732421875\n",
      "iteration=8 loss=2575.043212890625\n",
      "iteration=9 loss=2516.919189453125\n",
      "iteration=9 loss=2516.919189453125\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=1 loss=4997.9970703125\n",
      "iteration=2 loss=4011.04052734375\n",
      "iteration=3 loss=3327.263427734375\n",
      "iteration=4 loss=2939.2890625\n",
      "iteration=5 loss=2758.822509765625\n",
      "iteration=6 loss=2707.71533203125\n",
      "iteration=7 loss=2714.410400390625\n",
      "iteration=8 loss=2709.37939453125\n",
      "iteration=9 loss=2644.3837890625\n",
      "iteration=9 loss=2644.3837890625\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=1 loss=5031.20068359375\n",
      "iteration=2 loss=4062.97607421875\n",
      "iteration=3 loss=3370.138427734375\n",
      "iteration=4 loss=2958.609619140625\n",
      "iteration=5 loss=2742.236572265625\n",
      "iteration=6 loss=2642.4736328125\n",
      "iteration=7 loss=2598.89453125\n",
      "iteration=8 loss=2565.739990234375\n",
      "iteration=9 loss=2513.0263671875\n",
      "iteration=9 loss=2513.0263671875\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=1 loss=5998.1923828125\n",
      "iteration=2 loss=4221.92578125\n",
      "iteration=3 loss=3303.88330078125\n",
      "iteration=4 loss=2907.792724609375\n",
      "iteration=5 loss=2792.9462890625\n",
      "iteration=6 loss=2821.293701171875\n",
      "iteration=7 loss=2883.26806640625\n",
      "iteration=8 loss=2854.052734375\n",
      "iteration=9 loss=2759.062744140625\n",
      "iteration=9 loss=2759.062744140625\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=1 loss=6405.28564453125\n",
      "iteration=2 loss=4114.892578125\n",
      "iteration=3 loss=3167.1005859375\n",
      "iteration=4 loss=2847.634765625\n",
      "iteration=5 loss=2825.556640625\n",
      "iteration=6 loss=2935.91259765625\n",
      "iteration=7 loss=3045.556884765625\n",
      "iteration=8 loss=3015.14453125\n",
      "iteration=9 loss=2852.78759765625\n",
      "iteration=9 loss=2852.78759765625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=1 loss=7058.30029296875\n",
      "iteration=2 loss=4665.5537109375\n",
      "iteration=3 loss=3581.22705078125\n",
      "iteration=4 loss=3126.6162109375\n",
      "iteration=5 loss=2969.663330078125\n",
      "iteration=6 loss=2955.490234375\n",
      "iteration=7 loss=2959.5341796875\n",
      "iteration=8 loss=2868.6474609375\n",
      "iteration=9 loss=2723.16455078125\n",
      "iteration=9 loss=2723.16455078125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=1 loss=4937.01318359375\n",
      "iteration=2 loss=3951.827392578125\n",
      "iteration=3 loss=3250.3740234375\n",
      "iteration=4 loss=2815.603515625\n",
      "iteration=5 loss=2562.093505859375\n",
      "iteration=6 loss=2424.966064453125\n",
      "iteration=7 loss=2357.103271484375\n",
      "iteration=8 loss=2321.52734375\n",
      "iteration=9 loss=2274.587158203125\n",
      "iteration=9 loss=2274.587158203125\n",
      "Accuracy: 0.104\n",
      "F1: 0.657\n",
      "Recall: 0.547\n",
      "Precision: 0.825\n",
      "Accuracy: 0.007\n",
      "F1: 0.053\n",
      "Recall: 0.035\n",
      "Precision: 0.107\n",
      "\n",
      "Accuracy: 0.108\n",
      "F1: 0.666\n",
      "Recall: 0.570\n",
      "Precision: 0.800\n",
      "Accuracy: 0.009\n",
      "F1: 0.050\n",
      "Recall: 0.049\n",
      "Precision: 0.051\n",
      "\n",
      "Accuracy: 0.104\n",
      "F1: 0.497\n",
      "Recall: 0.549\n",
      "Precision: 0.454\n",
      "Accuracy: 0.011\n",
      "F1: 0.078\n",
      "Recall: 0.058\n",
      "Precision: 0.118\n",
      "\n",
      "Accuracy: 0.127\n",
      "F1: 0.756\n",
      "Recall: 0.672\n",
      "Precision: 0.865\n",
      "Accuracy: 0.011\n",
      "F1: 0.048\n",
      "Recall: 0.058\n",
      "Precision: 0.040\n",
      "\n",
      "Accuracy: 0.131\n",
      "F1: 0.725\n",
      "Recall: 0.692\n",
      "Precision: 0.760\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "\n",
      "Accuracy: 0.134\n",
      "F1: 0.733\n",
      "Recall: 0.706\n",
      "Precision: 0.762\n",
      "Accuracy: 0.007\n",
      "F1: 0.051\n",
      "Recall: 0.035\n",
      "Precision: 0.098\n",
      "\n",
      "Accuracy: 0.102\n",
      "F1: 0.575\n",
      "Recall: 0.541\n",
      "Precision: 0.614\n",
      "Accuracy: 0.025\n",
      "F1: 0.130\n",
      "Recall: 0.131\n",
      "Precision: 0.129\n",
      "\n",
      "Accuracy: 0.112\n",
      "F1: 0.578\n",
      "Recall: 0.593\n",
      "Precision: 0.564\n",
      "Accuracy: 0.001\n",
      "F1: 0.005\n",
      "Recall: 0.003\n",
      "Precision: 0.028\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.568\n",
      "Recall: 0.616\n",
      "Precision: 0.526\n",
      "Accuracy: 0.013\n",
      "F1: 0.077\n",
      "Recall: 0.067\n",
      "Precision: 0.092\n",
      "\n",
      "Accuracy: 0.097\n",
      "F1: 0.626\n",
      "Recall: 0.512\n",
      "Precision: 0.807\n",
      "Accuracy: 0.003\n",
      "F1: 0.014\n",
      "Recall: 0.017\n",
      "Precision: 0.012\n",
      "\n",
      "25 0.0001\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=2 loss=4217.1865234375\n",
      "iteration=4 loss=3179.128662109375\n",
      "iteration=6 loss=2773.415771484375\n",
      "iteration=8 loss=2649.3662109375\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=12 loss=2286.616455078125\n",
      "iteration=14 loss=2095.920166015625\n",
      "iteration=16 loss=1920.3570556640625\n",
      "iteration=18 loss=1780.140625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=22 loss=1587.9288330078125\n",
      "iteration=24 loss=1525.505859375\n",
      "iteration=24 loss=1525.505859375\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=2 loss=4018.0478515625\n",
      "iteration=4 loss=2795.703369140625\n",
      "iteration=6 loss=2503.117919921875\n",
      "iteration=8 loss=2407.119384765625\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=12 loss=1940.2430419921875\n",
      "iteration=14 loss=1703.6090087890625\n",
      "iteration=16 loss=1528.538818359375\n",
      "iteration=18 loss=1412.1640625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=22 loss=1287.14599609375\n",
      "iteration=24 loss=1213.020263671875\n",
      "iteration=24 loss=1213.020263671875\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=2 loss=4410.51953125\n",
      "iteration=4 loss=3216.73193359375\n",
      "iteration=6 loss=2761.02197265625\n",
      "iteration=8 loss=2665.072265625\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=12 loss=2283.8076171875\n",
      "iteration=14 loss=2000.7120361328125\n",
      "iteration=16 loss=1797.05126953125\n",
      "iteration=18 loss=1655.200439453125\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=22 loss=1445.082763671875\n",
      "iteration=24 loss=1378.5755615234375\n",
      "iteration=24 loss=1378.5755615234375\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=2 loss=3544.753173828125\n",
      "iteration=4 loss=2954.3408203125\n",
      "iteration=6 loss=2687.43212890625\n",
      "iteration=8 loss=2575.043212890625\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=12 loss=2236.79150390625\n",
      "iteration=14 loss=2025.2252197265625\n",
      "iteration=16 loss=1850.644287109375\n",
      "iteration=18 loss=1714.997314453125\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=22 loss=1523.8477783203125\n",
      "iteration=24 loss=1464.915771484375\n",
      "iteration=24 loss=1464.915771484375\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=2 loss=4011.04052734375\n",
      "iteration=4 loss=2939.2890625\n",
      "iteration=6 loss=2707.71533203125\n",
      "iteration=8 loss=2709.37939453125\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=12 loss=2186.947509765625\n",
      "iteration=14 loss=1949.0645751953125\n",
      "iteration=16 loss=1822.406982421875\n",
      "iteration=18 loss=1735.4365234375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=22 loss=1559.448486328125\n",
      "iteration=24 loss=1474.685791015625\n",
      "iteration=24 loss=1474.685791015625\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=2 loss=4062.97607421875\n",
      "iteration=4 loss=2958.609619140625\n",
      "iteration=6 loss=2642.4736328125\n",
      "iteration=8 loss=2565.739990234375\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=12 loss=2199.711181640625\n",
      "iteration=14 loss=1959.55322265625\n",
      "iteration=16 loss=1782.412109375\n",
      "iteration=18 loss=1666.940185546875\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=22 loss=1526.6484375\n",
      "iteration=24 loss=1470.9847412109375\n",
      "iteration=24 loss=1470.9847412109375\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=2 loss=4221.92578125\n",
      "iteration=4 loss=2907.792724609375\n",
      "iteration=6 loss=2821.293701171875\n",
      "iteration=8 loss=2854.052734375\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=12 loss=2285.84765625\n",
      "iteration=14 loss=2040.1253662109375\n",
      "iteration=16 loss=1909.2144775390625\n",
      "iteration=18 loss=1837.72607421875\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=22 loss=1702.6717529296875\n",
      "iteration=24 loss=1622.9281005859375\n",
      "iteration=24 loss=1622.9281005859375\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=2 loss=4114.892578125\n",
      "iteration=4 loss=2847.634765625\n",
      "iteration=6 loss=2935.91259765625\n",
      "iteration=8 loss=3015.14453125\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=12 loss=2192.5849609375\n",
      "iteration=14 loss=1915.02294921875\n",
      "iteration=16 loss=1784.4078369140625\n",
      "iteration=18 loss=1712.52392578125\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=22 loss=1556.7706298828125\n",
      "iteration=24 loss=1467.353759765625\n",
      "iteration=24 loss=1467.353759765625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=2 loss=4665.5537109375\n",
      "iteration=4 loss=3126.6162109375\n",
      "iteration=6 loss=2955.490234375\n",
      "iteration=8 loss=2868.6474609375\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=12 loss=2176.965087890625\n",
      "iteration=14 loss=1898.752197265625\n",
      "iteration=16 loss=1722.412841796875\n",
      "iteration=18 loss=1609.00341796875\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=22 loss=1459.6649169921875\n",
      "iteration=24 loss=1392.427734375\n",
      "iteration=24 loss=1392.427734375\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=2 loss=3951.827392578125\n",
      "iteration=4 loss=2815.603515625\n",
      "iteration=6 loss=2424.966064453125\n",
      "iteration=8 loss=2321.52734375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=12 loss=1944.0787353515625\n",
      "iteration=14 loss=1673.101318359375\n",
      "iteration=16 loss=1466.5843505859375\n",
      "iteration=18 loss=1324.346435546875\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=22 loss=1137.6453857421875\n",
      "iteration=24 loss=1078.4256591796875\n",
      "iteration=24 loss=1078.4256591796875\n",
      "Accuracy: 0.135\n",
      "F1: 0.683\n",
      "Recall: 0.712\n",
      "Precision: 0.657\n",
      "Accuracy: 0.009\n",
      "F1: 0.086\n",
      "Recall: 0.049\n",
      "Precision: 0.333\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.234\n",
      "Accuracy: 0.056\n",
      "F1: 0.415\n",
      "Recall: 0.294\n",
      "Precision: 0.706\n",
      "\n",
      "Accuracy: 0.083\n",
      "F1: 0.339\n",
      "Recall: 0.436\n",
      "Precision: 0.278\n",
      "Accuracy: 0.017\n",
      "F1: 0.151\n",
      "Recall: 0.087\n",
      "Precision: 0.556\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.709\n",
      "Recall: 0.619\n",
      "Precision: 0.829\n",
      "Accuracy: 0.013\n",
      "F1: 0.064\n",
      "Recall: 0.067\n",
      "Precision: 0.061\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.692\n",
      "Recall: 0.657\n",
      "Precision: 0.731\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.009\n",
      "Precision: 0.333\n",
      "\n",
      "Accuracy: 0.036\n",
      "F1: 0.209\n",
      "Recall: 0.189\n",
      "Precision: 0.234\n",
      "Accuracy: 0.115\n",
      "F1: 0.677\n",
      "Recall: 0.608\n",
      "Precision: 0.766\n",
      "\n",
      "Accuracy: 0.048\n",
      "F1: 0.233\n",
      "Recall: 0.256\n",
      "Precision: 0.214\n",
      "Accuracy: 0.066\n",
      "F1: 0.468\n",
      "Recall: 0.346\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.049\n",
      "F1: 0.211\n",
      "Recall: 0.259\n",
      "Precision: 0.178\n",
      "Accuracy: 0.067\n",
      "F1: 0.505\n",
      "Recall: 0.352\n",
      "Precision: 0.896\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.026\n",
      "Recall: 0.015\n",
      "Precision: 0.122\n",
      "Accuracy: 0.123\n",
      "F1: 0.759\n",
      "Recall: 0.648\n",
      "Precision: 0.914\n",
      "\n",
      "Accuracy: 0.148\n",
      "F1: 0.635\n",
      "Recall: 0.782\n",
      "Precision: 0.535\n",
      "Accuracy: 0.021\n",
      "F1: 0.167\n",
      "Recall: 0.110\n",
      "Precision: 0.342\n",
      "\n",
      "25 1e-05\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=2 loss=4217.1865234375\n",
      "iteration=4 loss=3179.128662109375\n",
      "iteration=6 loss=2773.415771484375\n",
      "iteration=8 loss=2649.3662109375\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=12 loss=2286.616455078125\n",
      "iteration=14 loss=2095.920166015625\n",
      "iteration=16 loss=1920.3570556640625\n",
      "iteration=18 loss=1780.140625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=22 loss=1587.9288330078125\n",
      "iteration=24 loss=1525.505859375\n",
      "iteration=24 loss=1525.505859375\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=2 loss=4018.0478515625\n",
      "iteration=4 loss=2795.703369140625\n",
      "iteration=6 loss=2503.117919921875\n",
      "iteration=8 loss=2407.119384765625\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=12 loss=1940.2430419921875\n",
      "iteration=14 loss=1703.6090087890625\n",
      "iteration=16 loss=1528.538818359375\n",
      "iteration=18 loss=1412.1640625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=22 loss=1287.14599609375\n",
      "iteration=24 loss=1213.020263671875\n",
      "iteration=24 loss=1213.020263671875\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=2 loss=4410.51953125\n",
      "iteration=4 loss=3216.73193359375\n",
      "iteration=6 loss=2761.02197265625\n",
      "iteration=8 loss=2665.072265625\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=12 loss=2283.8076171875\n",
      "iteration=14 loss=2000.7120361328125\n",
      "iteration=16 loss=1797.05126953125\n",
      "iteration=18 loss=1655.200439453125\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=22 loss=1445.082763671875\n",
      "iteration=24 loss=1378.5755615234375\n",
      "iteration=24 loss=1378.5755615234375\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=2 loss=3544.753173828125\n",
      "iteration=4 loss=2954.3408203125\n",
      "iteration=6 loss=2687.43212890625\n",
      "iteration=8 loss=2575.043212890625\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=12 loss=2236.79150390625\n",
      "iteration=14 loss=2025.2252197265625\n",
      "iteration=16 loss=1850.644287109375\n",
      "iteration=18 loss=1714.997314453125\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=22 loss=1523.8477783203125\n",
      "iteration=24 loss=1464.915771484375\n",
      "iteration=24 loss=1464.915771484375\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=2 loss=4011.04052734375\n",
      "iteration=4 loss=2939.2890625\n",
      "iteration=6 loss=2707.71533203125\n",
      "iteration=8 loss=2709.37939453125\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=12 loss=2186.947509765625\n",
      "iteration=14 loss=1949.0645751953125\n",
      "iteration=16 loss=1822.406982421875\n",
      "iteration=18 loss=1735.4365234375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=22 loss=1559.448486328125\n",
      "iteration=24 loss=1474.685791015625\n",
      "iteration=24 loss=1474.685791015625\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=2 loss=4062.97607421875\n",
      "iteration=4 loss=2958.609619140625\n",
      "iteration=6 loss=2642.4736328125\n",
      "iteration=8 loss=2565.739990234375\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=12 loss=2199.711181640625\n",
      "iteration=14 loss=1959.55322265625\n",
      "iteration=16 loss=1782.412109375\n",
      "iteration=18 loss=1666.940185546875\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=22 loss=1526.6484375\n",
      "iteration=24 loss=1470.9847412109375\n",
      "iteration=24 loss=1470.9847412109375\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=2 loss=4221.92578125\n",
      "iteration=4 loss=2907.792724609375\n",
      "iteration=6 loss=2821.293701171875\n",
      "iteration=8 loss=2854.052734375\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=12 loss=2285.84765625\n",
      "iteration=14 loss=2040.1253662109375\n",
      "iteration=16 loss=1909.2144775390625\n",
      "iteration=18 loss=1837.72607421875\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=22 loss=1702.6717529296875\n",
      "iteration=24 loss=1622.9281005859375\n",
      "iteration=24 loss=1622.9281005859375\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=2 loss=4114.892578125\n",
      "iteration=4 loss=2847.634765625\n",
      "iteration=6 loss=2935.91259765625\n",
      "iteration=8 loss=3015.14453125\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=12 loss=2192.5849609375\n",
      "iteration=14 loss=1915.02294921875\n",
      "iteration=16 loss=1784.4078369140625\n",
      "iteration=18 loss=1712.52392578125\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=22 loss=1556.7706298828125\n",
      "iteration=24 loss=1467.353759765625\n",
      "iteration=24 loss=1467.353759765625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=2 loss=4665.5537109375\n",
      "iteration=4 loss=3126.6162109375\n",
      "iteration=6 loss=2955.490234375\n",
      "iteration=8 loss=2868.6474609375\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=12 loss=2176.965087890625\n",
      "iteration=14 loss=1898.752197265625\n",
      "iteration=16 loss=1722.412841796875\n",
      "iteration=18 loss=1609.00341796875\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=22 loss=1459.6649169921875\n",
      "iteration=24 loss=1392.427734375\n",
      "iteration=24 loss=1392.427734375\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=2 loss=3951.827392578125\n",
      "iteration=4 loss=2815.603515625\n",
      "iteration=6 loss=2424.966064453125\n",
      "iteration=8 loss=2321.52734375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=12 loss=1944.0787353515625\n",
      "iteration=14 loss=1673.101318359375\n",
      "iteration=16 loss=1466.5843505859375\n",
      "iteration=18 loss=1324.346435546875\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=22 loss=1137.6453857421875\n",
      "iteration=24 loss=1078.4256591796875\n",
      "iteration=24 loss=1078.4256591796875\n",
      "Accuracy: 0.135\n",
      "F1: 0.683\n",
      "Recall: 0.712\n",
      "Precision: 0.657\n",
      "Accuracy: 0.009\n",
      "F1: 0.086\n",
      "Recall: 0.049\n",
      "Precision: 0.333\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.234\n",
      "Accuracy: 0.056\n",
      "F1: 0.415\n",
      "Recall: 0.294\n",
      "Precision: 0.706\n",
      "\n",
      "Accuracy: 0.083\n",
      "F1: 0.339\n",
      "Recall: 0.436\n",
      "Precision: 0.278\n",
      "Accuracy: 0.017\n",
      "F1: 0.151\n",
      "Recall: 0.087\n",
      "Precision: 0.556\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.709\n",
      "Recall: 0.619\n",
      "Precision: 0.829\n",
      "Accuracy: 0.013\n",
      "F1: 0.064\n",
      "Recall: 0.067\n",
      "Precision: 0.061\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.692\n",
      "Recall: 0.657\n",
      "Precision: 0.731\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.009\n",
      "Precision: 0.333\n",
      "\n",
      "Accuracy: 0.036\n",
      "F1: 0.209\n",
      "Recall: 0.189\n",
      "Precision: 0.234\n",
      "Accuracy: 0.115\n",
      "F1: 0.677\n",
      "Recall: 0.608\n",
      "Precision: 0.766\n",
      "\n",
      "Accuracy: 0.048\n",
      "F1: 0.233\n",
      "Recall: 0.256\n",
      "Precision: 0.214\n",
      "Accuracy: 0.066\n",
      "F1: 0.468\n",
      "Recall: 0.346\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.049\n",
      "F1: 0.211\n",
      "Recall: 0.259\n",
      "Precision: 0.178\n",
      "Accuracy: 0.067\n",
      "F1: 0.505\n",
      "Recall: 0.352\n",
      "Precision: 0.896\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.026\n",
      "Recall: 0.015\n",
      "Precision: 0.122\n",
      "Accuracy: 0.123\n",
      "F1: 0.759\n",
      "Recall: 0.648\n",
      "Precision: 0.914\n",
      "\n",
      "Accuracy: 0.148\n",
      "F1: 0.635\n",
      "Recall: 0.782\n",
      "Precision: 0.535\n",
      "Accuracy: 0.021\n",
      "F1: 0.167\n",
      "Recall: 0.110\n",
      "Precision: 0.342\n",
      "\n",
      "25 1e-06\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=2 loss=4217.1865234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=4 loss=3179.128662109375\n",
      "iteration=6 loss=2773.415771484375\n",
      "iteration=8 loss=2649.3662109375\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=12 loss=2286.616455078125\n",
      "iteration=14 loss=2095.920166015625\n",
      "iteration=16 loss=1920.3570556640625\n",
      "iteration=18 loss=1780.140625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=22 loss=1587.9288330078125\n",
      "iteration=24 loss=1525.505859375\n",
      "iteration=24 loss=1525.505859375\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=2 loss=4018.0478515625\n",
      "iteration=4 loss=2795.703369140625\n",
      "iteration=6 loss=2503.117919921875\n",
      "iteration=8 loss=2407.119384765625\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=12 loss=1940.2430419921875\n",
      "iteration=14 loss=1703.6090087890625\n",
      "iteration=16 loss=1528.538818359375\n",
      "iteration=18 loss=1412.1640625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=22 loss=1287.14599609375\n",
      "iteration=24 loss=1213.020263671875\n",
      "iteration=24 loss=1213.020263671875\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=2 loss=4410.51953125\n",
      "iteration=4 loss=3216.73193359375\n",
      "iteration=6 loss=2761.02197265625\n",
      "iteration=8 loss=2665.072265625\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=12 loss=2283.8076171875\n",
      "iteration=14 loss=2000.7120361328125\n",
      "iteration=16 loss=1797.05126953125\n",
      "iteration=18 loss=1655.200439453125\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=22 loss=1445.082763671875\n",
      "iteration=24 loss=1378.5755615234375\n",
      "iteration=24 loss=1378.5755615234375\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=2 loss=3544.753173828125\n",
      "iteration=4 loss=2954.3408203125\n",
      "iteration=6 loss=2687.43212890625\n",
      "iteration=8 loss=2575.043212890625\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=12 loss=2236.79150390625\n",
      "iteration=14 loss=2025.2252197265625\n",
      "iteration=16 loss=1850.644287109375\n",
      "iteration=18 loss=1714.997314453125\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=22 loss=1523.8477783203125\n",
      "iteration=24 loss=1464.915771484375\n",
      "iteration=24 loss=1464.915771484375\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=2 loss=4011.04052734375\n",
      "iteration=4 loss=2939.2890625\n",
      "iteration=6 loss=2707.71533203125\n",
      "iteration=8 loss=2709.37939453125\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=12 loss=2186.947509765625\n",
      "iteration=14 loss=1949.0645751953125\n",
      "iteration=16 loss=1822.406982421875\n",
      "iteration=18 loss=1735.4365234375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=22 loss=1559.448486328125\n",
      "iteration=24 loss=1474.685791015625\n",
      "iteration=24 loss=1474.685791015625\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=2 loss=4062.97607421875\n",
      "iteration=4 loss=2958.609619140625\n",
      "iteration=6 loss=2642.4736328125\n",
      "iteration=8 loss=2565.739990234375\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=12 loss=2199.711181640625\n",
      "iteration=14 loss=1959.55322265625\n",
      "iteration=16 loss=1782.412109375\n",
      "iteration=18 loss=1666.940185546875\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=22 loss=1526.6484375\n",
      "iteration=24 loss=1470.9847412109375\n",
      "iteration=24 loss=1470.9847412109375\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=2 loss=4221.92578125\n",
      "iteration=4 loss=2907.792724609375\n",
      "iteration=6 loss=2821.293701171875\n",
      "iteration=8 loss=2854.052734375\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=12 loss=2285.84765625\n",
      "iteration=14 loss=2040.1253662109375\n",
      "iteration=16 loss=1909.2144775390625\n",
      "iteration=18 loss=1837.72607421875\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=22 loss=1702.6717529296875\n",
      "iteration=24 loss=1622.9281005859375\n",
      "iteration=24 loss=1622.9281005859375\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=2 loss=4114.892578125\n",
      "iteration=4 loss=2847.634765625\n",
      "iteration=6 loss=2935.91259765625\n",
      "iteration=8 loss=3015.14453125\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=12 loss=2192.5849609375\n",
      "iteration=14 loss=1915.02294921875\n",
      "iteration=16 loss=1784.4078369140625\n",
      "iteration=18 loss=1712.52392578125\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=22 loss=1556.7706298828125\n",
      "iteration=24 loss=1467.353759765625\n",
      "iteration=24 loss=1467.353759765625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=2 loss=4665.5537109375\n",
      "iteration=4 loss=3126.6162109375\n",
      "iteration=6 loss=2955.490234375\n",
      "iteration=8 loss=2868.6474609375\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=12 loss=2176.965087890625\n",
      "iteration=14 loss=1898.752197265625\n",
      "iteration=16 loss=1722.412841796875\n",
      "iteration=18 loss=1609.00341796875\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=22 loss=1459.6649169921875\n",
      "iteration=24 loss=1392.427734375\n",
      "iteration=24 loss=1392.427734375\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=2 loss=3951.827392578125\n",
      "iteration=4 loss=2815.603515625\n",
      "iteration=6 loss=2424.966064453125\n",
      "iteration=8 loss=2321.52734375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=12 loss=1944.0787353515625\n",
      "iteration=14 loss=1673.101318359375\n",
      "iteration=16 loss=1466.5843505859375\n",
      "iteration=18 loss=1324.346435546875\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=22 loss=1137.6453857421875\n",
      "iteration=24 loss=1078.4256591796875\n",
      "iteration=24 loss=1078.4256591796875\n",
      "Accuracy: 0.135\n",
      "F1: 0.683\n",
      "Recall: 0.712\n",
      "Precision: 0.657\n",
      "Accuracy: 0.009\n",
      "F1: 0.086\n",
      "Recall: 0.049\n",
      "Precision: 0.333\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.234\n",
      "Accuracy: 0.056\n",
      "F1: 0.415\n",
      "Recall: 0.294\n",
      "Precision: 0.706\n",
      "\n",
      "Accuracy: 0.083\n",
      "F1: 0.339\n",
      "Recall: 0.436\n",
      "Precision: 0.278\n",
      "Accuracy: 0.017\n",
      "F1: 0.151\n",
      "Recall: 0.087\n",
      "Precision: 0.556\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.709\n",
      "Recall: 0.619\n",
      "Precision: 0.829\n",
      "Accuracy: 0.013\n",
      "F1: 0.064\n",
      "Recall: 0.067\n",
      "Precision: 0.061\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.692\n",
      "Recall: 0.657\n",
      "Precision: 0.731\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.009\n",
      "Precision: 0.333\n",
      "\n",
      "Accuracy: 0.036\n",
      "F1: 0.209\n",
      "Recall: 0.189\n",
      "Precision: 0.234\n",
      "Accuracy: 0.115\n",
      "F1: 0.677\n",
      "Recall: 0.608\n",
      "Precision: 0.766\n",
      "\n",
      "Accuracy: 0.048\n",
      "F1: 0.233\n",
      "Recall: 0.256\n",
      "Precision: 0.214\n",
      "Accuracy: 0.066\n",
      "F1: 0.468\n",
      "Recall: 0.346\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.049\n",
      "F1: 0.211\n",
      "Recall: 0.259\n",
      "Precision: 0.178\n",
      "Accuracy: 0.067\n",
      "F1: 0.505\n",
      "Recall: 0.352\n",
      "Precision: 0.896\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.026\n",
      "Recall: 0.015\n",
      "Precision: 0.122\n",
      "Accuracy: 0.123\n",
      "F1: 0.759\n",
      "Recall: 0.648\n",
      "Precision: 0.914\n",
      "\n",
      "Accuracy: 0.148\n",
      "F1: 0.635\n",
      "Recall: 0.782\n",
      "Precision: 0.535\n",
      "Accuracy: 0.021\n",
      "F1: 0.167\n",
      "Recall: 0.110\n",
      "Precision: 0.342\n",
      "\n",
      "25 1e-07\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=2 loss=4217.1865234375\n",
      "iteration=4 loss=3179.128662109375\n",
      "iteration=6 loss=2773.415771484375\n",
      "iteration=8 loss=2649.3662109375\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=12 loss=2286.616455078125\n",
      "iteration=14 loss=2095.920166015625\n",
      "iteration=16 loss=1920.3570556640625\n",
      "iteration=18 loss=1780.140625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=22 loss=1587.9288330078125\n",
      "iteration=24 loss=1525.505859375\n",
      "iteration=24 loss=1525.505859375\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=2 loss=4018.0478515625\n",
      "iteration=4 loss=2795.703369140625\n",
      "iteration=6 loss=2503.117919921875\n",
      "iteration=8 loss=2407.119384765625\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=12 loss=1940.2430419921875\n",
      "iteration=14 loss=1703.6090087890625\n",
      "iteration=16 loss=1528.538818359375\n",
      "iteration=18 loss=1412.1640625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=22 loss=1287.14599609375\n",
      "iteration=24 loss=1213.020263671875\n",
      "iteration=24 loss=1213.020263671875\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=2 loss=4410.51953125\n",
      "iteration=4 loss=3216.73193359375\n",
      "iteration=6 loss=2761.02197265625\n",
      "iteration=8 loss=2665.072265625\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=12 loss=2283.8076171875\n",
      "iteration=14 loss=2000.7120361328125\n",
      "iteration=16 loss=1797.05126953125\n",
      "iteration=18 loss=1655.200439453125\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=22 loss=1445.082763671875\n",
      "iteration=24 loss=1378.5755615234375\n",
      "iteration=24 loss=1378.5755615234375\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=2 loss=3544.753173828125\n",
      "iteration=4 loss=2954.3408203125\n",
      "iteration=6 loss=2687.43212890625\n",
      "iteration=8 loss=2575.043212890625\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=12 loss=2236.79150390625\n",
      "iteration=14 loss=2025.2252197265625\n",
      "iteration=16 loss=1850.644287109375\n",
      "iteration=18 loss=1714.997314453125\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=22 loss=1523.8477783203125\n",
      "iteration=24 loss=1464.915771484375\n",
      "iteration=24 loss=1464.915771484375\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=2 loss=4011.04052734375\n",
      "iteration=4 loss=2939.2890625\n",
      "iteration=6 loss=2707.71533203125\n",
      "iteration=8 loss=2709.37939453125\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=12 loss=2186.947509765625\n",
      "iteration=14 loss=1949.0645751953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=16 loss=1822.406982421875\n",
      "iteration=18 loss=1735.4365234375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=22 loss=1559.448486328125\n",
      "iteration=24 loss=1474.685791015625\n",
      "iteration=24 loss=1474.685791015625\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=2 loss=4062.97607421875\n",
      "iteration=4 loss=2958.609619140625\n",
      "iteration=6 loss=2642.4736328125\n",
      "iteration=8 loss=2565.739990234375\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=12 loss=2199.711181640625\n",
      "iteration=14 loss=1959.55322265625\n",
      "iteration=16 loss=1782.412109375\n",
      "iteration=18 loss=1666.940185546875\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=22 loss=1526.6484375\n",
      "iteration=24 loss=1470.9847412109375\n",
      "iteration=24 loss=1470.9847412109375\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=2 loss=4221.92578125\n",
      "iteration=4 loss=2907.792724609375\n",
      "iteration=6 loss=2821.293701171875\n",
      "iteration=8 loss=2854.052734375\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=12 loss=2285.84765625\n",
      "iteration=14 loss=2040.1253662109375\n",
      "iteration=16 loss=1909.2144775390625\n",
      "iteration=18 loss=1837.72607421875\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=22 loss=1702.6717529296875\n",
      "iteration=24 loss=1622.9281005859375\n",
      "iteration=24 loss=1622.9281005859375\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=2 loss=4114.892578125\n",
      "iteration=4 loss=2847.634765625\n",
      "iteration=6 loss=2935.91259765625\n",
      "iteration=8 loss=3015.14453125\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=12 loss=2192.5849609375\n",
      "iteration=14 loss=1915.02294921875\n",
      "iteration=16 loss=1784.4078369140625\n",
      "iteration=18 loss=1712.52392578125\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=22 loss=1556.7706298828125\n",
      "iteration=24 loss=1467.353759765625\n",
      "iteration=24 loss=1467.353759765625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=2 loss=4665.5537109375\n",
      "iteration=4 loss=3126.6162109375\n",
      "iteration=6 loss=2955.490234375\n",
      "iteration=8 loss=2868.6474609375\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=12 loss=2176.965087890625\n",
      "iteration=14 loss=1898.752197265625\n",
      "iteration=16 loss=1722.412841796875\n",
      "iteration=18 loss=1609.00341796875\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=22 loss=1459.6649169921875\n",
      "iteration=24 loss=1392.427734375\n",
      "iteration=24 loss=1392.427734375\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=2 loss=3951.827392578125\n",
      "iteration=4 loss=2815.603515625\n",
      "iteration=6 loss=2424.966064453125\n",
      "iteration=8 loss=2321.52734375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=12 loss=1944.0787353515625\n",
      "iteration=14 loss=1673.101318359375\n",
      "iteration=16 loss=1466.5843505859375\n",
      "iteration=18 loss=1324.346435546875\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=22 loss=1137.6453857421875\n",
      "iteration=24 loss=1078.4256591796875\n",
      "iteration=24 loss=1078.4256591796875\n",
      "Accuracy: 0.135\n",
      "F1: 0.683\n",
      "Recall: 0.712\n",
      "Precision: 0.657\n",
      "Accuracy: 0.009\n",
      "F1: 0.086\n",
      "Recall: 0.049\n",
      "Precision: 0.333\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.234\n",
      "Accuracy: 0.056\n",
      "F1: 0.415\n",
      "Recall: 0.294\n",
      "Precision: 0.706\n",
      "\n",
      "Accuracy: 0.083\n",
      "F1: 0.339\n",
      "Recall: 0.436\n",
      "Precision: 0.278\n",
      "Accuracy: 0.017\n",
      "F1: 0.151\n",
      "Recall: 0.087\n",
      "Precision: 0.556\n",
      "\n",
      "Accuracy: 0.117\n",
      "F1: 0.709\n",
      "Recall: 0.619\n",
      "Precision: 0.829\n",
      "Accuracy: 0.013\n",
      "F1: 0.064\n",
      "Recall: 0.067\n",
      "Precision: 0.061\n",
      "\n",
      "Accuracy: 0.125\n",
      "F1: 0.692\n",
      "Recall: 0.657\n",
      "Precision: 0.731\n",
      "Accuracy: 0.002\n",
      "F1: 0.017\n",
      "Recall: 0.009\n",
      "Precision: 0.333\n",
      "\n",
      "Accuracy: 0.036\n",
      "F1: 0.209\n",
      "Recall: 0.189\n",
      "Precision: 0.234\n",
      "Accuracy: 0.115\n",
      "F1: 0.677\n",
      "Recall: 0.608\n",
      "Precision: 0.766\n",
      "\n",
      "Accuracy: 0.048\n",
      "F1: 0.233\n",
      "Recall: 0.256\n",
      "Precision: 0.214\n",
      "Accuracy: 0.066\n",
      "F1: 0.468\n",
      "Recall: 0.346\n",
      "Precision: 0.721\n",
      "\n",
      "Accuracy: 0.049\n",
      "F1: 0.211\n",
      "Recall: 0.259\n",
      "Precision: 0.178\n",
      "Accuracy: 0.067\n",
      "F1: 0.505\n",
      "Recall: 0.352\n",
      "Precision: 0.896\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.026\n",
      "Recall: 0.015\n",
      "Precision: 0.122\n",
      "Accuracy: 0.123\n",
      "F1: 0.759\n",
      "Recall: 0.648\n",
      "Precision: 0.914\n",
      "\n",
      "Accuracy: 0.148\n",
      "F1: 0.635\n",
      "Recall: 0.782\n",
      "Precision: 0.535\n",
      "Accuracy: 0.021\n",
      "F1: 0.167\n",
      "Recall: 0.110\n",
      "Precision: 0.342\n",
      "\n",
      "50 0.0001\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=5 loss=2916.369873046875\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=15 loss=2004.2828369140625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=25 loss=1500.6927490234375\n",
      "iteration=30 loss=1357.379150390625\n",
      "iteration=35 loss=1238.488037109375\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=45 loss=1040.301513671875\n",
      "iteration=49 loss=980.8890380859375\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=5 loss=2583.005126953125\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=15 loss=1607.670166015625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=25 loss=1178.684326171875\n",
      "iteration=30 loss=1049.43798828125\n",
      "iteration=35 loss=943.6692504882812\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=45 loss=796.6639404296875\n",
      "iteration=49 loss=753.6226196289062\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=5 loss=2919.48681640625\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=15 loss=1888.6195068359375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=25 loss=1356.610107421875\n",
      "iteration=30 loss=1210.9149169921875\n",
      "iteration=35 loss=1102.93798828125\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=45 loss=933.1814575195312\n",
      "iteration=49 loss=880.9447021484375\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=5 loss=2788.410888671875\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=15 loss=1932.5836181640625\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=25 loss=1439.356689453125\n",
      "iteration=30 loss=1282.6077880859375\n",
      "iteration=35 loss=1157.9271240234375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=45 loss=963.78466796875\n",
      "iteration=49 loss=907.0369873046875\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=5 loss=2758.822509765625\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=15 loss=1876.363037109375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=25 loss=1435.83642578125\n",
      "iteration=30 loss=1276.5987548828125\n",
      "iteration=35 loss=1136.5814208984375\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=45 loss=943.5738525390625\n",
      "iteration=49 loss=889.4700927734375\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=5 loss=2742.236572265625\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=15 loss=1861.965087890625\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=25 loss=1439.5703125\n",
      "iteration=30 loss=1297.69677734375\n",
      "iteration=35 loss=1188.870849609375\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=45 loss=1006.1919555664062\n",
      "iteration=49 loss=944.5415649414062\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=5 loss=2792.9462890625\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=15 loss=1963.2392578125\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=25 loss=1584.427001953125\n",
      "iteration=30 loss=1429.028076171875\n",
      "iteration=35 loss=1297.962646484375\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=45 loss=1102.8558349609375\n",
      "iteration=49 loss=1042.1461181640625\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=5 loss=2825.556640625\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=15 loss=1837.097412109375\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=25 loss=1426.9168701171875\n",
      "iteration=30 loss=1270.780517578125\n",
      "iteration=35 loss=1140.966796875\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=45 loss=961.5462646484375\n",
      "iteration=49 loss=910.909912109375\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=5 loss=2969.663330078125\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=15 loss=1799.749755859375\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=25 loss=1356.2200927734375\n",
      "iteration=30 loss=1195.76513671875\n",
      "iteration=35 loss=1079.3731689453125\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=45 loss=902.2664794921875\n",
      "iteration=49 loss=850.6611328125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=5 loss=2562.093505859375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=15 loss=1560.3394775390625\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=25 loss=1059.6268310546875\n",
      "iteration=30 loss=952.5345458984375\n",
      "iteration=35 loss=881.6448974609375\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=45 loss=777.516357421875\n",
      "iteration=49 loss=746.052978515625\n",
      "Accuracy: 0.004\n",
      "F1: 0.041\n",
      "Recall: 0.023\n",
      "Precision: 0.186\n",
      "Accuracy: 0.025\n",
      "F1: 0.212\n",
      "Recall: 0.131\n",
      "Precision: 0.562\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.224\n",
      "Accuracy: 0.075\n",
      "F1: 0.521\n",
      "Recall: 0.395\n",
      "Precision: 0.764\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.224\n",
      "Accuracy: 0.027\n",
      "F1: 0.228\n",
      "Recall: 0.142\n",
      "Precision: 0.576\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.010\n",
      "F1: 0.101\n",
      "Recall: 0.055\n",
      "Precision: 0.613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.041\n",
      "Recall: 0.023\n",
      "Precision: 0.186\n",
      "Accuracy: 0.015\n",
      "F1: 0.142\n",
      "Recall: 0.081\n",
      "Precision: 0.560\n",
      "\n",
      "Accuracy: 0.029\n",
      "F1: 0.137\n",
      "Recall: 0.154\n",
      "Precision: 0.123\n",
      "Accuracy: 0.121\n",
      "F1: 0.696\n",
      "Recall: 0.637\n",
      "Precision: 0.768\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.036\n",
      "Recall: 0.020\n",
      "Precision: 0.163\n",
      "Accuracy: 0.037\n",
      "F1: 0.294\n",
      "Recall: 0.195\n",
      "Precision: 0.598\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.051\n",
      "Recall: 0.029\n",
      "Precision: 0.208\n",
      "Accuracy: 0.061\n",
      "F1: 0.460\n",
      "Recall: 0.320\n",
      "Precision: 0.821\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.031\n",
      "Recall: 0.017\n",
      "Precision: 0.146\n",
      "Accuracy: 0.091\n",
      "F1: 0.621\n",
      "Recall: 0.480\n",
      "Precision: 0.882\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.003\n",
      "Precision: 0.500\n",
      "Accuracy: 0.034\n",
      "F1: 0.282\n",
      "Recall: 0.180\n",
      "Precision: 0.653\n",
      "\n",
      "50 1e-05\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=5 loss=2916.369873046875\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=15 loss=2004.2828369140625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=25 loss=1500.6927490234375\n",
      "iteration=30 loss=1357.379150390625\n",
      "iteration=35 loss=1238.488037109375\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=45 loss=1040.301513671875\n",
      "iteration=49 loss=980.8890380859375\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=5 loss=2583.005126953125\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=15 loss=1607.670166015625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=25 loss=1178.684326171875\n",
      "iteration=30 loss=1049.43798828125\n",
      "iteration=35 loss=943.6692504882812\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=45 loss=796.6639404296875\n",
      "iteration=49 loss=753.6226196289062\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=5 loss=2919.48681640625\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=15 loss=1888.6195068359375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=25 loss=1356.610107421875\n",
      "iteration=30 loss=1210.9149169921875\n",
      "iteration=35 loss=1102.93798828125\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=45 loss=933.1814575195312\n",
      "iteration=49 loss=880.9447021484375\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=5 loss=2788.410888671875\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=15 loss=1932.5836181640625\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=25 loss=1439.356689453125\n",
      "iteration=30 loss=1282.6077880859375\n",
      "iteration=35 loss=1157.9271240234375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=45 loss=963.78466796875\n",
      "iteration=49 loss=907.0369873046875\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=5 loss=2758.822509765625\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=15 loss=1876.363037109375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=25 loss=1435.83642578125\n",
      "iteration=30 loss=1276.5987548828125\n",
      "iteration=35 loss=1136.5814208984375\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=45 loss=943.5738525390625\n",
      "iteration=49 loss=889.4700927734375\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=5 loss=2742.236572265625\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=15 loss=1861.965087890625\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=25 loss=1439.5703125\n",
      "iteration=30 loss=1297.69677734375\n",
      "iteration=35 loss=1188.870849609375\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=45 loss=1006.1919555664062\n",
      "iteration=49 loss=944.5415649414062\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=5 loss=2792.9462890625\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=15 loss=1963.2392578125\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=25 loss=1584.427001953125\n",
      "iteration=30 loss=1429.028076171875\n",
      "iteration=35 loss=1297.962646484375\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=45 loss=1102.8558349609375\n",
      "iteration=49 loss=1042.1461181640625\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=5 loss=2825.556640625\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=15 loss=1837.097412109375\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=25 loss=1426.9168701171875\n",
      "iteration=30 loss=1270.780517578125\n",
      "iteration=35 loss=1140.966796875\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=45 loss=961.5462646484375\n",
      "iteration=49 loss=910.909912109375\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=5 loss=2969.663330078125\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=15 loss=1799.749755859375\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=25 loss=1356.2200927734375\n",
      "iteration=30 loss=1195.76513671875\n",
      "iteration=35 loss=1079.3731689453125\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=45 loss=902.2664794921875\n",
      "iteration=49 loss=850.6611328125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=5 loss=2562.093505859375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=15 loss=1560.3394775390625\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=25 loss=1059.6268310546875\n",
      "iteration=30 loss=952.5345458984375\n",
      "iteration=35 loss=881.6448974609375\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=45 loss=777.516357421875\n",
      "iteration=49 loss=746.052978515625\n",
      "Accuracy: 0.004\n",
      "F1: 0.041\n",
      "Recall: 0.023\n",
      "Precision: 0.186\n",
      "Accuracy: 0.025\n",
      "F1: 0.212\n",
      "Recall: 0.131\n",
      "Precision: 0.562\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.224\n",
      "Accuracy: 0.075\n",
      "F1: 0.521\n",
      "Recall: 0.395\n",
      "Precision: 0.764\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.224\n",
      "Accuracy: 0.027\n",
      "F1: 0.228\n",
      "Recall: 0.142\n",
      "Precision: 0.576\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.010\n",
      "F1: 0.101\n",
      "Recall: 0.055\n",
      "Precision: 0.613\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.041\n",
      "Recall: 0.023\n",
      "Precision: 0.186\n",
      "Accuracy: 0.015\n",
      "F1: 0.142\n",
      "Recall: 0.081\n",
      "Precision: 0.560\n",
      "\n",
      "Accuracy: 0.029\n",
      "F1: 0.137\n",
      "Recall: 0.154\n",
      "Precision: 0.123\n",
      "Accuracy: 0.121\n",
      "F1: 0.696\n",
      "Recall: 0.637\n",
      "Precision: 0.768\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.036\n",
      "Recall: 0.020\n",
      "Precision: 0.163\n",
      "Accuracy: 0.037\n",
      "F1: 0.294\n",
      "Recall: 0.195\n",
      "Precision: 0.598\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.051\n",
      "Recall: 0.029\n",
      "Precision: 0.208\n",
      "Accuracy: 0.061\n",
      "F1: 0.460\n",
      "Recall: 0.320\n",
      "Precision: 0.821\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.031\n",
      "Recall: 0.017\n",
      "Precision: 0.146\n",
      "Accuracy: 0.091\n",
      "F1: 0.621\n",
      "Recall: 0.480\n",
      "Precision: 0.882\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.003\n",
      "Precision: 0.500\n",
      "Accuracy: 0.034\n",
      "F1: 0.282\n",
      "Recall: 0.180\n",
      "Precision: 0.653\n",
      "\n",
      "50 1e-06\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=5 loss=2916.369873046875\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=15 loss=2004.2828369140625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=25 loss=1500.6927490234375\n",
      "iteration=30 loss=1357.379150390625\n",
      "iteration=35 loss=1238.488037109375\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=45 loss=1040.301513671875\n",
      "iteration=49 loss=980.8890380859375\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=5 loss=2583.005126953125\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=15 loss=1607.670166015625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=25 loss=1178.684326171875\n",
      "iteration=30 loss=1049.43798828125\n",
      "iteration=35 loss=943.6692504882812\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=45 loss=796.6639404296875\n",
      "iteration=49 loss=753.6226196289062\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=5 loss=2919.48681640625\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=15 loss=1888.6195068359375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=25 loss=1356.610107421875\n",
      "iteration=30 loss=1210.9149169921875\n",
      "iteration=35 loss=1102.93798828125\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=45 loss=933.1814575195312\n",
      "iteration=49 loss=880.9447021484375\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=5 loss=2788.410888671875\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=15 loss=1932.5836181640625\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=25 loss=1439.356689453125\n",
      "iteration=30 loss=1282.6077880859375\n",
      "iteration=35 loss=1157.9271240234375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=45 loss=963.78466796875\n",
      "iteration=49 loss=907.0369873046875\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=5 loss=2758.822509765625\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=15 loss=1876.363037109375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=25 loss=1435.83642578125\n",
      "iteration=30 loss=1276.5987548828125\n",
      "iteration=35 loss=1136.5814208984375\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=45 loss=943.5738525390625\n",
      "iteration=49 loss=889.4700927734375\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=5 loss=2742.236572265625\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=15 loss=1861.965087890625\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=25 loss=1439.5703125\n",
      "iteration=30 loss=1297.69677734375\n",
      "iteration=35 loss=1188.870849609375\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=45 loss=1006.1919555664062\n",
      "iteration=49 loss=944.5415649414062\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=5 loss=2792.9462890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=10 loss=2615.05419921875\n",
      "iteration=15 loss=1963.2392578125\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=25 loss=1584.427001953125\n",
      "iteration=30 loss=1429.028076171875\n",
      "iteration=35 loss=1297.962646484375\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=45 loss=1102.8558349609375\n",
      "iteration=49 loss=1042.1461181640625\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=5 loss=2825.556640625\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=15 loss=1837.097412109375\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=25 loss=1426.9168701171875\n",
      "iteration=30 loss=1270.780517578125\n",
      "iteration=35 loss=1140.966796875\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=45 loss=961.5462646484375\n",
      "iteration=49 loss=910.909912109375\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=5 loss=2969.663330078125\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=15 loss=1799.749755859375\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=25 loss=1356.2200927734375\n",
      "iteration=30 loss=1195.76513671875\n",
      "iteration=35 loss=1079.3731689453125\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=45 loss=902.2664794921875\n",
      "iteration=49 loss=850.6611328125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=5 loss=2562.093505859375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=15 loss=1560.3394775390625\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=25 loss=1059.6268310546875\n",
      "iteration=30 loss=952.5345458984375\n",
      "iteration=35 loss=881.6448974609375\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=45 loss=777.516357421875\n",
      "iteration=49 loss=746.052978515625\n",
      "Accuracy: 0.004\n",
      "F1: 0.041\n",
      "Recall: 0.023\n",
      "Precision: 0.186\n",
      "Accuracy: 0.025\n",
      "F1: 0.212\n",
      "Recall: 0.131\n",
      "Precision: 0.562\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.224\n",
      "Accuracy: 0.075\n",
      "F1: 0.521\n",
      "Recall: 0.395\n",
      "Precision: 0.764\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.224\n",
      "Accuracy: 0.027\n",
      "F1: 0.228\n",
      "Recall: 0.142\n",
      "Precision: 0.576\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.010\n",
      "F1: 0.101\n",
      "Recall: 0.055\n",
      "Precision: 0.613\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.041\n",
      "Recall: 0.023\n",
      "Precision: 0.186\n",
      "Accuracy: 0.015\n",
      "F1: 0.142\n",
      "Recall: 0.081\n",
      "Precision: 0.560\n",
      "\n",
      "Accuracy: 0.029\n",
      "F1: 0.137\n",
      "Recall: 0.154\n",
      "Precision: 0.123\n",
      "Accuracy: 0.121\n",
      "F1: 0.696\n",
      "Recall: 0.637\n",
      "Precision: 0.768\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.036\n",
      "Recall: 0.020\n",
      "Precision: 0.163\n",
      "Accuracy: 0.037\n",
      "F1: 0.294\n",
      "Recall: 0.195\n",
      "Precision: 0.598\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.051\n",
      "Recall: 0.029\n",
      "Precision: 0.208\n",
      "Accuracy: 0.061\n",
      "F1: 0.460\n",
      "Recall: 0.320\n",
      "Precision: 0.821\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.031\n",
      "Recall: 0.017\n",
      "Precision: 0.146\n",
      "Accuracy: 0.091\n",
      "F1: 0.621\n",
      "Recall: 0.480\n",
      "Precision: 0.882\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.003\n",
      "Precision: 0.500\n",
      "Accuracy: 0.034\n",
      "F1: 0.282\n",
      "Recall: 0.180\n",
      "Precision: 0.653\n",
      "\n",
      "50 1e-07\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=5 loss=2916.369873046875\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=15 loss=2004.2828369140625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=25 loss=1500.6927490234375\n",
      "iteration=30 loss=1357.379150390625\n",
      "iteration=35 loss=1238.488037109375\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=45 loss=1040.301513671875\n",
      "iteration=49 loss=980.8890380859375\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=5 loss=2583.005126953125\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=15 loss=1607.670166015625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=25 loss=1178.684326171875\n",
      "iteration=30 loss=1049.43798828125\n",
      "iteration=35 loss=943.6692504882812\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=45 loss=796.6639404296875\n",
      "iteration=49 loss=753.6226196289062\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=5 loss=2919.48681640625\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=15 loss=1888.6195068359375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=25 loss=1356.610107421875\n",
      "iteration=30 loss=1210.9149169921875\n",
      "iteration=35 loss=1102.93798828125\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=45 loss=933.1814575195312\n",
      "iteration=49 loss=880.9447021484375\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=5 loss=2788.410888671875\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=15 loss=1932.5836181640625\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=25 loss=1439.356689453125\n",
      "iteration=30 loss=1282.6077880859375\n",
      "iteration=35 loss=1157.9271240234375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=45 loss=963.78466796875\n",
      "iteration=49 loss=907.0369873046875\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=5 loss=2758.822509765625\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=15 loss=1876.363037109375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=25 loss=1435.83642578125\n",
      "iteration=30 loss=1276.5987548828125\n",
      "iteration=35 loss=1136.5814208984375\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=45 loss=943.5738525390625\n",
      "iteration=49 loss=889.4700927734375\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=5 loss=2742.236572265625\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=15 loss=1861.965087890625\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=25 loss=1439.5703125\n",
      "iteration=30 loss=1297.69677734375\n",
      "iteration=35 loss=1188.870849609375\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=45 loss=1006.1919555664062\n",
      "iteration=49 loss=944.5415649414062\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=5 loss=2792.9462890625\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=15 loss=1963.2392578125\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=25 loss=1584.427001953125\n",
      "iteration=30 loss=1429.028076171875\n",
      "iteration=35 loss=1297.962646484375\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=45 loss=1102.8558349609375\n",
      "iteration=49 loss=1042.1461181640625\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=5 loss=2825.556640625\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=15 loss=1837.097412109375\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=25 loss=1426.9168701171875\n",
      "iteration=30 loss=1270.780517578125\n",
      "iteration=35 loss=1140.966796875\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=45 loss=961.5462646484375\n",
      "iteration=49 loss=910.909912109375\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=5 loss=2969.663330078125\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=15 loss=1799.749755859375\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=25 loss=1356.2200927734375\n",
      "iteration=30 loss=1195.76513671875\n",
      "iteration=35 loss=1079.3731689453125\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=45 loss=902.2664794921875\n",
      "iteration=49 loss=850.6611328125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=5 loss=2562.093505859375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=15 loss=1560.3394775390625\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=25 loss=1059.6268310546875\n",
      "iteration=30 loss=952.5345458984375\n",
      "iteration=35 loss=881.6448974609375\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=45 loss=777.516357421875\n",
      "iteration=49 loss=746.052978515625\n",
      "Accuracy: 0.004\n",
      "F1: 0.041\n",
      "Recall: 0.023\n",
      "Precision: 0.186\n",
      "Accuracy: 0.025\n",
      "F1: 0.212\n",
      "Recall: 0.131\n",
      "Precision: 0.562\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.224\n",
      "Accuracy: 0.075\n",
      "F1: 0.521\n",
      "Recall: 0.395\n",
      "Precision: 0.764\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.056\n",
      "Recall: 0.032\n",
      "Precision: 0.224\n",
      "Accuracy: 0.027\n",
      "F1: 0.228\n",
      "Recall: 0.142\n",
      "Precision: 0.576\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.010\n",
      "F1: 0.101\n",
      "Recall: 0.055\n",
      "Precision: 0.613\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.041\n",
      "Recall: 0.023\n",
      "Precision: 0.186\n",
      "Accuracy: 0.015\n",
      "F1: 0.142\n",
      "Recall: 0.081\n",
      "Precision: 0.560\n",
      "\n",
      "Accuracy: 0.029\n",
      "F1: 0.137\n",
      "Recall: 0.154\n",
      "Precision: 0.123\n",
      "Accuracy: 0.121\n",
      "F1: 0.696\n",
      "Recall: 0.637\n",
      "Precision: 0.768\n",
      "\n",
      "Accuracy: 0.004\n",
      "F1: 0.036\n",
      "Recall: 0.020\n",
      "Precision: 0.163\n",
      "Accuracy: 0.037\n",
      "F1: 0.294\n",
      "Recall: 0.195\n",
      "Precision: 0.598\n",
      "\n",
      "Accuracy: 0.006\n",
      "F1: 0.051\n",
      "Recall: 0.029\n",
      "Precision: 0.208\n",
      "Accuracy: 0.061\n",
      "F1: 0.460\n",
      "Recall: 0.320\n",
      "Precision: 0.821\n",
      "\n",
      "Accuracy: 0.003\n",
      "F1: 0.031\n",
      "Recall: 0.017\n",
      "Precision: 0.146\n",
      "Accuracy: 0.091\n",
      "F1: 0.621\n",
      "Recall: 0.480\n",
      "Precision: 0.882\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.003\n",
      "Precision: 0.500\n",
      "Accuracy: 0.034\n",
      "F1: 0.282\n",
      "Recall: 0.180\n",
      "Precision: 0.653\n",
      "\n",
      "100 0.0001\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=30 loss=1357.379150390625\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=50 loss=967.42822265625\n",
      "iteration=60 loss=856.2899169921875\n",
      "iteration=70 loss=777.806396484375\n",
      "iteration=80 loss=715.9603881835938\n",
      "iteration=90 loss=664.8099975585938\n",
      "iteration=99 loss=626.4541015625\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=20 loss=1337.7255859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=30 loss=1049.43798828125\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=50 loss=743.9108276367188\n",
      "iteration=60 loss=666.1441040039062\n",
      "iteration=70 loss=610.3531494140625\n",
      "iteration=80 loss=567.1578979492188\n",
      "iteration=90 loss=537.26806640625\n",
      "iteration=99 loss=515.082275390625\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=30 loss=1210.9149169921875\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=50 loss=869.088134765625\n",
      "iteration=60 loss=770.5421142578125\n",
      "iteration=70 loss=696.7645263671875\n",
      "iteration=80 loss=642.19580078125\n",
      "iteration=90 loss=598.3699340820312\n",
      "iteration=99 loss=565.8165283203125\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=30 loss=1282.6077880859375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=50 loss=894.216552734375\n",
      "iteration=60 loss=789.1647338867188\n",
      "iteration=70 loss=712.5748901367188\n",
      "iteration=80 loss=657.9627685546875\n",
      "iteration=90 loss=617.5780029296875\n",
      "iteration=99 loss=587.6024780273438\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=30 loss=1276.5987548828125\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=50 loss=877.371826171875\n",
      "iteration=60 loss=778.4125366210938\n",
      "iteration=70 loss=707.5772094726562\n",
      "iteration=80 loss=656.8345947265625\n",
      "iteration=90 loss=616.321044921875\n",
      "iteration=99 loss=586.3966064453125\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=30 loss=1297.69677734375\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=50 loss=929.9364624023438\n",
      "iteration=60 loss=804.29931640625\n",
      "iteration=70 loss=710.1395874023438\n",
      "iteration=80 loss=638.6434326171875\n",
      "iteration=90 loss=583.5235595703125\n",
      "iteration=99 loss=544.1700439453125\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=30 loss=1429.028076171875\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=50 loss=1028.1865234375\n",
      "iteration=60 loss=909.41259765625\n",
      "iteration=70 loss=819.3133544921875\n",
      "iteration=80 loss=752.3092041015625\n",
      "iteration=90 loss=699.2025146484375\n",
      "iteration=99 loss=659.0181274414062\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=30 loss=1270.780517578125\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=50 loss=899.578857421875\n",
      "iteration=60 loss=806.4021606445312\n",
      "iteration=70 loss=740.3987426757812\n",
      "iteration=80 loss=693.2643432617188\n",
      "iteration=90 loss=653.9887084960938\n",
      "iteration=99 loss=623.6529541015625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=30 loss=1195.76513671875\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=50 loss=839.1048583984375\n",
      "iteration=60 loss=744.3742065429688\n",
      "iteration=70 loss=675.487060546875\n",
      "iteration=80 loss=622.3905029296875\n",
      "iteration=90 loss=580.7379760742188\n",
      "iteration=99 loss=550.1382446289062\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=30 loss=952.5345458984375\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=50 loss=738.7559204101562\n",
      "iteration=60 loss=676.3593139648438\n",
      "iteration=70 loss=627.4212646484375\n",
      "iteration=80 loss=587.8203735351562\n",
      "iteration=90 loss=555.3527221679688\n",
      "iteration=99 loss=531.1490478515625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.075\n",
      "F1: 0.518\n",
      "Recall: 0.395\n",
      "Precision: 0.751\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.061\n",
      "F1: 0.463\n",
      "Recall: 0.323\n",
      "Precision: 0.822\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.056\n",
      "F1: 0.418\n",
      "Recall: 0.297\n",
      "Precision: 0.708\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.008\n",
      "F1: 0.082\n",
      "Recall: 0.044\n",
      "Precision: 0.652\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.023\n",
      "F1: 0.200\n",
      "Recall: 0.122\n",
      "Precision: 0.560\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.041\n",
      "F1: 0.346\n",
      "Recall: 0.215\n",
      "Precision: 0.881\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.003\n",
      "Precision: 0.500\n",
      "Accuracy: 0.065\n",
      "F1: 0.464\n",
      "Recall: 0.343\n",
      "Precision: 0.715\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.082\n",
      "F1: 0.560\n",
      "Recall: 0.430\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.051\n",
      "F1: 0.415\n",
      "Recall: 0.270\n",
      "Precision: 0.894\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.061\n",
      "F1: 0.458\n",
      "Recall: 0.323\n",
      "Precision: 0.787\n",
      "\n",
      "100 1e-05\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=30 loss=1357.379150390625\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=50 loss=967.42822265625\n",
      "iteration=60 loss=856.2899169921875\n",
      "iteration=70 loss=777.806396484375\n",
      "iteration=80 loss=715.9603881835938\n",
      "iteration=90 loss=664.8099975585938\n",
      "iteration=99 loss=626.4541015625\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=30 loss=1049.43798828125\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=50 loss=743.9108276367188\n",
      "iteration=60 loss=666.1441040039062\n",
      "iteration=70 loss=610.3531494140625\n",
      "iteration=80 loss=567.1578979492188\n",
      "iteration=90 loss=537.26806640625\n",
      "iteration=99 loss=515.082275390625\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=30 loss=1210.9149169921875\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=50 loss=869.088134765625\n",
      "iteration=60 loss=770.5421142578125\n",
      "iteration=70 loss=696.7645263671875\n",
      "iteration=80 loss=642.19580078125\n",
      "iteration=90 loss=598.3699340820312\n",
      "iteration=99 loss=565.8165283203125\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=30 loss=1282.6077880859375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=50 loss=894.216552734375\n",
      "iteration=60 loss=789.1647338867188\n",
      "iteration=70 loss=712.5748901367188\n",
      "iteration=80 loss=657.9627685546875\n",
      "iteration=90 loss=617.5780029296875\n",
      "iteration=99 loss=587.6024780273438\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=30 loss=1276.5987548828125\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=50 loss=877.371826171875\n",
      "iteration=60 loss=778.4125366210938\n",
      "iteration=70 loss=707.5772094726562\n",
      "iteration=80 loss=656.8345947265625\n",
      "iteration=90 loss=616.321044921875\n",
      "iteration=99 loss=586.3966064453125\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=30 loss=1297.69677734375\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=50 loss=929.9364624023438\n",
      "iteration=60 loss=804.29931640625\n",
      "iteration=70 loss=710.1395874023438\n",
      "iteration=80 loss=638.6434326171875\n",
      "iteration=90 loss=583.5235595703125\n",
      "iteration=99 loss=544.1700439453125\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=30 loss=1429.028076171875\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=50 loss=1028.1865234375\n",
      "iteration=60 loss=909.41259765625\n",
      "iteration=70 loss=819.3133544921875\n",
      "iteration=80 loss=752.3092041015625\n",
      "iteration=90 loss=699.2025146484375\n",
      "iteration=99 loss=659.0181274414062\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=30 loss=1270.780517578125\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=50 loss=899.578857421875\n",
      "iteration=60 loss=806.4021606445312\n",
      "iteration=70 loss=740.3987426757812\n",
      "iteration=80 loss=693.2643432617188\n",
      "iteration=90 loss=653.9887084960938\n",
      "iteration=99 loss=623.6529541015625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=30 loss=1195.76513671875\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=50 loss=839.1048583984375\n",
      "iteration=60 loss=744.3742065429688\n",
      "iteration=70 loss=675.487060546875\n",
      "iteration=80 loss=622.3905029296875\n",
      "iteration=90 loss=580.7379760742188\n",
      "iteration=99 loss=550.1382446289062\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=20 loss=1219.568603515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=30 loss=952.5345458984375\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=50 loss=738.7559204101562\n",
      "iteration=60 loss=676.3593139648438\n",
      "iteration=70 loss=627.4212646484375\n",
      "iteration=80 loss=587.8203735351562\n",
      "iteration=90 loss=555.3527221679688\n",
      "iteration=99 loss=531.1490478515625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.075\n",
      "F1: 0.518\n",
      "Recall: 0.395\n",
      "Precision: 0.751\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.061\n",
      "F1: 0.463\n",
      "Recall: 0.323\n",
      "Precision: 0.822\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.056\n",
      "F1: 0.418\n",
      "Recall: 0.297\n",
      "Precision: 0.708\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.008\n",
      "F1: 0.082\n",
      "Recall: 0.044\n",
      "Precision: 0.652\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.023\n",
      "F1: 0.200\n",
      "Recall: 0.122\n",
      "Precision: 0.560\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.041\n",
      "F1: 0.346\n",
      "Recall: 0.215\n",
      "Precision: 0.881\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.003\n",
      "Precision: 0.500\n",
      "Accuracy: 0.065\n",
      "F1: 0.464\n",
      "Recall: 0.343\n",
      "Precision: 0.715\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.082\n",
      "F1: 0.560\n",
      "Recall: 0.430\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.051\n",
      "F1: 0.415\n",
      "Recall: 0.270\n",
      "Precision: 0.894\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.061\n",
      "F1: 0.458\n",
      "Recall: 0.323\n",
      "Precision: 0.787\n",
      "\n",
      "100 1e-06\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=30 loss=1357.379150390625\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=50 loss=967.42822265625\n",
      "iteration=60 loss=856.2899169921875\n",
      "iteration=70 loss=777.806396484375\n",
      "iteration=80 loss=715.9603881835938\n",
      "iteration=90 loss=664.8099975585938\n",
      "iteration=99 loss=626.4541015625\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=30 loss=1049.43798828125\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=50 loss=743.9108276367188\n",
      "iteration=60 loss=666.1441040039062\n",
      "iteration=70 loss=610.3531494140625\n",
      "iteration=80 loss=567.1578979492188\n",
      "iteration=90 loss=537.26806640625\n",
      "iteration=99 loss=515.082275390625\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=30 loss=1210.9149169921875\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=50 loss=869.088134765625\n",
      "iteration=60 loss=770.5421142578125\n",
      "iteration=70 loss=696.7645263671875\n",
      "iteration=80 loss=642.19580078125\n",
      "iteration=90 loss=598.3699340820312\n",
      "iteration=99 loss=565.8165283203125\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=30 loss=1282.6077880859375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=50 loss=894.216552734375\n",
      "iteration=60 loss=789.1647338867188\n",
      "iteration=70 loss=712.5748901367188\n",
      "iteration=80 loss=657.9627685546875\n",
      "iteration=90 loss=617.5780029296875\n",
      "iteration=99 loss=587.6024780273438\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=30 loss=1276.5987548828125\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=50 loss=877.371826171875\n",
      "iteration=60 loss=778.4125366210938\n",
      "iteration=70 loss=707.5772094726562\n",
      "iteration=80 loss=656.8345947265625\n",
      "iteration=90 loss=616.321044921875\n",
      "iteration=99 loss=586.3966064453125\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=30 loss=1297.69677734375\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=50 loss=929.9364624023438\n",
      "iteration=60 loss=804.29931640625\n",
      "iteration=70 loss=710.1395874023438\n",
      "iteration=80 loss=638.6434326171875\n",
      "iteration=90 loss=583.5235595703125\n",
      "iteration=99 loss=544.1700439453125\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=30 loss=1429.028076171875\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=50 loss=1028.1865234375\n",
      "iteration=60 loss=909.41259765625\n",
      "iteration=70 loss=819.3133544921875\n",
      "iteration=80 loss=752.3092041015625\n",
      "iteration=90 loss=699.2025146484375\n",
      "iteration=99 loss=659.0181274414062\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=30 loss=1270.780517578125\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=50 loss=899.578857421875\n",
      "iteration=60 loss=806.4021606445312\n",
      "iteration=70 loss=740.3987426757812\n",
      "iteration=80 loss=693.2643432617188\n",
      "iteration=90 loss=653.9887084960938\n",
      "iteration=99 loss=623.6529541015625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=30 loss=1195.76513671875\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=50 loss=839.1048583984375\n",
      "iteration=60 loss=744.3742065429688\n",
      "iteration=70 loss=675.487060546875\n",
      "iteration=80 loss=622.3905029296875\n",
      "iteration=90 loss=580.7379760742188\n",
      "iteration=99 loss=550.1382446289062\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=30 loss=952.5345458984375\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=50 loss=738.7559204101562\n",
      "iteration=60 loss=676.3593139648438\n",
      "iteration=70 loss=627.4212646484375\n",
      "iteration=80 loss=587.8203735351562\n",
      "iteration=90 loss=555.3527221679688\n",
      "iteration=99 loss=531.1490478515625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.075\n",
      "F1: 0.518\n",
      "Recall: 0.395\n",
      "Precision: 0.751\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.061\n",
      "F1: 0.463\n",
      "Recall: 0.323\n",
      "Precision: 0.822\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.056\n",
      "F1: 0.418\n",
      "Recall: 0.297\n",
      "Precision: 0.708\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.008\n",
      "F1: 0.082\n",
      "Recall: 0.044\n",
      "Precision: 0.652\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.023\n",
      "F1: 0.200\n",
      "Recall: 0.122\n",
      "Precision: 0.560\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.041\n",
      "F1: 0.346\n",
      "Recall: 0.215\n",
      "Precision: 0.881\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.003\n",
      "Precision: 0.500\n",
      "Accuracy: 0.065\n",
      "F1: 0.464\n",
      "Recall: 0.343\n",
      "Precision: 0.715\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.082\n",
      "F1: 0.560\n",
      "Recall: 0.430\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.051\n",
      "F1: 0.415\n",
      "Recall: 0.270\n",
      "Precision: 0.894\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.061\n",
      "F1: 0.458\n",
      "Recall: 0.323\n",
      "Precision: 0.787\n",
      "\n",
      "100 1e-07\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=10 loss=2467.298095703125\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=30 loss=1357.379150390625\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=50 loss=967.42822265625\n",
      "iteration=60 loss=856.2899169921875\n",
      "iteration=70 loss=777.806396484375\n",
      "iteration=80 loss=715.9603881835938\n",
      "iteration=90 loss=664.8099975585938\n",
      "iteration=99 loss=626.4541015625\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=10 loss=2195.864013671875\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=30 loss=1049.43798828125\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=50 loss=743.9108276367188\n",
      "iteration=60 loss=666.1441040039062\n",
      "iteration=70 loss=610.3531494140625\n",
      "iteration=80 loss=567.1578979492188\n",
      "iteration=90 loss=537.26806640625\n",
      "iteration=99 loss=515.082275390625\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=10 loss=2551.470458984375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=30 loss=1210.9149169921875\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=50 loss=869.088134765625\n",
      "iteration=60 loss=770.5421142578125\n",
      "iteration=70 loss=696.7645263671875\n",
      "iteration=80 loss=642.19580078125\n",
      "iteration=90 loss=598.3699340820312\n",
      "iteration=99 loss=565.8165283203125\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=10 loss=2439.8662109375\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=30 loss=1282.6077880859375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=50 loss=894.216552734375\n",
      "iteration=60 loss=789.1647338867188\n",
      "iteration=70 loss=712.5748901367188\n",
      "iteration=80 loss=657.9627685546875\n",
      "iteration=90 loss=617.5780029296875\n",
      "iteration=99 loss=587.6024780273438\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=10 loss=2515.5615234375\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=30 loss=1276.5987548828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=40 loss=1028.272216796875\n",
      "iteration=50 loss=877.371826171875\n",
      "iteration=60 loss=778.4125366210938\n",
      "iteration=70 loss=707.5772094726562\n",
      "iteration=80 loss=656.8345947265625\n",
      "iteration=90 loss=616.321044921875\n",
      "iteration=99 loss=586.3966064453125\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=10 loss=2431.718505859375\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=30 loss=1297.69677734375\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=50 loss=929.9364624023438\n",
      "iteration=60 loss=804.29931640625\n",
      "iteration=70 loss=710.1395874023438\n",
      "iteration=80 loss=638.6434326171875\n",
      "iteration=90 loss=583.5235595703125\n",
      "iteration=99 loss=544.1700439453125\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=10 loss=2615.05419921875\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=30 loss=1429.028076171875\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=50 loss=1028.1865234375\n",
      "iteration=60 loss=909.41259765625\n",
      "iteration=70 loss=819.3133544921875\n",
      "iteration=80 loss=752.3092041015625\n",
      "iteration=90 loss=699.2025146484375\n",
      "iteration=99 loss=659.0181274414062\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=10 loss=2634.830810546875\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=30 loss=1270.780517578125\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=50 loss=899.578857421875\n",
      "iteration=60 loss=806.4021606445312\n",
      "iteration=70 loss=740.3987426757812\n",
      "iteration=80 loss=693.2643432617188\n",
      "iteration=90 loss=653.9887084960938\n",
      "iteration=99 loss=623.6529541015625\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=10 loss=2543.1083984375\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=30 loss=1195.76513671875\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=50 loss=839.1048583984375\n",
      "iteration=60 loss=744.3742065429688\n",
      "iteration=70 loss=675.487060546875\n",
      "iteration=80 loss=622.3905029296875\n",
      "iteration=90 loss=580.7379760742188\n",
      "iteration=99 loss=550.1382446289062\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=10 loss=2191.009521484375\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=30 loss=952.5345458984375\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=50 loss=738.7559204101562\n",
      "iteration=60 loss=676.3593139648438\n",
      "iteration=70 loss=627.4212646484375\n",
      "iteration=80 loss=587.8203735351562\n",
      "iteration=90 loss=555.3527221679688\n",
      "iteration=99 loss=531.1490478515625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.075\n",
      "F1: 0.518\n",
      "Recall: 0.395\n",
      "Precision: 0.751\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.061\n",
      "F1: 0.463\n",
      "Recall: 0.323\n",
      "Precision: 0.822\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.056\n",
      "F1: 0.418\n",
      "Recall: 0.297\n",
      "Precision: 0.708\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.008\n",
      "F1: 0.082\n",
      "Recall: 0.044\n",
      "Precision: 0.652\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.023\n",
      "F1: 0.200\n",
      "Recall: 0.122\n",
      "Precision: 0.560\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.041\n",
      "F1: 0.346\n",
      "Recall: 0.215\n",
      "Precision: 0.881\n",
      "\n",
      "Accuracy: 0.001\n",
      "F1: 0.006\n",
      "Recall: 0.003\n",
      "Precision: 0.500\n",
      "Accuracy: 0.065\n",
      "F1: 0.464\n",
      "Recall: 0.343\n",
      "Precision: 0.715\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.082\n",
      "F1: 0.560\n",
      "Recall: 0.430\n",
      "Precision: 0.800\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.051\n",
      "F1: 0.415\n",
      "Recall: 0.270\n",
      "Precision: 0.894\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.061\n",
      "F1: 0.458\n",
      "Recall: 0.323\n",
      "Precision: 0.787\n",
      "\n",
      "200 0.0001\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=60 loss=856.2899169921875\n",
      "iteration=80 loss=715.9603881835938\n",
      "iteration=100 loss=622.6627807617188\n",
      "iteration=120 loss=557.0338745117188\n",
      "iteration=140 loss=507.1540222167969\n",
      "iteration=160 loss=468.8624267578125\n",
      "iteration=180 loss=439.07403564453125\n",
      "iteration=199 loss=416.6258544921875\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=60 loss=666.1441040039062\n",
      "iteration=80 loss=567.1578979492188\n",
      "iteration=100 loss=512.8164672851562\n",
      "iteration=120 loss=474.1331481933594\n",
      "iteration=140 loss=445.2003173828125\n",
      "iteration=160 loss=422.91741943359375\n",
      "iteration=180 loss=405.0823669433594\n",
      "iteration=199 loss=391.085205078125\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=60 loss=770.5421142578125\n",
      "iteration=80 loss=642.19580078125\n",
      "iteration=100 loss=562.5022583007812\n",
      "iteration=120 loss=506.714111328125\n",
      "iteration=140 loss=466.47344970703125\n",
      "iteration=160 loss=436.9141845703125\n",
      "iteration=180 loss=414.7760314941406\n",
      "iteration=199 loss=399.0871276855469\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=60 loss=789.1647338867188\n",
      "iteration=80 loss=657.9627685546875\n",
      "iteration=100 loss=584.5404052734375\n",
      "iteration=120 loss=532.5022583007812\n",
      "iteration=140 loss=493.7177734375\n",
      "iteration=160 loss=463.9582824707031\n",
      "iteration=180 loss=440.70599365234375\n",
      "iteration=199 loss=423.1640625\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=60 loss=778.4125366210938\n",
      "iteration=80 loss=656.8345947265625\n",
      "iteration=100 loss=583.4747924804688\n",
      "iteration=120 loss=534.490966796875\n",
      "iteration=140 loss=499.5630187988281\n",
      "iteration=160 loss=473.03143310546875\n",
      "iteration=180 loss=452.0975341796875\n",
      "iteration=199 loss=435.84881591796875\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=60 loss=804.29931640625\n",
      "iteration=80 loss=638.6434326171875\n",
      "iteration=100 loss=540.2887573242188\n",
      "iteration=120 loss=478.44183349609375\n",
      "iteration=140 loss=442.7610168457031\n",
      "iteration=160 loss=417.161376953125\n",
      "iteration=180 loss=396.869140625\n",
      "iteration=199 loss=1562.33056640625\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=60 loss=909.41259765625\n",
      "iteration=80 loss=752.3092041015625\n",
      "iteration=100 loss=654.9193115234375\n",
      "iteration=120 loss=588.0122680664062\n",
      "iteration=140 loss=538.7683715820312\n",
      "iteration=160 loss=500.2598571777344\n",
      "iteration=180 loss=469.50384521484375\n",
      "iteration=199 loss=446.2757568359375\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=60 loss=806.4021606445312\n",
      "iteration=80 loss=693.2643432617188\n",
      "iteration=100 loss=620.5284423828125\n",
      "iteration=120 loss=566.2553100585938\n",
      "iteration=140 loss=524.3209838867188\n",
      "iteration=160 loss=491.2142028808594\n",
      "iteration=180 loss=463.6517333984375\n",
      "iteration=199 loss=441.34881591796875\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=60 loss=744.3742065429688\n",
      "iteration=80 loss=622.3905029296875\n",
      "iteration=100 loss=547.0148315429688\n",
      "iteration=120 loss=493.99005126953125\n",
      "iteration=140 loss=455.2908020019531\n",
      "iteration=160 loss=428.498046875\n",
      "iteration=180 loss=407.9398193359375\n",
      "iteration=199 loss=392.41729736328125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=60 loss=676.3593139648438\n",
      "iteration=80 loss=587.8203735351562\n",
      "iteration=100 loss=528.6596069335938\n",
      "iteration=120 loss=486.51727294921875\n",
      "iteration=140 loss=454.3327941894531\n",
      "iteration=160 loss=429.13232421875\n",
      "iteration=180 loss=409.0931701660156\n",
      "iteration=199 loss=393.66705322265625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.036\n",
      "F1: 0.316\n",
      "Recall: 0.192\n",
      "Precision: 0.892\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.069\n",
      "F1: 0.506\n",
      "Recall: 0.363\n",
      "Precision: 0.833\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.058\n",
      "F1: 0.445\n",
      "Recall: 0.305\n",
      "Precision: 0.820\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.115\n",
      "F1: 0.696\n",
      "Recall: 0.605\n",
      "Precision: 0.819\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.064\n",
      "F1: 0.458\n",
      "Recall: 0.337\n",
      "Precision: 0.712\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.033\n",
      "Recall: 0.026\n",
      "Precision: 0.046\n",
      "Accuracy: 0.050\n",
      "F1: 0.350\n",
      "Recall: 0.265\n",
      "Precision: 0.517\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.096\n",
      "F1: 0.616\n",
      "Recall: 0.506\n",
      "Precision: 0.787\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.111\n",
      "F1: 0.690\n",
      "Recall: 0.584\n",
      "Precision: 0.841\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.430\n",
      "Recall: 0.282\n",
      "Precision: 0.907\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.110\n",
      "F1: 0.693\n",
      "Recall: 0.581\n",
      "Precision: 0.858\n",
      "\n",
      "200 1e-05\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=60 loss=856.2899169921875\n",
      "iteration=80 loss=715.9603881835938\n",
      "iteration=100 loss=622.6627807617188\n",
      "iteration=120 loss=557.0338745117188\n",
      "iteration=140 loss=507.1540222167969\n",
      "iteration=160 loss=468.8624267578125\n",
      "iteration=180 loss=439.07403564453125\n",
      "iteration=199 loss=416.6258544921875\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=60 loss=666.1441040039062\n",
      "iteration=80 loss=567.1578979492188\n",
      "iteration=100 loss=512.8164672851562\n",
      "iteration=120 loss=474.1331481933594\n",
      "iteration=140 loss=445.2003173828125\n",
      "iteration=160 loss=422.91741943359375\n",
      "iteration=180 loss=405.0823669433594\n",
      "iteration=199 loss=391.085205078125\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=60 loss=770.5421142578125\n",
      "iteration=80 loss=642.19580078125\n",
      "iteration=100 loss=562.5022583007812\n",
      "iteration=120 loss=506.714111328125\n",
      "iteration=140 loss=466.47344970703125\n",
      "iteration=160 loss=436.9141845703125\n",
      "iteration=180 loss=414.7760314941406\n",
      "iteration=199 loss=399.0871276855469\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=60 loss=789.1647338867188\n",
      "iteration=80 loss=657.9627685546875\n",
      "iteration=100 loss=584.5404052734375\n",
      "iteration=120 loss=532.5022583007812\n",
      "iteration=140 loss=493.7177734375\n",
      "iteration=160 loss=463.9582824707031\n",
      "iteration=180 loss=440.70599365234375\n",
      "iteration=199 loss=423.1640625\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=60 loss=778.4125366210938\n",
      "iteration=80 loss=656.8345947265625\n",
      "iteration=100 loss=583.4747924804688\n",
      "iteration=120 loss=534.490966796875\n",
      "iteration=140 loss=499.5630187988281\n",
      "iteration=160 loss=473.03143310546875\n",
      "iteration=180 loss=452.0975341796875\n",
      "iteration=199 loss=435.84881591796875\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=60 loss=804.29931640625\n",
      "iteration=80 loss=638.6434326171875\n",
      "iteration=100 loss=540.2887573242188\n",
      "iteration=120 loss=478.44183349609375\n",
      "iteration=140 loss=442.7610168457031\n",
      "iteration=160 loss=417.161376953125\n",
      "iteration=180 loss=396.869140625\n",
      "iteration=199 loss=1562.33056640625\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=60 loss=909.41259765625\n",
      "iteration=80 loss=752.3092041015625\n",
      "iteration=100 loss=654.9193115234375\n",
      "iteration=120 loss=588.0122680664062\n",
      "iteration=140 loss=538.7683715820312\n",
      "iteration=160 loss=500.2598571777344\n",
      "iteration=180 loss=469.50384521484375\n",
      "iteration=199 loss=446.2757568359375\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=60 loss=806.4021606445312\n",
      "iteration=80 loss=693.2643432617188\n",
      "iteration=100 loss=620.5284423828125\n",
      "iteration=120 loss=566.2553100585938\n",
      "iteration=140 loss=524.3209838867188\n",
      "iteration=160 loss=491.2142028808594\n",
      "iteration=180 loss=463.6517333984375\n",
      "iteration=199 loss=441.34881591796875\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=60 loss=744.3742065429688\n",
      "iteration=80 loss=622.3905029296875\n",
      "iteration=100 loss=547.0148315429688\n",
      "iteration=120 loss=493.99005126953125\n",
      "iteration=140 loss=455.2908020019531\n",
      "iteration=160 loss=428.498046875\n",
      "iteration=180 loss=407.9398193359375\n",
      "iteration=199 loss=392.41729736328125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=60 loss=676.3593139648438\n",
      "iteration=80 loss=587.8203735351562\n",
      "iteration=100 loss=528.6596069335938\n",
      "iteration=120 loss=486.51727294921875\n",
      "iteration=140 loss=454.3327941894531\n",
      "iteration=160 loss=429.13232421875\n",
      "iteration=180 loss=409.0931701660156\n",
      "iteration=199 loss=393.66705322265625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.036\n",
      "F1: 0.316\n",
      "Recall: 0.192\n",
      "Precision: 0.892\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.069\n",
      "F1: 0.506\n",
      "Recall: 0.363\n",
      "Precision: 0.833\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.058\n",
      "F1: 0.445\n",
      "Recall: 0.305\n",
      "Precision: 0.820\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.115\n",
      "F1: 0.696\n",
      "Recall: 0.605\n",
      "Precision: 0.819\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.064\n",
      "F1: 0.458\n",
      "Recall: 0.337\n",
      "Precision: 0.712\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.033\n",
      "Recall: 0.026\n",
      "Precision: 0.046\n",
      "Accuracy: 0.050\n",
      "F1: 0.350\n",
      "Recall: 0.265\n",
      "Precision: 0.517\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.096\n",
      "F1: 0.616\n",
      "Recall: 0.506\n",
      "Precision: 0.787\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.111\n",
      "F1: 0.690\n",
      "Recall: 0.584\n",
      "Precision: 0.841\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.430\n",
      "Recall: 0.282\n",
      "Precision: 0.907\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.110\n",
      "F1: 0.693\n",
      "Recall: 0.581\n",
      "Precision: 0.858\n",
      "\n",
      "200 1e-06\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=60 loss=856.2899169921875\n",
      "iteration=80 loss=715.9603881835938\n",
      "iteration=100 loss=622.6627807617188\n",
      "iteration=120 loss=557.0338745117188\n",
      "iteration=140 loss=507.1540222167969\n",
      "iteration=160 loss=468.8624267578125\n",
      "iteration=180 loss=439.07403564453125\n",
      "iteration=199 loss=416.6258544921875\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=60 loss=666.1441040039062\n",
      "iteration=80 loss=567.1578979492188\n",
      "iteration=100 loss=512.8164672851562\n",
      "iteration=120 loss=474.1331481933594\n",
      "iteration=140 loss=445.2003173828125\n",
      "iteration=160 loss=422.91741943359375\n",
      "iteration=180 loss=405.0823669433594\n",
      "iteration=199 loss=391.085205078125\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=60 loss=770.5421142578125\n",
      "iteration=80 loss=642.19580078125\n",
      "iteration=100 loss=562.5022583007812\n",
      "iteration=120 loss=506.714111328125\n",
      "iteration=140 loss=466.47344970703125\n",
      "iteration=160 loss=436.9141845703125\n",
      "iteration=180 loss=414.7760314941406\n",
      "iteration=199 loss=399.0871276855469\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=60 loss=789.1647338867188\n",
      "iteration=80 loss=657.9627685546875\n",
      "iteration=100 loss=584.5404052734375\n",
      "iteration=120 loss=532.5022583007812\n",
      "iteration=140 loss=493.7177734375\n",
      "iteration=160 loss=463.9582824707031\n",
      "iteration=180 loss=440.70599365234375\n",
      "iteration=199 loss=423.1640625\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=60 loss=778.4125366210938\n",
      "iteration=80 loss=656.8345947265625\n",
      "iteration=100 loss=583.4747924804688\n",
      "iteration=120 loss=534.490966796875\n",
      "iteration=140 loss=499.5630187988281\n",
      "iteration=160 loss=473.03143310546875\n",
      "iteration=180 loss=452.0975341796875\n",
      "iteration=199 loss=435.84881591796875\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=60 loss=804.29931640625\n",
      "iteration=80 loss=638.6434326171875\n",
      "iteration=100 loss=540.2887573242188\n",
      "iteration=120 loss=478.44183349609375\n",
      "iteration=140 loss=442.7610168457031\n",
      "iteration=160 loss=417.161376953125\n",
      "iteration=180 loss=396.869140625\n",
      "iteration=199 loss=1562.33056640625\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=60 loss=909.41259765625\n",
      "iteration=80 loss=752.3092041015625\n",
      "iteration=100 loss=654.9193115234375\n",
      "iteration=120 loss=588.0122680664062\n",
      "iteration=140 loss=538.7683715820312\n",
      "iteration=160 loss=500.2598571777344\n",
      "iteration=180 loss=469.50384521484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=199 loss=446.2757568359375\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=60 loss=806.4021606445312\n",
      "iteration=80 loss=693.2643432617188\n",
      "iteration=100 loss=620.5284423828125\n",
      "iteration=120 loss=566.2553100585938\n",
      "iteration=140 loss=524.3209838867188\n",
      "iteration=160 loss=491.2142028808594\n",
      "iteration=180 loss=463.6517333984375\n",
      "iteration=199 loss=441.34881591796875\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=60 loss=744.3742065429688\n",
      "iteration=80 loss=622.3905029296875\n",
      "iteration=100 loss=547.0148315429688\n",
      "iteration=120 loss=493.99005126953125\n",
      "iteration=140 loss=455.2908020019531\n",
      "iteration=160 loss=428.498046875\n",
      "iteration=180 loss=407.9398193359375\n",
      "iteration=199 loss=392.41729736328125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=60 loss=676.3593139648438\n",
      "iteration=80 loss=587.8203735351562\n",
      "iteration=100 loss=528.6596069335938\n",
      "iteration=120 loss=486.51727294921875\n",
      "iteration=140 loss=454.3327941894531\n",
      "iteration=160 loss=429.13232421875\n",
      "iteration=180 loss=409.0931701660156\n",
      "iteration=199 loss=393.66705322265625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.036\n",
      "F1: 0.316\n",
      "Recall: 0.192\n",
      "Precision: 0.892\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.069\n",
      "F1: 0.506\n",
      "Recall: 0.363\n",
      "Precision: 0.833\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.058\n",
      "F1: 0.445\n",
      "Recall: 0.305\n",
      "Precision: 0.820\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.115\n",
      "F1: 0.696\n",
      "Recall: 0.605\n",
      "Precision: 0.819\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.064\n",
      "F1: 0.458\n",
      "Recall: 0.337\n",
      "Precision: 0.712\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.033\n",
      "Recall: 0.026\n",
      "Precision: 0.046\n",
      "Accuracy: 0.050\n",
      "F1: 0.350\n",
      "Recall: 0.265\n",
      "Precision: 0.517\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.096\n",
      "F1: 0.616\n",
      "Recall: 0.506\n",
      "Precision: 0.787\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.111\n",
      "F1: 0.690\n",
      "Recall: 0.584\n",
      "Precision: 0.841\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.430\n",
      "Recall: 0.282\n",
      "Precision: 0.907\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.110\n",
      "F1: 0.693\n",
      "Recall: 0.581\n",
      "Precision: 0.858\n",
      "\n",
      "200 1e-07\n",
      "0\n",
      "iteration=0 loss=5474.5625\n",
      "iteration=20 loss=1671.983642578125\n",
      "iteration=40 loss=1130.2415771484375\n",
      "iteration=60 loss=856.2899169921875\n",
      "iteration=80 loss=715.9603881835938\n",
      "iteration=100 loss=622.6627807617188\n",
      "iteration=120 loss=557.0338745117188\n",
      "iteration=140 loss=507.1540222167969\n",
      "iteration=160 loss=468.8624267578125\n",
      "iteration=180 loss=439.07403564453125\n",
      "iteration=199 loss=416.6258544921875\n",
      "1\n",
      "iteration=0 loss=6173.22900390625\n",
      "iteration=20 loss=1337.7255859375\n",
      "iteration=40 loss=862.2739868164062\n",
      "iteration=60 loss=666.1441040039062\n",
      "iteration=80 loss=567.1578979492188\n",
      "iteration=100 loss=512.8164672851562\n",
      "iteration=120 loss=474.1331481933594\n",
      "iteration=140 loss=445.2003173828125\n",
      "iteration=160 loss=422.91741943359375\n",
      "iteration=180 loss=405.0823669433594\n",
      "iteration=199 loss=391.085205078125\n",
      "2\n",
      "iteration=0 loss=6010.771484375\n",
      "iteration=20 loss=1540.8245849609375\n",
      "iteration=40 loss=1009.1867065429688\n",
      "iteration=60 loss=770.5421142578125\n",
      "iteration=80 loss=642.19580078125\n",
      "iteration=100 loss=562.5022583007812\n",
      "iteration=120 loss=506.714111328125\n",
      "iteration=140 loss=466.47344970703125\n",
      "iteration=160 loss=436.9141845703125\n",
      "iteration=180 loss=414.7760314941406\n",
      "iteration=199 loss=399.0871276855469\n",
      "3\n",
      "iteration=0 loss=4199.771484375\n",
      "iteration=20 loss=1607.2415771484375\n",
      "iteration=40 loss=1049.369140625\n",
      "iteration=60 loss=789.1647338867188\n",
      "iteration=80 loss=657.9627685546875\n",
      "iteration=100 loss=584.5404052734375\n",
      "iteration=120 loss=532.5022583007812\n",
      "iteration=140 loss=493.7177734375\n",
      "iteration=160 loss=463.9582824707031\n",
      "iteration=180 loss=440.70599365234375\n",
      "iteration=199 loss=423.1640625\n",
      "4\n",
      "iteration=0 loss=5909.1123046875\n",
      "iteration=20 loss=1648.56298828125\n",
      "iteration=40 loss=1028.272216796875\n",
      "iteration=60 loss=778.4125366210938\n",
      "iteration=80 loss=656.8345947265625\n",
      "iteration=100 loss=583.4747924804688\n",
      "iteration=120 loss=534.490966796875\n",
      "iteration=140 loss=499.5630187988281\n",
      "iteration=160 loss=473.03143310546875\n",
      "iteration=180 loss=452.0975341796875\n",
      "iteration=199 loss=435.84881591796875\n",
      "5\n",
      "iteration=0 loss=5901.8134765625\n",
      "iteration=20 loss=1587.555908203125\n",
      "iteration=40 loss=1091.114013671875\n",
      "iteration=60 loss=804.29931640625\n",
      "iteration=80 loss=638.6434326171875\n",
      "iteration=100 loss=540.2887573242188\n",
      "iteration=120 loss=478.44183349609375\n",
      "iteration=140 loss=442.7610168457031\n",
      "iteration=160 loss=417.161376953125\n",
      "iteration=180 loss=396.869140625\n",
      "iteration=199 loss=1562.33056640625\n",
      "6\n",
      "iteration=0 loss=8174.98974609375\n",
      "iteration=20 loss=1775.3250732421875\n",
      "iteration=40 loss=1191.1910400390625\n",
      "iteration=60 loss=909.41259765625\n",
      "iteration=80 loss=752.3092041015625\n",
      "iteration=100 loss=654.9193115234375\n",
      "iteration=120 loss=588.0122680664062\n",
      "iteration=140 loss=538.7683715820312\n",
      "iteration=160 loss=500.2598571777344\n",
      "iteration=180 loss=469.50384521484375\n",
      "iteration=199 loss=446.2757568359375\n",
      "7\n",
      "iteration=0 loss=9998.80078125\n",
      "iteration=20 loss=1642.0953369140625\n",
      "iteration=40 loss=1039.478271484375\n",
      "iteration=60 loss=806.4021606445312\n",
      "iteration=80 loss=693.2643432617188\n",
      "iteration=100 loss=620.5284423828125\n",
      "iteration=120 loss=566.2553100585938\n",
      "iteration=140 loss=524.3209838867188\n",
      "iteration=160 loss=491.2142028808594\n",
      "iteration=180 loss=463.6517333984375\n",
      "iteration=199 loss=441.34881591796875\n",
      "8\n",
      "iteration=0 loss=10594.029296875\n",
      "iteration=20 loss=1527.7679443359375\n",
      "iteration=40 loss=981.3070068359375\n",
      "iteration=60 loss=744.3742065429688\n",
      "iteration=80 loss=622.3905029296875\n",
      "iteration=100 loss=547.0148315429688\n",
      "iteration=120 loss=493.99005126953125\n",
      "iteration=140 loss=455.2908020019531\n",
      "iteration=160 loss=428.498046875\n",
      "iteration=180 loss=407.9398193359375\n",
      "iteration=199 loss=392.41729736328125\n",
      "9\n",
      "iteration=0 loss=5845.88818359375\n",
      "iteration=20 loss=1219.568603515625\n",
      "iteration=40 loss=822.9971923828125\n",
      "iteration=60 loss=676.3593139648438\n",
      "iteration=80 loss=587.8203735351562\n",
      "iteration=100 loss=528.6596069335938\n",
      "iteration=120 loss=486.51727294921875\n",
      "iteration=140 loss=454.3327941894531\n",
      "iteration=160 loss=429.13232421875\n",
      "iteration=180 loss=409.0931701660156\n",
      "iteration=199 loss=393.66705322265625\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.036\n",
      "F1: 0.316\n",
      "Recall: 0.192\n",
      "Precision: 0.892\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.069\n",
      "F1: 0.506\n",
      "Recall: 0.363\n",
      "Precision: 0.833\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.058\n",
      "F1: 0.445\n",
      "Recall: 0.305\n",
      "Precision: 0.820\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.115\n",
      "F1: 0.696\n",
      "Recall: 0.605\n",
      "Precision: 0.819\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.064\n",
      "F1: 0.458\n",
      "Recall: 0.337\n",
      "Precision: 0.712\n",
      "\n",
      "Accuracy: 0.005\n",
      "F1: 0.033\n",
      "Recall: 0.026\n",
      "Precision: 0.046\n",
      "Accuracy: 0.050\n",
      "F1: 0.350\n",
      "Recall: 0.265\n",
      "Precision: 0.517\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.096\n",
      "F1: 0.616\n",
      "Recall: 0.506\n",
      "Precision: 0.787\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.111\n",
      "F1: 0.690\n",
      "Recall: 0.584\n",
      "Precision: 0.841\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.053\n",
      "F1: 0.430\n",
      "Recall: 0.282\n",
      "Precision: 0.907\n",
      "\n",
      "Accuracy: 0.000\n",
      "F1: 0.000\n",
      "Recall: 0.000\n",
      "Precision: 0.000\n",
      "Accuracy: 0.110\n",
      "F1: 0.693\n",
      "Recall: 0.581\n",
      "Precision: 0.858\n",
      "\n",
      "[5, 9, DPLabelModel(), 0.14325068870523416, 0.806201550387597, 0.7558139534883721, 0.8637873754152824, 0.008815426997245178, 0.035555555555555556, 0.046511627906976744, 0.02877697841726619]\n",
      "CPU times: user 1h 20min 57s, sys: 2min 37s, total: 1h 23min 35s\n",
      "Wall time: 1h 23min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best = None\n",
    "for iterations in [1, 5,10, 25, 50, 100, 200]:\n",
    "    for learning_rate in [1e-4, 1e-5, 1e-6, 1e-7]:\n",
    "        print(iterations, learning_rate)\n",
    "        max_seed = 10\n",
    "        temporal_models = [None,]*max_seed\n",
    "        for seed in range(max_seed):\n",
    "            print(seed)\n",
    "            markov_model = DPLabelModel(m=m_per_task*T, \n",
    "                                        T=T,\n",
    "                                        edges=[(i,i+m_per_task) for i in range((T-1)*m_per_task)],\n",
    "                                        coverage_sets=[[t,] for t in range(T) for _ in range(m_per_task)],\n",
    "                                        mu_sharing=[[t*m_per_task+i for t in range(T)] for i in range(m_per_task)],\n",
    "                                        phi_sharing=[[(t*m_per_task+i, (t+1)*m_per_task+i)\n",
    "                                                      for t in range(T-1)] for i in range(m_per_task)],\n",
    "                                        device=device,\n",
    "                                        class_balance=torch.tensor(class_balance).float().to(device),\n",
    "                                        seed=seed)\n",
    "            optimize(markov_model, L_hat=MRI_data_temporal['Li_train'], num_iter=iterations,\n",
    "                     lr=1e-5, momentum=0.8, clamp=True, \n",
    "                     verbose=iterations >= 10, seed=seed)\n",
    "            temporal_models[seed] = markov_model\n",
    "\n",
    "        for seed, model in enumerate(temporal_models):\n",
    "            Li_dev = torch.LongTensor(MRI_data_temporal['Li_dev'].cpu().numpy())\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            scores = [iterations, seed, model]\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "                \n",
    "            model.flip_params()\n",
    "            R_pred_frame_label = model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "            for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "                score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "                print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "                scores.append(score)\n",
    "            \n",
    "            model.flip_params()\n",
    "\n",
    "            if best == None or scores[4] > max(best[4], best[8]) or scores[8] > max(best[4], best[8]):\n",
    "                best = scores\n",
    "            print()\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.143\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.120\n",
      "F1: 0.784\n",
      "Recall: 0.738\n",
      "Precision: 0.838\n"
     ]
    }
   ],
   "source": [
    "Li_test = torch.LongTensor(MRI_data_temporal['Li_test'].cpu().numpy())\n",
    "R_pred_frame_label = best_model.predict_element_proba(Li_test.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_test.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.083\n",
      "F1: 0.690\n",
      "Recall: 0.575\n",
      "Precision: 0.863\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = best_model.predict_element_proba(Li_dev.to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1287.,  184.,   37.,   44.,   26.,   28.,   37.,   29.,   53.,\n",
       "         120.]),\n",
       " array([5.02966725e-06, 9.92960581e-02, 1.98587087e-01, 2.97878115e-01,\n",
       "        3.97169143e-01, 4.96460172e-01, 5.95751200e-01, 6.95042229e-01,\n",
       "        7.94333257e-01, 8.93624286e-01, 9.92915314e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_frame_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, 'models/ts_labelmodel_best_tuning_downsampled_same_val_test.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/ts_labelmodel.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.143\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = model.eval().predict_element_proba(\n",
    "    MRI_data_temporal['Li_dev'].to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.120\n",
      "F1: 0.784\n",
      "Recall: 0.738\n",
      "Precision: 0.838\n"
     ]
    }
   ],
   "source": [
    "R_pred_frame_label = model.eval().predict_element_proba(\n",
    "    MRI_data_temporal['Li_test'].to(device)) #predict per element\n",
    "\n",
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_test.cpu(), np.round(R_pred_frame_label), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions for everything and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sparse\n",
    "import pickle\n",
    "import rekall\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "from rekall.interval_list import IntervalList\n",
    "from rekall.temporal_predicates import *\n",
    "from metal.label_model.baselines import MajorityLabelVoter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load manually annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 11655.47it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 12861.74it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/manually_annotated_shots.pkl', 'rb') as f:\n",
    "    shots = VideoIntervalCollection(pickle.load(f))\n",
    "with open('../../data/shot_detection_folds.pkl', 'rb') as f:\n",
    "    shot_detection_folds = pickle.load(f)\n",
    "clips = shots.dilate(1).coalesce().dilate(-1)\n",
    "shot_boundaries = shots.map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.start, intrvl.payload)\n",
    ").set_union(\n",
    "    shots.map(lambda intrvl: (intrvl.end + 1, intrvl.end + 1, intrvl.payload))\n",
    ").coalesce()\n",
    "boundary_frames = {\n",
    "    video_id: [\n",
    "        intrvl.start\n",
    "        for intrvl in shot_boundaries.get_intervallist(video_id).get_intervals()\n",
    "    ]\n",
    "    for video_id in shot_boundaries.get_allintervals()\n",
    "}\n",
    "video_ids = sorted(list(clips.get_allintervals().keys()))\n",
    "frames_per_video = {\n",
    "    video_id: sorted([\n",
    "        f\n",
    "        for interval in clips.get_intervallist(video_id).get_intervals()\n",
    "        for f in range(interval.start, interval.end + 2)\n",
    "    ])\n",
    "    for video_id in video_ids\n",
    "}\n",
    "ground_truth = {\n",
    "    video_id: [\n",
    "        1 if f in boundary_frames[video_id] else 2\n",
    "        for f in frames_per_video[video_id]\n",
    "    ] \n",
    "    for video_id in video_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load label matrix with all frames in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/all_labels.pkl', 'rb') as f:\n",
    "    weak_labels_all_movies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load videos and number of frames per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/frame_counts.pkl', 'rb') as f:\n",
    "    frame_counts = pickle.load(f)\n",
    "video_ids_all = sorted(list(frame_counts.keys()))\n",
    "video_ids_train = sorted(list(set(video_ids_all).difference(set(video_ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct windows for each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, construct windows of 16 frames for each video\n",
    "windows = VideoIntervalCollection({\n",
    "    video_id: [\n",
    "        (f, f + 16, video_id)\n",
    "        for f in range(0, frame_counts[video_id] - 16, 16)\n",
    "    ]\n",
    "    for video_id in video_ids_all\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ground truth labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, intersect the windows with ground truth and get ground truth labels for the windows\n",
    "windows_intersecting_ground_truth = windows.filter_against(\n",
    "    clips,\n",
    "    predicate=overlaps()\n",
    ").map(lambda intrvl: (intrvl.start, intrvl.end, 2))\n",
    "windows_with_shot_boundaries = windows_intersecting_ground_truth.filter_against(\n",
    "    shot_boundaries,\n",
    "    predicate = lambda window, shot_boundary:\n",
    "        shot_boundary.start >= window.start and shot_boundary.start < window.end\n",
    ").map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.end, 1)\n",
    ")\n",
    "windows_with_labels = windows_with_shot_boundaries.set_union(\n",
    "    windows_intersecting_ground_truth\n",
    ").coalesce(\n",
    "    predicate = equal(),\n",
    "    payload_merge_op = lambda p1, p2: min(p1, p2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weak labels for all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label windows with the weak labels in our labeling functions\n",
    "def label_window(per_frame_weak_labels):\n",
    "    if 1 in per_frame_weak_labels:\n",
    "        return 1\n",
    "    if len([l for l in per_frame_weak_labels if l == 2]) >= len(per_frame_weak_labels) / 2:\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "windows_with_weak_labels = windows.map(\n",
    "    lambda window: (\n",
    "        window.start,\n",
    "        window.end,\n",
    "        [\n",
    "            label_window([\n",
    "                lf[window.payload][f-1]\n",
    "                for f in range(window.start, window.end)\n",
    "            ])\n",
    "            for lf in weak_labels_all_movies\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_everything_windows = csr_matrix([\n",
    "    intrvl.payload\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows_downsampled.npy', 'wb') as f:\n",
    "    np.save(f, L_everything_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/shot_detection_weak_labels/L_everything_windows_downsampled.npy', 'rb') as f:\n",
    "    L_everything_windows = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert L matrix to timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5\n",
    "m_per_task = L_everything_windows.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled = torch.FloatTensor(L_everything_windows[:L_everything_windows.shape[0] -\n",
    "                                                      (L_everything_windows.shape[0] % T)]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_per_task_unlabelled = L_unlabelled.size(1)\n",
    "n_frames_unlabelled = L_unlabelled.size(0)\n",
    "n_patients_unlabelled = n_frames_unlabelled//T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_unlabelled_ts = torch.LongTensor(\n",
    "    L_unlabelled.view(n_patients_unlabelled, (m_per_task*T)).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235081"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled_ts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.flip_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val = model.eval().predict_element_proba(MRI_data_temporal['Li_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1278.,  148.,   35.,   29.,   23.,   25.,   33.,   40.,   59.,\n",
       "         145.]),\n",
       " array([1.88074608e-06, 9.98230374e-02, 1.99644194e-01, 2.99465351e-01,\n",
       "        3.99286507e-01, 4.99107664e-01, 5.98928821e-01, 6.98749977e-01,\n",
       "        7.98571134e-01, 8.98392291e-01, 9.98213447e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.931\n",
      "F1: 0.806\n",
      "Recall: 0.756\n",
      "Precision: 0.864\n"
     ]
    }
   ],
   "source": [
    "for metric in ['accuracy', 'f1', 'recall', 'precision']:\n",
    "    score = metric_score(Y_dev.cpu().where(Y_dev.cpu() == torch.tensor(1.), torch.tensor(0.)),\n",
    "                         np.round(predictions_val), metric)\n",
    "    print(f\"{metric.capitalize()}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n"
     ]
    }
   ],
   "source": [
    "predictions_everything = []\n",
    "for i in range(0, L_unlabelled_ts.shape[0], 100000):\n",
    "    print(i)\n",
    "    start = i\n",
    "    end = i + 100000\n",
    "    labels = L_unlabelled_ts[start:end] if end < L_unlabelled_ts.shape[0] else L_unlabelled_ts[start:]\n",
    "    predictions_for_labels = model.eval().predict_element_proba(labels.to(device))\n",
    "    predictions_everything.append(predictions_for_labels)\n",
    "    del predictions_for_labels\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6175405,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(predictions_everything).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_pred_probs_per_frame = np.concatenate(predictions_everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3759959.,  734106.,  692494.,  145111.,  139209.,   97392.,\n",
       "         131007.,  160536.,  158791.,  156800.]),\n",
       " array([1.41829968e-07, 9.99912640e-02, 1.99982386e-01, 2.99973508e-01,\n",
       "        3.99964631e-01, 4.99955753e-01, 5.99946875e-01, 6.99937997e-01,\n",
       "        7.99929120e-01, 8.99920242e-01, 9.99911364e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(R_pred_probs_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "         0],\n",
       "        [1, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0,\n",
       "         0],\n",
       "        [2, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0,\n",
       "         0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_unlabelled_ts[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_nums = [\n",
    "    (video_id, intrvl.start, intrvl.end)\n",
    "    for video_id in sorted(list(video_ids_all))\n",
    "    for intrvl in windows_with_weak_labels.get_intervallist(video_id).get_intervals()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows = [\n",
    "    (window_info, np.array([prediction, 1. - prediction]))\n",
    "    for window_info, prediction in zip(window_nums, R_pred_probs_per_frame)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we needed to cut the predictions to a multiple of T\n",
    "last_preds = []\n",
    "for window_info in window_nums[len(predictions_to_save_windows):]:\n",
    "    last_preds.append((window_info, np.array([0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_to_save_windows += last_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np_windows = np.array(predictions_to_save_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions to disk\n",
    "with open('../../data/shot_detection_weak_labels/ts_weak_labels_all_windows_tuned_downsampled_same_val_test.npy', 'wb') as f:\n",
    "    np.save(f, preds_np_windows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
