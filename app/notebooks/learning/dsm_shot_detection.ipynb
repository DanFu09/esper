{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#DSM-Shot-Detection\" data-toc-modified-id=\"DSM-Shot-Detection-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>DSM Shot Detection</a></span></li><li><span><a href=\"#Part-1:-Adaptive-Filtering\" data-toc-modified-id=\"Part-1:-Adaptive-Filtering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Part 1: Adaptive Filtering</a></span><ul class=\"toc-item\"><li><span><a href=\"#SqueezeNet\" data-toc-modified-id=\"SqueezeNet-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>SqueezeNet</a></span></li><li><span><a href=\"#Compute-SqueezeNet-embeddings\" data-toc-modified-id=\"Compute-SqueezeNet-embeddings-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Compute SqueezeNet embeddings</a></span></li><li><span><a href=\"#Shot-Candidates-from-Differences-in-SqueezeNet-Embeddings\" data-toc-modified-id=\"Shot-Candidates-from-Differences-in-SqueezeNet-Embeddings-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Shot Candidates from Differences in SqueezeNet Embeddings</a></span></li><li><span><a href=\"#Experiment:-Find-Best-Window-Size\" data-toc-modified-id=\"Experiment:-Find-Best-Window-Size-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Experiment: Find Best Window Size</a></span></li></ul></li><li><span><a href=\"#Part-2:-Hard-Cut-Detection\" data-toc-modified-id=\"Part-2:-Hard-Cut-Detection-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Part 2: Hard Cut Detection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adaptive-filtering-for-candidates\" data-toc-modified-id=\"Adaptive-filtering-for-candidates-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Adaptive filtering for candidates</a></span></li><li><span><a href=\"#Load-Hard-Cut-Prediction-Model\" data-toc-modified-id=\"Load-Hard-Cut-Prediction-Model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Load Hard Cut Prediction Model</a></span></li><li><span><a href=\"#ClipShots-dataloader\" data-toc-modified-id=\"ClipShots-dataloader-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>ClipShots dataloader</a></span></li><li><span><a href=\"#Test-Model\" data-toc-modified-id=\"Test-Model-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Test Model</a></span></li></ul></li><li><span><a href=\"#Part-4:-Gradual-Cut-Detection\" data-toc-modified-id=\"Part-4:-Gradual-Cut-Detection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Part 4: Gradual Cut Detection</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:24.116872Z",
     "start_time": "2019-02-25T22:57:23.625809Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSM Shot Detection\n",
    "In this notebook we'll try to recreate DSM shot detection as outlined in [this paper](https://arxiv.org/pdf/1808.04234.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:25.150835Z",
     "start_time": "2019-02-25T22:57:25.082019Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Adaptive Filtering\n",
    "\n",
    "The first part of the DSM pipeline is adaptive filtering using SqueezeNet activations trained on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:53:39.751743Z",
     "start_time": "2019-02-25T22:53:38.597722Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, load up the results of adaptive filtering from when they did it\n",
    "items = []\n",
    "with open('/app/data/dsm_cut_detection/data/data_list/cut_train_data_video.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        boundary = line.strip().split(' ')\n",
    "        if len(boundary) == 5:\n",
    "            [_, path, start_frame, end_frame, label] = boundary\n",
    "        else:\n",
    "            [path, start_frame, end_frame, label] = boundary\n",
    "        path = os.path.basename(path)\n",
    "        start_frame = int(start_frame)\n",
    "        end_frame = int(end_frame)\n",
    "        label = int(label)\n",
    "        items.append((path, start_frame, end_frame, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:53:39.925372Z",
     "start_time": "2019-02-25T22:53:39.754380Z"
    }
   },
   "outputs": [],
   "source": [
    "vids = {}\n",
    "for i in items:\n",
    "    key = i[0]\n",
    "    if key not in vids:\n",
    "        vids[key] = []\n",
    "    vids[key].append((i[1], i[2], i[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:53:40.000673Z",
     "start_time": "2019-02-25T22:53:39.978759Z"
    }
   },
   "outputs": [],
   "source": [
    "test_vid = items[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:32:55.440267Z",
     "start_time": "2019-02-25T22:32:52.801136Z"
    }
   },
   "outputs": [],
   "source": [
    "squeezenet = models.squeezenet1_1(pretrained=True).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute SqueezeNet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:17.525113Z",
     "start_time": "2019-02-25T19:47:17.305777Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numbers\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        for t in self.transforms:\n",
    "            t.randomize_parameters()\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n",
    "    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm_value=255):\n",
    "        self.norm_value = norm_value\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
    "            # backward compatibility\n",
    "            return img.float().div(self.norm_value)\n",
    "\n",
    "        if accimage is not None and isinstance(pic, accimage.Image):\n",
    "            nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n",
    "            pic.copyto(nppic)\n",
    "            return torch.from_numpy(nppic)\n",
    "\n",
    "        # handle PIL Image\n",
    "        if pic.mode == 'I':\n",
    "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
    "        elif pic.mode == 'I;16':\n",
    "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
    "        else:\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
    "        if pic.mode == 'YCbCr':\n",
    "            nchannel = 3\n",
    "        elif pic.mode == 'I;16':\n",
    "            nchannel = 1\n",
    "        else:\n",
    "            nchannel = len(pic.mode)\n",
    "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
    "        # put it from HWC to CHW format\n",
    "        # yikes, this transpose takes 80% of the loading time/CPU\n",
    "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        if isinstance(img, torch.ByteTensor):\n",
    "            return img.float().div(self.norm_value)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    \"\"\"Rescale the input PIL.Image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (w, h), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be scaled.\n",
    "        Returns:\n",
    "            PIL.Image: Rescaled image.\n",
    "        \"\"\"\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size, self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "def get_mean(norm_value=255):\n",
    "    return [114.7748 / norm_value, 107.7354 / norm_value, 99.4750 / norm_value]\n",
    "\n",
    "def get_test_spatial_transform(opt):\n",
    "    return Compose([Scale((opt.spatial_size,opt.spatial_size)),\n",
    "                    ToTensor(opt.norm_value),\n",
    "                    Normalize(get_mean(opt.norm_value), [1, 1, 1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:18.055162Z",
     "start_time": "2019-02-25T19:47:18.019134Z"
    }
   },
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            with Image.open(f) as img:\n",
    "                return img.convert('RGB')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:18.807360Z",
     "start_time": "2019-02-25T19:47:18.773337Z"
    }
   },
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.norm_value = 1\n",
    "        self.spatial_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:19.386791Z",
     "start_time": "2019-02-25T19:47:19.350386Z"
    }
   },
   "outputs": [],
   "source": [
    "class VidDataset(Dataset):\n",
    "    def __init__(self, path, frame_num):\n",
    "        self.path = path\n",
    "        self.frame_num = frame_num\n",
    "        self.transform = get_test_spatial_transform(Opt())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.frame_num\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(pil_loader(os.path.join(self.path, 'image_{}.jpg'.format(idx + 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:20.743918Z",
     "start_time": "2019-02-25T19:47:20.709489Z"
    }
   },
   "outputs": [],
   "source": [
    "annotations_path = '/app/data/ClipShots/annotations/train.json'\n",
    "frame_path = os.path.join('/app/data/ClipShots/frames/train', test_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:21.555102Z",
     "start_time": "2019-02-25T19:47:21.293313Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(annotations_path, 'r') as f:\n",
    "    train_annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:22.119947Z",
     "start_time": "2019-02-25T19:47:22.098166Z"
    }
   },
   "outputs": [],
   "source": [
    "test_vid_dataset = VidDataset(frame_path, int(train_annotations[test_vid]['frame_num']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:23.472465Z",
     "start_time": "2019-02-25T19:47:23.442815Z"
    }
   },
   "outputs": [],
   "source": [
    "test_vid_dataloader = DataLoader(test_vid_dataset, shuffle=False, batch_size=8, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:00:27.456111Z",
     "start_time": "2019-02-22T01:00:17.382245Z"
    }
   },
   "outputs": [],
   "source": [
    "squeezenet_features = []\n",
    "squeezenet_classifications = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:01:49.013543Z",
     "start_time": "2019-02-22T01:00:27.458991Z"
    }
   },
   "outputs": [],
   "source": [
    "for images in tqdm(test_vid_dataloader):\n",
    "    images = images.to(device)\n",
    "    squeezenet_classifications += squeezenet(images).detach().cpu().numpy().tolist()\n",
    "    squeezenet_features += squeezenet.features(images).detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:02:06.254935Z",
     "start_time": "2019-02-22T01:01:53.141124Z"
    }
   },
   "outputs": [],
   "source": [
    "squeezenet_features_flat = [\n",
    "    torch.tensor(f).view(1, -1)\n",
    "    for f in squeezenet_features\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shot Candidates from Differences in SqueezeNet Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T00:59:21.663613Z",
     "start_time": "2019-02-22T00:59:21.633251Z"
    }
   },
   "outputs": [],
   "source": [
    "print(vids[test_vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T00:35:21.326197Z",
     "start_time": "2019-02-22T00:35:21.292410Z"
    }
   },
   "outputs": [],
   "source": [
    "# print out the ground truth again\n",
    "print(sorted([t1 for t1, t2, gt in vids[test_vid]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:20:55.518743Z",
     "start_time": "2019-02-22T01:20:55.487140Z"
    }
   },
   "outputs": [],
   "source": [
    "true_cuts = set([t1 for t1, t2, gt in vids[test_vid] if gt == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:19:21.098394Z",
     "start_time": "2019-02-22T01:19:21.066685Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(vids[test_vid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:54.792294Z",
     "start_time": "2019-02-25T19:47:54.755195Z"
    }
   },
   "outputs": [],
   "source": [
    "scales = [1, 2, 4, 8, 16, 32]\n",
    "window_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:55.533364Z",
     "start_time": "2019-02-25T19:47:55.489848Z"
    }
   },
   "outputs": [],
   "source": [
    "SIGMA = 0.05\n",
    "T = 0.5\n",
    "def get_candidates_from_scale(vectors, scale, window_size):\n",
    "    vec_scaled = vectors[::scale]\n",
    "    diffs = [\n",
    "        cosine(vec_scaled[i-1], vec_scaled[i])\n",
    "        for i in range(1, len(vec_scaled))\n",
    "    ]\n",
    "    \n",
    "    n = len(diffs)\n",
    "    candidate_boundaries = []\n",
    "    for i in range(window_size, n-window_size):\n",
    "        window = diffs[max(i - window_size, 0):min(i+window_size, n)]\n",
    "        threshold = T + (SIGMA / len(window)) * np.sum(window)\n",
    "        if diffs[i] > threshold:\n",
    "            candidate_boundaries.append(i * scale)\n",
    "    \n",
    "    return candidate_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:47:56.184390Z",
     "start_time": "2019-02-25T19:47:56.145089Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_candidates(vectors, window_size):\n",
    "    candidates = [\n",
    "        get_candidates_from_scale(vectors, scale, window_size)\n",
    "        for scale in scales\n",
    "    ]\n",
    "    \n",
    "    final_candidates = []\n",
    "    for scale, candidates_for_scale in zip(scales, candidates):\n",
    "        for c in candidates_for_scale:\n",
    "            duplicate_candidate = False\n",
    "            for existing_candidate in final_candidates:\n",
    "                if abs(c - existing_candidate) <= scale:\n",
    "                    duplicate_candidate = True\n",
    "                    break\n",
    "            if not duplicate_candidate:\n",
    "                final_candidates.append(c)\n",
    "    \n",
    "    return sorted(final_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T01:22:32.263531Z",
     "start_time": "2019-02-22T01:22:09.632435Z"
    }
   },
   "outputs": [],
   "source": [
    "for window_size in window_sizes:\n",
    "    candidate_list = set(get_all_candidates(squeezenet_features_flat, window_size))\n",
    "    print('Window size {} has {} candidates; {} precision and {} recall'.format(\n",
    "        window_size,\n",
    "        len(candidate_list),\n",
    "        len(candidate_list.intersection(true_cuts)) / len(candidate_list),\n",
    "        len(candidate_list.intersection(true_cuts)) / len(true_cuts)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Find Best Window Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:46:22.698114Z",
     "start_time": "2019-02-25T19:46:22.669276Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:46:23.341013Z",
     "start_time": "2019-02-25T19:46:23.310290Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:46:23.952321Z",
     "start_time": "2019-02-25T19:46:23.916482Z"
    }
   },
   "outputs": [],
   "source": [
    "all_vids = sorted(list(vids.keys()))\n",
    "random.shuffle(all_vids)\n",
    "test_vids = all_vids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:46:24.760880Z",
     "start_time": "2019-02-25T19:46:24.720946Z"
    }
   },
   "outputs": [],
   "source": [
    "test_vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:48:02.347051Z",
     "start_time": "2019-02-25T19:47:59.757855Z"
    }
   },
   "outputs": [],
   "source": [
    "for vid in test_vids:\n",
    "    print(vid)\n",
    "    true_cuts = set([t1 for t1, t2, gt in vids[vid] if gt == 1])\n",
    "    print('{} has {} true transitions'.format(vid, len(true_cuts)))\n",
    "    frame_path = os.path.join('/app/data/ClipShots/frames/train', vid)\n",
    "    vid_dataset = VidDataset(frame_path, int(train_annotations[vid]['frame_num']))\n",
    "    vid_dataloader = DataLoader(vid_dataset, shuffle=False, batch_size=8, num_workers=0)\n",
    "    \n",
    "    squeezenet_features = []\n",
    "    squeezenet_classifications = []\n",
    "    for images in vid_dataloader:\n",
    "        images = images.to(device)\n",
    "        squeezenet_classifications += squeezenet(images).detach().cpu().numpy().tolist()\n",
    "        squeezenet_features += squeezenet.features(images).detach().cpu().numpy().tolist()\n",
    "        \n",
    "    squeezenet_features_flat = [\n",
    "        torch.tensor(f).view(1, -1)\n",
    "        for f in squeezenet_features\n",
    "    ]\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        candidate_list = set(get_all_candidates(squeezenet_features_flat, window_size))\n",
    "        print('Window size {} has {} candidates; {} precision and {} recall'.format(\n",
    "            window_size,\n",
    "            len(candidate_list),\n",
    "            0 if len(candidate_list) == 0 else len(candidate_list.intersection(true_cuts)) / len(candidate_list),\n",
    "            0 if len(true_cuts) == 0 else len(candidate_list.intersection(true_cuts)) / len(true_cuts)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Hard Cut Detection\n",
    "Use adaptive filtering with a window size of 6 to get candidates, and then run hard cut detection on the candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive filtering for candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T19:46:18.689459Z",
     "start_time": "2019-02-25T19:46:18.657629Z"
    }
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T20:00:51.617774Z",
     "start_time": "2019-02-25T19:48:15.859333Z"
    }
   },
   "outputs": [],
   "source": [
    "shot_candidates = {}\n",
    "for test_vid in tqdm(test_vids):\n",
    "    frame_path = os.path.join('/app/data/ClipShots/frames/train', test_vid)\n",
    "    vid_dataset = VidDataset(frame_path, int(train_annotations[test_vid]['frame_num']))\n",
    "    vid_dataloader = DataLoader(vid_dataset, shuffle=False, batch_size=8, num_workers=0)\n",
    "    \n",
    "    squeezenet_features = []\n",
    "    for images in vid_dataloader:\n",
    "        images = images.to(device)\n",
    "        squeezenet_features += squeezenet.features(images).detach().cpu().numpy().tolist()\n",
    "        \n",
    "    squeezenet_features_flat = [\n",
    "        torch.tensor(f).view(1, -1)\n",
    "        for f in squeezenet_features\n",
    "    ]    \n",
    "    \n",
    "    candidate_list = set(get_all_candidates(squeezenet_features_flat, WINDOW_SIZE))\n",
    "    \n",
    "    shot_candidates[test_vid] = candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T20:04:42.894186Z",
     "start_time": "2019-02-25T20:04:42.851370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store candidates in a local file for the dataloader\n",
    "with open('/app/data/cut_candidates.txt', 'w') as f:\n",
    "    for video in shot_candidates:\n",
    "        gt = sorted([t1 for t1, t2, gt in vids[video]])\n",
    "        for candidate in sorted(shot_candidates[video]):\n",
    "            f.write('{} {} {} {} {}\\n'.format(\n",
    "                video, video, candidate, candidate + 1, 1 if candidate in gt else 0\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hard Cut Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:39.448507Z",
     "start_time": "2019-02-25T22:57:38.931635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model for 3D Resnet\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "bn_momentum=0.1\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes,momentum=bn_momentum)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes,momentum=bn_momentum)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4,momentum=bn_momentum)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers,input_channel, num_classes=1000,no_fc=False):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channel, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64,momentum=bn_momentum)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(4, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.no_fc=no_fc\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if not self.no_fc:\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(opt, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    if opt.img_concat:\n",
    "        input_channel=opt.image_num*(6 if opt.center_crop else 3)\n",
    "    else:\n",
    "        input_channel=3\n",
    "\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3],input_channel ,2, no_fc=opt.no_fc)\n",
    "    if opt.pretrain_path:\n",
    "        pretrain_weights=torch.load(opt.pretrain_path, map_location=lambda storage, loc: storage)\n",
    "        current_param=model.state_dict()\n",
    "        pretrained={k:v for k,v in pretrain_weights.items() if k in current_param if k in current_param and k.split('.')[0]!='conv1' and k.split('.')[0]!='fc'}\n",
    "        print(pretrained.keys())\n",
    "        current_param.update(pretrained)\n",
    "        model.load_state_dict(current_param)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:43.193994Z",
     "start_time": "2019-02-25T22:57:43.160027Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = Object()\n",
    "opt.pretrain_path = None\n",
    "opt.img_concat = True\n",
    "opt.center_crop = False\n",
    "opt.no_fc = False\n",
    "opt.image_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:44.572246Z",
     "start_time": "2019-02-25T22:57:43.942952Z"
    }
   },
   "outputs": [],
   "source": [
    "cut_model = resnet50(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:48.012265Z",
     "start_time": "2019-02-25T22:57:45.198876Z"
    }
   },
   "outputs": [],
   "source": [
    "load_path = os.path.join(\n",
    "    \"/app/data/img_concat_6_frames_resnet\",\n",
    "    'resnet50_epoch_{}.pth'.format(19)\n",
    ")\n",
    "states = torch.load(load_path)['state_dict']\n",
    "new_resnet_state_dict = OrderedDict()\n",
    "for k, v in states.items():\n",
    "    name = k[7:]\n",
    "    new_resnet_state_dict[name] = v\n",
    "\n",
    "cut_model.load_state_dict(new_resnet_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:48.418651Z",
     "start_time": "2019-02-25T22:57:48.014350Z"
    }
   },
   "outputs": [],
   "source": [
    "cut_model_5_epochs = resnet50(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:49.320964Z",
     "start_time": "2019-02-25T22:57:49.030333Z"
    }
   },
   "outputs": [],
   "source": [
    "load_path = os.path.join(\n",
    "    \"/app/data/img_concat_6_frames_resnet\",\n",
    "    'resnet50_epoch_{}.pth'.format(5)\n",
    ")\n",
    "states = torch.load(load_path)['state_dict']\n",
    "new_resnet_state_dict = OrderedDict()\n",
    "for k, v in states.items():\n",
    "    name = k[7:]\n",
    "    new_resnet_state_dict[name] = v\n",
    "\n",
    "cut_model_5_epochs.load_state_dict(new_resnet_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClipShots dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:51.990877Z",
     "start_time": "2019-02-25T22:57:51.481603Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            with Image.open(f) as img:\n",
    "                return img.convert('RGB')\n",
    "        \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def video_loader(video_dir_path, frame_indices,sample_duration):\n",
    "    video = []\n",
    "    for i in range(frame_indices,frame_indices+sample_duration):\n",
    "        image_path = os.path.join(video_dir_path, 'image_{}.jpg'.format(i))\n",
    "        if os.path.exists(image_path):\n",
    "            video.append(pil_loader(image_path))\n",
    "        else:\n",
    "            return video\n",
    "    assert(len(video)==sample_duration)\n",
    "    return video\n",
    "\n",
    "def numpy_loader(numpy_path):\n",
    "    if os.path.exists(numpy_path):\n",
    "        try:\n",
    "            video=np.load(numpy_path)\n",
    "            length=video.shape[0]\n",
    "            video=np.split(video,length)\n",
    "            video=[Image.fromarray(cv2.cvtColor(np.squeeze(frame), cv2.COLOR_BGR2RGB)).convert('RGB') for frame in video]\n",
    "            return video\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_numpy_path(root_path,video_path,begin):\n",
    "    if video_path is None:\n",
    "        return\n",
    "    videoname=video_path.split(\"/\")[-1]\n",
    "    return os.path.join(root_path,'numpy_img_cut',videoname[0],videoname,\"{}.npy\".format(str(begin)))\n",
    "\n",
    "def get_default_video_loader():\n",
    "    return pil_loader\n",
    "\n",
    "\n",
    "def make_dataset(root_path, image_list_path,label_in):\n",
    "    image_list=[]\n",
    "    with open(image_list_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            words=line.split(' ')\n",
    "            if len(words)==4:\n",
    "                path,idx1,idx2,label=words\n",
    "                path=os.path.basename(path)\n",
    "                video_path=None\n",
    "            elif len(words)==5:\n",
    "                video_path,path,idx1,idx2,label=words\n",
    "                path=os.path.basename(path)\n",
    "            idx1,idx2=int(idx1),int(idx2)\n",
    "            if idx1>idx2:\n",
    "                idx1,idx2=idx2,idx1\n",
    "            if idx1<0:\n",
    "                continue\n",
    "            if int(label)!=label_in:\n",
    "                continue\n",
    "            info={\"root_path\":root_path,\"idx1\":idx1,\"idx2\":idx2,\"path\":path,'label':int(label),'video_path':video_path,'numpy_path':get_numpy_path(root_path,video_path,idx1-2)}\n",
    "            image_list.append(info)\n",
    "\n",
    "    return image_list\n",
    "\n",
    "\n",
    "class DataSet(data.Dataset):\n",
    "\n",
    "    def __init__(self, root_path, image_list_path,balance_pos_neg,label,center_crop_transform=None,image_num=4,\n",
    "                 spatial_transform=None,get_loader=get_default_video_loader):\n",
    "        #print(label)\n",
    "        self.image_list = make_dataset(root_path, image_list_path,label)\n",
    "        assert(len(self.image_list)>0)\n",
    "        self.spatial_transform = spatial_transform\n",
    "        self.loader = get_loader()\n",
    "        self.index_shuffled=list(range(len(self.image_list)))\n",
    "\n",
    "        self.balance_pos_neg=balance_pos_neg\n",
    "        self.pos_cnt=0\n",
    "        self.neg_cnt=0\n",
    "        self.image_num=image_num\n",
    "        self.center_crop_transform=center_crop_transform\n",
    "\n",
    "    def get_pos_index(self):\n",
    "        if len(self.pos_index)==0:\n",
    "            return self.get_neg_index()\n",
    "\n",
    "        index=self.pos_index[self.pos_cnt]\n",
    "        self.pos_cnt+=1\n",
    "        self.pos_cnt%=len(self.pos_index)\n",
    "        return index\n",
    "    \n",
    "    def get_neg_index(self):\n",
    "        if len(self.neg_index)==0:\n",
    "            return self.get_pos_index()\n",
    "\n",
    "        index=self.neg_index[self.neg_cnt]\n",
    "        self.neg_cnt+=1\n",
    "        self.neg_cnt%=len(self.neg_index)\n",
    "        return index\n",
    "    \n",
    "    def get_index(self,index):\n",
    "        if not self.balance_pos_neg:\n",
    "            index=self.index_shuffled[index]\n",
    "        else:\n",
    "            if index%2==0:\n",
    "                #index=self.pos_index[int(index/2)%len(self.pos_index)]\n",
    "                index=self.get_pos_index()\n",
    "            else:\n",
    "                #index=self.neg_index[int(index/2)%len(self.neg_index)]\n",
    "                index=self.get_neg_index()\n",
    "        return index\n",
    "\n",
    "    def get_imgs(self,root_path,idx1,idx2,path):\n",
    "        imgs=[]\n",
    "        half_len=int(self.image_num/2)\n",
    "        for i in range(idx1,idx1-half_len,-1):\n",
    "            img=self.loader(os.path.join(root_path,path,\"image_{}.jpg\".format(i)))\n",
    "            if img is None:\n",
    "                if len(imgs)>0:\n",
    "                    if self.center_crop_transform is not None:\n",
    "                        imgs+=imgs[-2:]\n",
    "                    else:\n",
    "                        imgs+=imgs[-1:]\n",
    "                else:\n",
    "                    return\n",
    "                continue\n",
    "            else:\n",
    "                if self.spatial_transform is not None:\n",
    "                    img_t=self.spatial_transform(img)\n",
    "                    imgs.append(img_t)\n",
    "                if self.center_crop_transform is not None:\n",
    "                    img_c=self.center_crop_transform(img)\n",
    "                    imgs.append(img_c)\n",
    "\n",
    "        imgs=list(reversed(imgs))\n",
    "\n",
    "        for i in range(idx2,idx2+half_len):\n",
    "            img=self.loader(os.path.join(root_path,path,\"image_{}.jpg\".format(i)))\n",
    "            \n",
    "            if img is None:\n",
    "                if len(imgs)>0:\n",
    "                    if self.center_crop_transform is not None:\n",
    "                        imgs+=imgs[-2:]\n",
    "                    else:\n",
    "                        imgs+=imgs[-1:]\n",
    "                else:\n",
    "                    return\n",
    "                continue\n",
    "            else:\n",
    "                if self.spatial_transform is not None:\n",
    "                    img_t=self.spatial_transform(img)\n",
    "                    imgs.append(img_t)\n",
    "                if self.center_crop_transform is not None:\n",
    "                    img_c=self.center_crop_transform(img)\n",
    "                    imgs.append(img_c)\n",
    "                \n",
    "\n",
    "        return imgs\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        imgs=None\n",
    "        raw_index=index\n",
    "        if self.spatial_transform is not None:\n",
    "            self.spatial_transform.randomize_parameters()\n",
    "        while imgs is None:\n",
    "            \n",
    "            index=self.get_index(index)\n",
    "            info = self.image_list[index]\n",
    "\n",
    "            root_path=info['root_path']\n",
    "            idx1=info['idx1']\n",
    "            idx2=info['idx2']\n",
    "            path=info['path']\n",
    "            label=info['label']\n",
    "            video_path=info['video_path']\n",
    "            numpy_path=info['numpy_path']\n",
    "            imgs=None\n",
    "            #if self.image_num==6 and numpy_path is not None:\n",
    "            #    imgs=numpy_loader(numpy_path)\n",
    "            #    if imgs is not None:\n",
    "            #        imgs=[self.spatial_transform(img) for img in imgs]\n",
    "            if imgs is None:\n",
    "                imgs=self.get_imgs(root_path,idx1,idx2,path)\n",
    "            if imgs is None:\n",
    "                index=random.randint(0,len(self.image_list)-1)\n",
    "\n",
    "        #torch.from_numpy(np.array(label,dtype=np.float32))\n",
    "        return imgs,torch.from_numpy(np.array([label],dtype=np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:57:56.734448Z",
     "start_time": "2019-02-25T22:57:55.892985Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numbers\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        for t in self.transforms:\n",
    "            t.randomize_parameters()\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n",
    "    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm_value=255):\n",
    "        self.norm_value = norm_value\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
    "            # backward compatibility\n",
    "            return img.float().div(self.norm_value)\n",
    "\n",
    "        if accimage is not None and isinstance(pic, accimage.Image):\n",
    "            nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n",
    "            pic.copyto(nppic)\n",
    "            return torch.from_numpy(nppic)\n",
    "\n",
    "        # handle PIL Image\n",
    "        if pic.mode == 'I':\n",
    "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
    "        elif pic.mode == 'I;16':\n",
    "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
    "        else:\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
    "        if pic.mode == 'YCbCr':\n",
    "            nchannel = 3\n",
    "        elif pic.mode == 'I;16':\n",
    "            nchannel = 1\n",
    "        else:\n",
    "            nchannel = len(pic.mode)\n",
    "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
    "        # put it from HWC to CHW format\n",
    "        # yikes, this transpose takes 80% of the loading time/CPU\n",
    "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        if isinstance(img, torch.ByteTensor):\n",
    "            return img.float().div(self.norm_value)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    \"\"\"Rescale the input PIL.Image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (w, h), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be scaled.\n",
    "        Returns:\n",
    "            PIL.Image: Rescaled image.\n",
    "        \"\"\"\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size, self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crops the given PIL.Image at the center.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL.Image: Cropped image.\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = self.size\n",
    "        x1 = int(round((w - tw) / 2.))\n",
    "        y1 = int(round((h - th) / 2.))\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CornerCrop(object):\n",
    "    def __init__(self, size, crop_position=None):\n",
    "        self.size = size\n",
    "        if crop_position is None:\n",
    "            self.randomize = True\n",
    "        else:\n",
    "            self.randomize = False\n",
    "        self.crop_position = crop_position\n",
    "        self.crop_positions = ['c', 'tl', 'tr', 'bl', 'br']\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            th, tw = (self.size, self.size)\n",
    "            x1 = int(round((image_width - tw) / 2.))\n",
    "            y1 = int(round((image_height - th) / 2.))\n",
    "            x2 = x1 + tw\n",
    "            y2 = y1 + th\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = self.size\n",
    "            y2 = self.size\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = image_width - self.size\n",
    "            y1 = 0\n",
    "            x2 = image_width\n",
    "            y2 = self.size\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = image_height - self.size\n",
    "            x2 = self.size\n",
    "            y2 = image_height\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = image_width - self.size\n",
    "            y1 = image_height - self.size\n",
    "            x2 = image_width\n",
    "            y2 = image_height\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        if self.randomize:\n",
    "            self.crop_position = self.crop_positions[\n",
    "                random.randint(0, len(self.crop_positions) - 1)]\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if self.p < 0.5:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.p = random.random()\n",
    "\n",
    "\n",
    "class MultiScaleCornerCrop(object):\n",
    "    \"\"\"Crop the given PIL.Image to randomly selected size.\n",
    "    A crop of size is selected from scales of the original size.\n",
    "    A position of cropping is randomly selected from 4 corners and 1 center.\n",
    "    This crop is finally resized to given size.\n",
    "    Args:\n",
    "        scales: cropping scales of the original size\n",
    "        size: size of the smaller edge\n",
    "        interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scales, size, interpolation=Image.BILINEAR):\n",
    "        self.scales = scales\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        self.crop_positions = ['c', 'tl', 'tr', 'bl', 'br']\n",
    "\n",
    "    def __call__(self, img):\n",
    "        min_length = min(img.size[0], img.size[1])\n",
    "        crop_size = int(min_length * self.scale)\n",
    "\n",
    "        image_width = img.size[0]\n",
    "        image_height = img.size[1]\n",
    "\n",
    "        if self.crop_position == 'c':\n",
    "            center_x = image_width // 2\n",
    "            center_y = image_height // 2\n",
    "            box_half = crop_size // 2\n",
    "            x1 = center_x - box_half\n",
    "            y1 = center_y - box_half\n",
    "            x2 = center_x + box_half\n",
    "            y2 = center_y + box_half\n",
    "        elif self.crop_position == 'tl':\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            x2 = crop_size\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'tr':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = 0\n",
    "            x2 = image_width\n",
    "            y2 = crop_size\n",
    "        elif self.crop_position == 'bl':\n",
    "            x1 = 0\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = crop_size\n",
    "            y2 = image_height\n",
    "        elif self.crop_position == 'br':\n",
    "            x1 = image_width - crop_size\n",
    "            y1 = image_height - crop_size\n",
    "            x2 = image_width\n",
    "            y2 = image_height\n",
    "\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        return img.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "        self.crop_position = self.crop_positions[random.randint(0, len(self.scales) - 1)]\n",
    "\n",
    "class RandomGrayscale(object):\n",
    "    \"\"\"Randomly convert image to grayscale with a probability of p (default 0.1).\n",
    "    Args:\n",
    "        p (float): probability that image should be converted to grayscale.\n",
    "        num_output_channels (int): (1 or 3) number of channels desired for output image\n",
    "    Returns:\n",
    "        PIL Image: grayscale version of the input image with probability p\n",
    "                   if num_output_channels == 1 : returned image is single channel\n",
    "                   if num_output_channels == 3 : returned image is 3 channel with r == g == b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.1, num_output_channels=1):\n",
    "        self.p = p\n",
    "        self.num_output_channels = num_output_channels\n",
    "        self.v=random.random()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be converted to grayscale.\n",
    "        Returns:\n",
    "            PIL Image: Randomly grayscaled image.\n",
    "        \"\"\"\n",
    "        if self.v < self.p:\n",
    "            if self.num_output_channels == 1:\n",
    "                img = img.convert('L')\n",
    "            elif self.num_output_channels == 3:\n",
    "                img = img.convert('L')\n",
    "                np_img = np.array(img, dtype=np.uint8)\n",
    "                np_img = np.dstack([np_img, np_img, np_img])\n",
    "                img = Image.fromarray(np_img, 'RGB')\n",
    "            else:\n",
    "                raise ValueError('num_output_channels should be either 1 or 3')\n",
    "\n",
    "        return img\n",
    "\n",
    "    def randomize_parameters(self):\n",
    "        self.v=random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:58:01.799365Z",
     "start_time": "2019-02-25T22:58:01.750298Z"
    }
   },
   "outputs": [],
   "source": [
    "transform=Compose([\n",
    "    Scale((128, 128)),\n",
    "    ToTensor(255)\n",
    "])\n",
    "pos_data=DataSet(\n",
    "    '/app/data/ClipShots/frames/train',\n",
    "    '/app/data/cut_candidates.txt',\n",
    "    image_num=6,\n",
    "    balance_pos_neg=False,\n",
    "    label=1,\n",
    "    center_crop_transform=None,\n",
    "    spatial_transform=transform)\n",
    "neg_data=DataSet(\n",
    "    '/app/data/ClipShots/frames/train',\n",
    "    '/app/data/cut_candidates.txt',\n",
    "    image_num=6,\n",
    "    balance_pos_neg=False,\n",
    "    label=0,\n",
    "    center_crop_transform=None,\n",
    "    spatial_transform=transform)\n",
    "pos_test_dataloader=DataLoader(pos_data,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    batch_size=128)\n",
    "neg_test_dataloader=DataLoader(neg_data,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:58:04.672871Z",
     "start_time": "2019-02-25T22:58:04.638829Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:58:06.465670Z",
     "start_time": "2019-02-25T22:58:06.396604Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, targets):\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, pred = outputs.topk(1, 1, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(targets.view(1, -1))\n",
    "    n_correct_elems = correct.float().sum().item()\n",
    "\n",
    "    return n_correct_elems / batch_size\n",
    "\n",
    "def prf1_confusion_matrix(outputs, targets):\n",
    "    tp = 0.\n",
    "    fp = 0.\n",
    "    tn = 0.\n",
    "    fn = 0.\n",
    "\n",
    "    _, preds = outputs.topk(1, 1, True)\n",
    "    preds = preds.view(1, -1).squeeze()\n",
    "\n",
    "    preds = preds.data.cpu().numpy().tolist()\n",
    "    gt = targets.data.cpu().numpy().tolist()\n",
    "\n",
    "    for truth, pred in zip(gt, preds):\n",
    "        if truth == pred:\n",
    "            if pred == 1:\n",
    "                tp += 1.\n",
    "            else:\n",
    "                tn += 1.\n",
    "        else:\n",
    "            if pred == 1:\n",
    "                fp += 1.\n",
    "            else:\n",
    "                fn += 1.\n",
    "                    \n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "            \n",
    "    return (precision, recall, f1, tp, tn, fp, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:58:07.714896Z",
     "start_time": "2019-02-25T22:58:07.606061Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model, pos_data_loader, neg_data_loader,criterion):\n",
    "    print('testing model')\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    num_batches = 0\n",
    "    total_batches = len(pos_data_loader) + len(neg_data_loader)\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_tp = 0\n",
    "    total_tn = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    loaders = [iter(pos_data_loader), iter(neg_data_loader)]\n",
    "\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    test = True\n",
    "    \n",
    "    for l in loaders:\n",
    "        for imgs, targets in l:\n",
    "            imgs = [img for img in imgs]\n",
    "            img_concat = torch.cat(imgs,1).to(device)\n",
    "            targets = targets.long().view(-1).to(device)\n",
    "\n",
    "            outputs = model(img_concat)\n",
    "            \n",
    "            if test:\n",
    "                all_outputs.append(outputs)\n",
    "                all_targets.append(targets)\n",
    "                test = False\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            acc = calculate_accuracy(outputs, targets)\n",
    "            precision, recall, f1, tp, tn, fp, fn = prf1_confusion_matrix(outputs, targets)\n",
    "\n",
    "            print('Batch: [{0}/{1}]\\t'\n",
    "                  'Loss_conf {loss_c:.4f}\\t'\n",
    "                  'acc {acc:.4f}\\t'\n",
    "                  'pre {pre:.4f}\\t'\n",
    "                  'rec {rec:.4f}\\t'\n",
    "                  'f1 {f1: .4f}\\t'\n",
    "                  'TP {tp} '\n",
    "                  'TN {tn} '\n",
    "                  'FP {fp} '\n",
    "                  'FN {fn} '\n",
    "                  .format(\n",
    "                      num_batches + 1, total_batches, loss_c=loss.item(),acc=acc,\n",
    "                      pre=precision, rec=recall, f1=f1,\n",
    "                      tp=tp, tn=tn, fp=fp, fn=fn))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            total_tp += tp\n",
    "            total_tn += tn\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "    avg_acc = total_acc / total_batches\n",
    "\n",
    "    final_precision = total_tp / (total_tp + total_fp)\n",
    "    final_recall = total_tp / (total_tp + total_fn)\n",
    "    final_f1 = 2 * (final_precision * final_recall) / (final_precision + final_recall)\n",
    "\n",
    "    print('Final stats\\t'\n",
    "          'Loss {loss:.4f}\\t'\n",
    "          'acc {acc:.4f}\\t'\n",
    "          'pre {pre:.4f}\\t'\n",
    "          'rec {rec:.4f}\\t'\n",
    "          'f1 {f1: .4f}\\t'\n",
    "          'TP {tp} '\n",
    "          'TN {tn} '\n",
    "          'FP {fp} '\n",
    "          'FN {fn} '\n",
    "          .format(loss=avg_loss, acc=avg_acc, pre=final_precision,\n",
    "              rec=final_recall, f1=final_f1, tp=total_tp, tn=total_tn, fp=total_fp,\n",
    "              fn=total_fn))\n",
    "    \n",
    "    return all_outputs, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:58:32.319247Z",
     "start_time": "2019-02-25T22:58:09.623465Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs, targets = test_model(cut_model, pos_test_dataloader, neg_test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:59:10.121281Z",
     "start_time": "2019-02-25T22:59:10.070971Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs[0].detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:59:30.665186Z",
     "start_time": "2019-02-25T22:59:30.629785Z"
    }
   },
   "outputs": [],
   "source": [
    "targets[0].detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:51:19.196099Z",
     "start_time": "2019-02-25T22:50:58.075530Z"
    }
   },
   "outputs": [],
   "source": [
    "test_model(cut_model_5_epochs, pos_test_dataloader, neg_test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Gradual Cut Detection\n",
    "\n",
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
