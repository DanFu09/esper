{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T07:55:19.261981Z",
     "start_time": "2018-12-05T07:55:17.211611Z"
    }
   },
   "outputs": [],
   "source": [
    "from esper.prelude import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "import pyro.infer as infer\n",
    "from torch.utils.data import DataLoader\n",
    "from custom_mlp import MLP, Exp\n",
    "from transcript_utils import *\n",
    "from timeit import default_timer as now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-02T20:39:19.929113Z",
     "start_time": "2018-12-02T20:39:19.895872Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute_vectors(video_list(), vocabulary, SEGMENT_SIZE, SEGMENT_STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T07:55:22.669970Z",
     "start_time": "2018-12-05T07:55:22.558032Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a PyTorch module for the VAE\n",
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, z_dim=50, hidden_layers=(500,), categories=2, use_cuda=False):\n",
    "        super(VAE, self).__init__()\n",
    "        # create the encoder and decoder networks\n",
    "        \n",
    "        self.input_size = vocab_size\n",
    "        self.output_size = categories\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.hidden_layers = list(hidden_layers)\n",
    "        \n",
    "        self.encoder_y = MLP([self.input_size] + self.hidden_layers + [self.output_size],\n",
    "                             activation=nn.Softplus,\n",
    "                             output_activation=nn.Softmax,\n",
    "                             use_cuda=self.use_cuda)\n",
    "\n",
    "        self.encoder_z = MLP([self.input_size + self.output_size] +\n",
    "                             self.hidden_layers + [[self.z_dim, self.z_dim]],\n",
    "                             activation=nn.Softplus,\n",
    "                             output_activation=[None, Exp],\n",
    "                             use_cuda=self.use_cuda)\n",
    "        \n",
    "        self.decoder = MLP([self.z_dim + self.output_size] +\n",
    "                           self.hidden_layers + [self.input_size],\n",
    "                           activation=nn.Softplus,\n",
    "                           output_activation=nn.Sigmoid,\n",
    "                           use_cuda=self.use_cuda)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "\n",
    "\n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, xs, ys=None):\n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        batch_size = xs.shape[0]\n",
    "        with pyro.iarange(\"data\", batch_size):\n",
    "            # setup hyperparameters for prior p(z)\n",
    "            z_loc = xs.new_zeros(torch.Size((batch_size, self.z_dim)))\n",
    "            z_scale = xs.new_ones(torch.Size((batch_size, self.z_dim)))\n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            zs = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "            \n",
    "            alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "            \n",
    "            # decode the latent code z\n",
    "            loc = self.decoder.forward([zs, ys])\n",
    "            # score against actual images\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(loc).independent(1), obs=xs)\n",
    "            # return the loc so we can visualize it later\n",
    "            return loc\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, xs, ys=None):\n",
    "        with pyro.iarange(\"data\", xs.size(0)):\n",
    "            \n",
    "            if ys is None:\n",
    "                alpha = self.encoder_y.forward(xs)\n",
    "                ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))                        \n",
    "            \n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, z_scale = self.encoder_z.forward([xs, ys])\n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "            \n",
    "    def classifier(self, xs):\n",
    "        alpha = self.encoder_y.forward(xs)\n",
    "        res, ind = torch.topk(alpha, 1)\n",
    "        ys = xs.new_zeros(alpha.size())\n",
    "        ys = ys.scatter_(1, ind, 1.0)\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T07:55:39.370186Z",
     "start_time": "2018-12-05T07:55:38.973623Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unsup_dataset = SegmentVectorDataset(video_list(), vocab_size=vocab_size)\n",
    "sup_dataset = LabeledSegmentDataset(unsup_dataset, pcache.get('labeled_segments'), categories=2)\n",
    "loader_params = {'shuffle': True}\n",
    "unsup_loader = DataLoader(unsup_dataset, batch_size=8, **loader_params)\n",
    "sup_loader = DataLoader(sup_dataset, batch_size=8, **loader_params)\n",
    "data_loaders = {\"unsup\": unsup_loader, \"sup\": sup_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T08:01:29.291494Z",
     "start_time": "2018-12-05T08:01:29.086552Z"
    }
   },
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "vae = VAE()\n",
    "optimizer = optim.ClippedAdam({\"lr\": 0.001, \"betas\": [0.9, 0.999]})\n",
    "svi = infer.SVI(vae.model, infer.config_enumerate(vae.guide), optimizer, loss=infer.Trace_ELBO())\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T07:57:09.654152Z",
     "start_time": "2018-12-05T07:57:09.593788Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_inference_for_epoch(data_loaders, losses, sup_batches, unsup_batches, periodic_interval_batches):\n",
    "    \"\"\"\n",
    "    runs the inference algorithm for an epoch\n",
    "    returns the values of all losses separately on supervised and unsupervised parts\n",
    "    \"\"\"\n",
    "    num_losses = len(losses)\n",
    "\n",
    "    # compute number of batches for an epoch\n",
    "    batches_per_epoch = sup_batches + unsup_batches\n",
    "\n",
    "    # initialize variables to store loss values\n",
    "    epoch_losses_sup = [0.] * num_losses\n",
    "    epoch_losses_unsup = [0.] * num_losses\n",
    "\n",
    "    # setup the iterators for training data loaders\n",
    "    sup_iter = iter(data_loaders[\"sup\"])\n",
    "    unsup_iter = iter(data_loaders[\"unsup\"])\n",
    "\n",
    "    # count the number of supervised batches seen in this epoch\n",
    "    ctr_sup = 0\n",
    "    for i in range(batches_per_epoch):\n",
    "\n",
    "        # whether this batch is supervised or not\n",
    "        is_supervised = (i % periodic_interval_batches == 1) and ctr_sup < sup_batches\n",
    "\n",
    "        # extract the corresponding batch\n",
    "        start = now()\n",
    "        if is_supervised:\n",
    "            (xs, ys, _) = next(sup_iter)\n",
    "            ctr_sup += 1\n",
    "        else:\n",
    "            xs, _ = next(unsup_iter)\n",
    "        #print('load: {:.04f}'.format(now() - start))\n",
    "\n",
    "        # run the inference for each loss with supervised or un-supervised\n",
    "        # data as arguments\n",
    "        start = now()\n",
    "        for loss_id in range(num_losses):\n",
    "            if is_supervised:\n",
    "                new_loss = losses[loss_id].step(xs, ys)\n",
    "                epoch_losses_sup[loss_id] += new_loss\n",
    "            else:\n",
    "                new_loss = losses[loss_id].step(xs)\n",
    "                epoch_losses_unsup[loss_id] += new_loss\n",
    "        #print('loss: {:.04f}'.format(now() - start))\n",
    "\n",
    "    # return the values of all losses\n",
    "    return epoch_losses_sup, epoch_losses_unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T07:57:10.428177Z",
     "start_time": "2018-12-05T07:57:10.385057Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(data_loader, classifier_fn):\n",
    "    \"\"\"\n",
    "    compute the accuracy over the supervised training set or the testing set\n",
    "    \"\"\"\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    # use the appropriate data loader\n",
    "    for (xs, ys, _) in data_loader:\n",
    "        # use classification function to compute all predictions for each batch\n",
    "        predictions.append(classifier_fn(xs))\n",
    "        actuals.append(ys)\n",
    "        \n",
    "    # compute the number of accurate predictions\n",
    "    accurate_preds = 0\n",
    "    for pred, act in zip(predictions, actuals):\n",
    "        for i in range(pred.size(0)):\n",
    "            v = torch.sum(pred[i] == act[i])\n",
    "            accurate_preds += (v.item() == pred[i].shape[0])\n",
    "\n",
    "    # calculate the accuracy between 0 and 1\n",
    "    accuracy = accurate_preds / (len(predictions) * len(predictions[0]))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T08:37:17.993307Z",
     "start_time": "2018-12-05T08:01:34.669549Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = [svi]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    sup_batches = len(sup_loader)\n",
    "    unsup_batches = len(sup_loader) * 100\n",
    "    epoch_losses_sup, epoch_losses_unsup = run_inference_for_epoch(\n",
    "        data_loaders, \n",
    "        losses, \n",
    "        sup_batches=sup_batches,\n",
    "        unsup_batches=unsup_batches,\n",
    "        periodic_interval_batches=100)\n",
    "    \n",
    "    # compute average epoch losses i.e. losses per example\n",
    "    avg_epoch_losses_sup = list(map(lambda v: v / sup_batches, epoch_losses_sup))\n",
    "    avg_epoch_losses_unsup = list(map(lambda v: v / unsup_batches, epoch_losses_unsup))\n",
    "    loss_history.append((avg_epoch_losses_sup, avg_epoch_losses_unsup))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        # store the loss and validation/testing accuracies in the logfile\n",
    "        str_loss_sup = \" \".join(map(lambda s: '{:.04f}'.format(s), avg_epoch_losses_sup))\n",
    "        str_loss_unsup = \" \".join(map(lambda s: '{:.04f}'.format(s), avg_epoch_losses_unsup))\n",
    "\n",
    "        str_print = \"{} epoch: avg losses {}\".format(epoch, \"{} {}\".format(str_loss_sup, str_loss_unsup))\n",
    "\n",
    "        test_accuracy = get_accuracy(data_loaders[\"sup\"], vae.classifier)\n",
    "        str_print += \", sup accuracy {:.04f}\".format(test_accuracy)\n",
    "\n",
    "        print(str_print)\n",
    "        pyro.get_param_store().save(\n",
    "            '/app/data/models/transcript_ssvae_weights_epoch{:05d}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-02T20:38:55.496Z"
    }
   },
   "outputs": [],
   "source": [
    "vae.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-02T20:38:55.497Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([l[0] for l in loss_history[10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-02T20:38:55.499Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([l[1] for l in loss_history[10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-02T20:38:55.500Z"
    }
   },
   "outputs": [],
   "source": [
    "get_accuracy(data_loaders['sup'], vae.classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
