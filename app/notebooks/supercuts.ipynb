{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Supercuts\" data-toc-modified-id=\"Supercuts-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Supercuts</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supercuts\n",
    "\n",
    "Our goal is to get intervals for a short supercut video of a certain person (e.g, Anderson Cooper) saying a funny sentence, like:\n",
    "\n",
    "```\n",
    "P = a person in the dataset\n",
    "sentence = \"Intel is great because they fund us.\"\n",
    "```\n",
    "\n",
    "We'll use `rekall` to get the candidate intervals and the caption index to get caption intervals. Make sure the caption index and `rekall` are installed in your Esper instance before running this notebook. If they aren't, the imports will fail.\n",
    "\n",
    "Strategy:\n",
    "1. Get all intervals where person P is on screen\n",
    "2. For each word W in sentence, create list of intervals for W and intersect with person P intervals\n",
    "3. Get all intervals where there is exactly one face on screen\n",
    "4. For each word W in sentence, intersect P + W intervals with one face intervals to get P + W alone intervals\n",
    "5. Pick one element from each P + W alone interval list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T01:02:25.472866Z",
     "start_time": "2018-11-07T01:02:25.436896Z"
    }
   },
   "outputs": [],
   "source": [
    "# import rekall\n",
    "from esper.rekall import *\n",
    "from rekall.interval_list import Interval, IntervalList\n",
    "from rekall.temporal_predicates import *\n",
    "from rekall.spatial_predicates import *\n",
    "\n",
    "# import caption search\n",
    "from esper.captions import *\n",
    "\n",
    "# import face identities for person search\n",
    "from query.models import Face, FaceIdentity\n",
    "\n",
    "# import esper widget for debugging\n",
    "from esper.prelude import esper_widget\n",
    "\n",
    "import random\n",
    "import os\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T01:02:25.510223Z",
     "start_time": "2018-11-07T01:02:25.476326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set these parameters for the notebook.\n",
    "person_name = \"Anderson Cooper\"\n",
    "sentence = \"Intel is great because they fund us\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T01:02:32.628107Z",
     "start_time": "2018-11-07T01:02:25.513508Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Get all intervals of person P\n",
    "person_intrvllists = qs_to_intrvllists(\n",
    "    FaceIdentity.objects\n",
    "        .filter(identity__name=person_name.lower())\n",
    "        .filter(probability__gt=0.99)\n",
    "        .annotate(video_id=F(\"face__shot__video_id\"))\n",
    "        .annotate(shot_id=F(\"face__shot_id\"))\n",
    "        .annotate(min_frame=F(\"face__shot__min_frame\"))\n",
    "        .annotate(max_frame=F(\"face__shot__max_frame\")),\n",
    "    schema={\n",
    "        'start': 'min_frame',\n",
    "        'end': 'max_frame',\n",
    "        'payload': 'shot_id'\n",
    "    })\n",
    "print(\"Got all occurrences of {}\".format(person_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T01:02:32.746102Z",
     "start_time": "2018-11-07T01:02:32.632381Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper function for 2. to convert caption search to dict mapping from video ID to IntervalList\n",
    "def caption_to_intrvllists(search_term, dilation=0, video_ids=None):\n",
    "    results = topic_search([search_term], dilation)\n",
    "    if video_ids == None:\n",
    "        videos = {v.id: v for v in Video.objects.all()}\n",
    "    else:\n",
    "        videos = {v.id: v for v in Video.objects.filter(id__in=video_ids).all()}\n",
    "    \n",
    "    def convert_time(k, t):\n",
    "        return int(t * videos[k].fps)\n",
    "    \n",
    "    segments_by_video = {}\n",
    "    flattened = [\n",
    "        (v.id, convert_time(v.id, l.start), convert_time(v.id, l.end)) \n",
    "        for v in results.documents if v.id in videos\n",
    "        for l in v.locations\n",
    "    ]\n",
    "    \n",
    "    for video_id, t1, t2 in flattened:\n",
    "        if video_id in segments_by_video:\n",
    "            segments_by_video[video_id].append((t1, t2, 0))\n",
    "        else:\n",
    "            segments_by_video[video_id] = [(t1, t2, 0)]\n",
    "    \n",
    "    for video in segments_by_video:\n",
    "        segments_by_video[video] = IntervalList(segments_by_video[video])\n",
    "        \n",
    "    print(\"Got all occurrences of the word {} by searching\".format(search_term))\n",
    "    \n",
    "    return segments_by_video\n",
    "\n",
    "def search_terms_lists(video_id):\n",
    "    valid_intervals = [[] for term in SEARCH_TERM_IDS]\n",
    "\n",
    "    doc_id = VIDEO_ID_TO_DOCUMENT_ID.get(video_id, None)\n",
    "    if doc_id is None:\n",
    "        return valid_intervals\n",
    "    for interval in DOCUMENT_DATA.token_intervals(doc_id, 0, DOCUMENTS[doc_id].duration):\n",
    "        for token in interval.tokens:\n",
    "            if token in SEARCH_TERM_IDS:\n",
    "                index = SEARCH_TERM_IDS.index(token)\n",
    "                valid_intervals[index].append(\n",
    "                    (interval.start,\n",
    "                    interval.end,\n",
    "                    0))\n",
    "    return valid_intervals\n",
    "\n",
    "# scans for search terms across videos in parallel\n",
    "def scan_for_search_terms_intrvllist(search_terms, video_ids, dilation=0):\n",
    "    search_term_lexicon_ids = [LEXICON[term].id for term in search_terms]\n",
    "    \n",
    "    global SEARCH_TERM_IDS\n",
    "    SEARCH_TERM_IDS = search_term_lexicon_ids\n",
    "    \n",
    "    with Pool(os.cpu_count()) as pool:\n",
    "        results = pool.map(search_terms_lists, video_ids)\n",
    "    \n",
    "    search_terms_intrvllists = [{} for term in search_terms]\n",
    "    videos = {v.id: v for v in Video.objects.filter(id__in=video_ids).all()}\n",
    "    def convert_time(k, t):\n",
    "        return int(t * videos[k].fps)\n",
    "    \n",
    "    for video_id, result in zip(video_ids, results):\n",
    "        for i, term in enumerate(search_terms):\n",
    "            term_result = result[i]\n",
    "            interval_list = IntervalList([\n",
    "                (convert_time(video_id, start - dilation),\n",
    "                convert_time(video_id, end + dilation),\n",
    "                payload)\n",
    "                for start, end, payload in term_result\n",
    "            ])\n",
    "            if interval_list.size() > 0:\n",
    "                search_terms_intrvllists[i][video_id] = interval_list\n",
    "        \n",
    "    print(\"Got all occurrences of the words {} by scanning\".format(search_terms))\n",
    "    \n",
    "    return search_terms_intrvllists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T01:03:55.346009Z",
     "start_time": "2018-11-07T01:02:32.749425Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. for each word W in sentence, intersect list of intervals W with list of intervals of P alone\n",
    "\n",
    "# # Get extremely frequent words\n",
    "EXTREMELY_FREQUENT_WORDS = {\n",
    "    w.token for w in caption_util.frequent_words(LEXICON, 99.997)\n",
    "}\n",
    "\n",
    "# Split words into words to search by index and words to scan through documents for\n",
    "words = [word.upper() for word in sentence.split()]\n",
    "words_to_scan = set()\n",
    "words_to_search_by_index = set()\n",
    "for word in words:\n",
    "    if word in EXTREMELY_FREQUENT_WORDS:\n",
    "        words_to_scan.add(word)\n",
    "    else:\n",
    "        words_to_search_by_index.add(word)\n",
    "words_to_scan = list(words_to_scan)\n",
    "words_to_search_by_index = list(words_to_search_by_index)\n",
    "\n",
    "video_ids = list(person_intrvllists.keys())\n",
    "\n",
    "scanned_words = scan_for_search_terms_intrvllist(words_to_scan, video_ids)\n",
    "searched_words = [\n",
    "    caption_to_intrvllists(word, video_ids=video_ids) for word in words_to_search_by_index \n",
    "]\n",
    "\n",
    "sentence_intrvllists = [\n",
    "    scanned_words[words_to_scan.index(word)]\n",
    "    if word in words_to_scan else\n",
    "    searched_words[words_to_search_by_index.index(word)]\n",
    "    for word in words\n",
    "]\n",
    "\n",
    "# This will be a list of tuples (video id, start frame, end frame); there will be one tuple for each word\n",
    "person_with_sentence_intrvllists = []\n",
    "for i, word_intrvllists in enumerate(sentence_intrvllists):\n",
    "    person_with_word_intrvllists = {}\n",
    "    for video in person_intrvllists:\n",
    "        if video in word_intrvllists:\n",
    "            person_list = person_intrvllists[video]\n",
    "            word_list = word_intrvllists[video]\n",
    "            \n",
    "            intersection_list = person_list.overlaps(word_list)\n",
    "            \n",
    "            if intersection_list.size() > 0:\n",
    "                person_with_word_intrvllists[video] = intersection_list\n",
    "    if person_with_word_intrvllists == {}:\n",
    "        print(\"Could not find instance of person {} with word {}\".format(person_name, words[i]))\n",
    "    person_with_sentence_intrvllists.append(person_with_word_intrvllists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T01:03:56.226032Z",
     "start_time": "2018-11-07T01:03:55.349592Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Get all intervals where there is exactly one face on screen\n",
    "relevant_shots = set()\n",
    "for person_with_word_intrvllists in person_with_sentence_intrvllists:\n",
    "    for intrvllist in list(person_with_word_intrvllists.values()):\n",
    "        for interval in intrvllist.get_intervals():\n",
    "            relevant_shots.add(interval.get_payload())\n",
    "\n",
    "faces = Face.objects.filter(shot__in=list(relevant_shots)) \\\n",
    "        .annotate(video_id=F('shot__video_id')) \\\n",
    "        .annotate(min_frame=F('shot__min_frame')) \\\n",
    "        .annotate(max_frame=F('shot__max_frame'))\n",
    "face_vids = {}\n",
    "for face in faces:\n",
    "    video_id = face.video_id\n",
    "    shot_id = face.shot_id\n",
    "    if video_id not in face_vids:\n",
    "        face_vids[video_id] = {}\n",
    "    if shot_id not in face_vids[video_id]:\n",
    "        face_vids[video_id][shot_id] = {'min_frame': face.min_frame, 'max_frame': face.max_frame, 'objects': []}\n",
    "    face_vids[video_id][shot_id]['objects'].append(\n",
    "        {'x1': face.bbox_x1, 'y1': face.bbox_y1, 'x2': face.bbox_x2, 'y2': face.bbox_y2})\n",
    "\n",
    "oneface_intrvllists = {}\n",
    "for video in face_vids:\n",
    "    oneface_intrvllist = IntervalList([(\n",
    "        shot['min_frame'], \n",
    "        shot['max_frame'],\n",
    "        {\n",
    "            'type': 'bbox_list',\n",
    "            'objects': shot['objects']\n",
    "        }) for shot in list(face_vids[video].values())]).filter(exactly(1))\n",
    "    if oneface_intrvllist.size() > 0:\n",
    "        oneface_intrvllists[video] = oneface_intrvllist\n",
    "\n",
    "print(\"Got all intervals where there is exactly one face on screen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T01:03:56.398065Z",
     "start_time": "2018-11-07T01:03:56.229194Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. For each word W in sentence, intersect P with word intervals with one face intervals\n",
    "person_with_sentence_alone_intrvllists = []\n",
    "for person_with_word_intrvllists in person_with_sentence_intrvllists:\n",
    "    person_with_word_alone_intrvllists = {}\n",
    "    for video in person_with_word_intrvllists:\n",
    "        if video in oneface_intrvllists:\n",
    "            person_alone_intrvllist = person_with_word_intrvllists[video].overlaps(oneface_intrvllists[video])\n",
    "            if person_alone_intrvllist.size() > 0:\n",
    "                person_with_word_alone_intrvllists[video] = person_alone_intrvllist\n",
    "    person_with_sentence_alone_intrvllists.append(person_with_word_alone_intrvllists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T01:03:56.458243Z",
     "start_time": "2018-11-07T01:03:56.400976Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Pick one element from each P + W alone interval list\n",
    "supercut_intervals_all = []\n",
    "for i, person_with_word_alone_intrvllist in enumerate(person_with_sentence_alone_intrvllists):\n",
    "    supercut_intervals = []\n",
    "    for video in list(person_with_word_alone_intrvllist.keys()):\n",
    "        intrvllist = person_with_word_alone_intrvllist[video]\n",
    "        for interval in intrvllist.get_intervals():\n",
    "            supercut_intervals.append((video, interval.get_start(), interval.get_end()))\n",
    "    supercut_intervals_all.append(supercut_intervals)\n",
    "    if len(supercut_intervals) == 0:\n",
    "        print(\"Could not find interval of person {} alone saying {}\".format(person_name, words[i]))\n",
    "\n",
    "# Select one element from each interval\n",
    "supercut_intervals = [random.choice(intervals) for intervals in supercut_intervals_all]\n",
    "print(\"Supercut intervals: \", supercut_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-07T01:03:56.592577Z",
     "start_time": "2018-11-07T01:03:56.461948Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the supercut intervals in Esper widget for debugging\n",
    "supercut_intrvllists = {}\n",
    "for video, start, end in supercut_intervals:\n",
    "    supercut_intrvllists[video] = IntervalList([(start, end, 0)])\n",
    "esper_widget(intrvllists_to_result(supercut_intrvllists,\n",
    "                                   video_order = [video for video, start, end in supercut_intervals]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
