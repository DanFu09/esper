{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversations query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T01:25:32.846357Z",
     "start_time": "2019-01-14T01:25:32.817509Z"
    }
   },
   "outputs": [],
   "source": [
    "from rekall.interval_list import IntervalList, Interval\n",
    "from rekall.temporal_predicates import overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Identity Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T23:17:23.104576Z",
     "start_time": "2019-01-13T23:17:22.690209Z"
    }
   },
   "outputs": [],
   "source": [
    "def conversationsq(video_name):\n",
    "    from query.models import FaceCharacterActor, Shot\n",
    "    from rekall.video_interval_collection import VideoIntervalCollection\n",
    "    from rekall.parsers import in_array, bbox_payload_parser, merge_dict_parsers, dict_payload_parser\n",
    "    from rekall.merge_ops import payload_plus\n",
    "    from rekall.payload_predicates import payload_satisfies\n",
    "    from rekall.spatial_predicates import scene_graph\n",
    "    from esper.rekall import intrvllists_to_result_bbox\n",
    "    from query.models import Face\n",
    "    from rekall.video_interval_collection import VideoIntervalCollection\n",
    "    from rekall.parsers import in_array, bbox_payload_parser\n",
    "    from rekall.merge_ops import payload_plus, merge_named_payload, payload_second\n",
    "    from esper.rekall import intrvllists_to_result_bbox\n",
    "    from rekall.payload_predicates import payload_satisfies\n",
    "    from rekall.list_predicates import length_at_most\n",
    "    from rekall.logical_predicates import and_pred, or_pred, true_pred\n",
    "    from rekall.spatial_predicates import scene_graph, make_region\n",
    "    from rekall.temporal_predicates import before, after, overlaps\n",
    "    from rekall.bbox_predicates import height_at_least\n",
    "    from esper.rekall import intrvllists_to_result, intrvllists_to_result_with_objects, add_intrvllists_to_result\n",
    "    from esper.prelude import esper_widget\n",
    "    from rekall.interval_list import Interval, IntervalList\n",
    "    \n",
    "    # faces are sampled every 12 frames\n",
    "    ONE_FRAME = 1\n",
    "   \n",
    "\n",
    "    faces_with_character_actor_qs = FaceCharacterActor.objects.annotate(\n",
    "        min_frame=F('face__frame__number'),\n",
    "        max_frame=F('face__frame__number'),\n",
    "        video_id=F('face__frame__video_id'),\n",
    "        bbox_x1=F('face__bbox_x1'),\n",
    "        bbox_y1=F('face__bbox_y1'),\n",
    "        bbox_x2=F('face__bbox_x2'),\n",
    "        bbox_y2=F('face__bbox_y2'),\n",
    "        character_name=F('characteractor__character__name')\n",
    "    ).filter(face__frame__video__name__contains=video_name)\n",
    "    \n",
    "    faces_with_identity = VideoIntervalCollection.from_django_qs(\n",
    "        faces_with_character_actor_qs,\n",
    "        with_payload=in_array(merge_dict_parsers([\n",
    "            bbox_payload_parser(VideoIntervalCollection.django_accessor),\n",
    "            dict_payload_parser(VideoIntervalCollection.django_accessor, { 'character': 'character_name' }),\n",
    "        ]))\n",
    "    ).coalesce(payload_merge_op=payload_plus)\n",
    "\n",
    "    shots_qs = Shot.objects.filter(cinematic=True)\n",
    "    shots = VideoIntervalCollection.from_django_qs(shots_qs)\n",
    "    \n",
    "    def payload_unique_characters(payload1, payload2):\n",
    "        if 'characters' not in payload1[0]:\n",
    "            unique_characters = set([p['character'] for p in payload1])\n",
    "            for p in payload2:\n",
    "                unique_characters.add(p['character'])\n",
    "            payload1[0]['characters'] = list(unique_characters)\n",
    "        else:\n",
    "            unique_characters = set([p['character'] for p in payload2])\n",
    "            unique_characters.update(payload1[0]['characters'])\n",
    "            payload1[0]['characters'] = list(unique_characters)\n",
    "        return payload1\n",
    "        \n",
    "    shots_with_faces = shots.merge(faces_with_identity, \n",
    "                                  predicate=overlaps(), \n",
    "                                  payload_merge_op=payload_second)\n",
    "                                   \n",
    "    shots_with_faces = shots_with_faces.coalesce(payload_merge_op=payload_unique_characters)\n",
    "\n",
    "    def cross_product_faces(intrvl1, intrvl2):\n",
    "        payload1 = intrvl1.get_payload()\n",
    "        payload2 = intrvl2.get_payload()\n",
    "        chrtrs1 = payload1[0]['characters'] if 'characters' in payload1[0] else list(set([p['character'] for p in payload1]))\n",
    "        chrtrs2 = payload2[0]['characters'] if 'characters' in payload2[0] else list(set([p['character'] for p in payload2]))\n",
    "        new_intervals = []\n",
    "        for i in chrtrs1:\n",
    "            for j in chrtrs2:\n",
    "                if i!=j:\n",
    "                    new_payload = {'A': i, 'B': j}\n",
    "                    start = min(intrvl1.start, intrvl2.start)\n",
    "                    end = max(intrvl1.end, intrvl2.end)\n",
    "                    new_intervals.append(Interval(start, end, {'A': i, 'B': j}))\n",
    "\n",
    "        return new_intervals\n",
    "        \n",
    "    def faces_equal(payload1, payload2):\n",
    "        p1 = [payload1]\n",
    "        if type(payload1) is dict and 'chrs' in payload1:\n",
    "            p1 = payload1['chrs']\n",
    "        elif type(payload1) is list:\n",
    "            p1 = payload1\n",
    "        \n",
    "        p2 = [payload2]\n",
    "        if type(payload2) is dict and 'chrs' in payload2:\n",
    "            p2 = payload2['chrs']\n",
    "        elif type(payload2) is list:\n",
    "            p2 = payload2\n",
    "            \n",
    "        payload1 = p1\n",
    "        payload2 = p2\n",
    "        \n",
    "        if type(payload1) is not list and type(payload1) is not list:\n",
    "            return (payload1['A'] == payload2['A'] and payload1['B'] == payload2['B']) or (payload1['A'] == payload2['B'] and payload1['B'] == payload2['A'])\n",
    "        elif type(payload1) is list and type(payload2) is list:\n",
    "            for i in payload1:\n",
    "                for j in payload2:\n",
    "                    if i['A'] == j['A'] and i['B'] == j['B']:\n",
    "                        return True\n",
    "                    if i['A'] == j['B'] and i['B'] == j['A']:\n",
    "                        return True\n",
    "        elif type(payload1) is list:\n",
    "            for i in payload1:\n",
    "                if i['A'] == payload2['A'] and i['B'] == payload2['B']:\n",
    "                    return True\n",
    "                if i['A'] == payload2['B'] and i['B'] == payload2['A']:\n",
    "                    return True\n",
    "        else:\n",
    "            for i in payload2:\n",
    "                if i['A'] == payload1['A'] and i['B'] == payload1['B']:\n",
    "                    return True\n",
    "                if i['A'] == payload1['B'] and i['B'] == payload1['A']:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def times_equal(intrvl1, intrvl2):\n",
    "        return (intrvl1.start >= intrvl2.start and intrvl1.end <= intrvl2.end) or (intrvl2.start >= intrvl1.start and intrvl2.end <= intrvl1.end)\n",
    "        \n",
    "    def times_overlap(intrvl1, intrvl2):\n",
    "        return intrvl1.start <= intrvl2.end and intrvl2.start <= intrvl1.end\n",
    "    \n",
    "    def merge_to_list(payload1, payload2):\n",
    "        p1 = payload1 if type(payload1) is list else [payload1]\n",
    "        p2 = payload2 if type(payload2) is list else [payload2]\n",
    "        return p1+p2\n",
    "    \n",
    "    def count_shots(payload1, payload2):\n",
    "        p1 = [payload1]\n",
    "        if type(payload1) is dict and 'chrs' in payload1:\n",
    "            p1 = payload1['chrs']\n",
    "        elif type(payload1) is list:\n",
    "            p1 = payload1\n",
    "        \n",
    "        p2 = [payload2]\n",
    "        if type(payload2) is dict and 'chrs' in payload2:\n",
    "            p2 = payload2['chrs']\n",
    "        elif type(payload2) is list:\n",
    "            p2 = payload2\n",
    "        \n",
    "        p1_shots = payload1['shots'] if type(payload1) is dict and 'shots' in payload1 else 1\n",
    "        p2_shots = payload2['shots'] if type(payload2) is dict and 'shots' in payload2 else 1\n",
    "        return {'shots': p1_shots + p2_shots, 'chrs': p1 + p2}\n",
    "        \n",
    "    def shots_equal(payload1, payload2):\n",
    "        p1 = [payload1]\n",
    "        if type(payload1) is dict and 'chrs' in payload1:\n",
    "            p1 = payload1['chrs']\n",
    "        elif type(payload1) is list:\n",
    "            p1 = payload1\n",
    "        \n",
    "        p2 = [payload2]\n",
    "        if type(payload2) is dict and 'chrs' in payload2:\n",
    "            p2 = payload2['chrs']\n",
    "        elif type(payload2) is list:\n",
    "            p2 = payload2\n",
    "        \n",
    "        p1_shots = payload1['shots'] if type(payload1) is dict and 'shots' in payload1 else 1\n",
    "        p2_shots = payload2['shots'] if type(payload2) is dict and 'shots' in payload2 else 1\n",
    "\n",
    "        shots = p1_shots if p1_shots > p2_shots else p2_shots\n",
    "        return {'shots': shots, 'chrs': p1 + p2}\n",
    "\n",
    "    two_shots = shots_with_faces.join(shots_with_faces, predicate=after(max_dist=ONE_FRAME, min_dist=ONE_FRAME), \n",
    "                                merge_op=cross_product_faces)\n",
    "\n",
    "    convs = two_shots.coalesce(predicate=times_equal, payload_merge_op=merge_to_list)\n",
    "    convs = convs.coalesce(predicate=payload_satisfies(faces_equal, arity=2), payload_merge_op=count_shots)\n",
    "        \n",
    "    adjacent_seq = convs.merge(convs, predicate=and_pred(after(max_dist=ONE_FRAME, min_dist=ONE_FRAME), payload_satisfies(faces_equal, arity=2), arity=2), payload_merge_op=count_shots)\n",
    "    convs = convs.set_union(adjacent_seq)\n",
    "    # convs = convs.coalesce(predicate=times_equal, payload_merge_op=shots_equal)\n",
    "\n",
    "    def filter_fn(intvl):\n",
    "        payload = intvl.get_payload()\n",
    "        if type(payload) is dict and 'shots' in payload:\n",
    "            return payload['shots'] >= 2\n",
    "        return False \n",
    "    \n",
    "    convs = convs.filter(filter_fn)\n",
    "    convs = convs.coalesce(predicate=times_overlap)\n",
    "\n",
    "    for video_id in convs.intervals.keys():\n",
    "        print(video_id)\n",
    "        intvllist = convs.get_intervallist(video_id)\n",
    "        for intvl in intvllist.get_intervals():\n",
    "            print(intvl.payload)\n",
    "            print(str(intvl.start) + ':' + str(intvl.end))\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Numbers\n",
    "\n",
    "```\n",
    "Godfather Part iii\n",
    "Precision:  0.7562506843815017\n",
    "Recall:  0.9028280099350734\n",
    "Precision Per Item:  0.5555555555555556\n",
    "Recall Per Item:  1.0\n",
    "\n",
    "Apollo 13\n",
    "Precision:  0.9801451458304806\n",
    "Recall:  0.7144069065322621\n",
    "Precision Per Item:  1.0\n",
    "Recall Per Item:  0.9333333333333333\n",
    "\n",
    "Harry Potter 2\n",
    "Precision:  0.8393842579146094\n",
    "Recall:  0.5495863839497955\n",
    "Precision Per Item:  0.75\n",
    "Recall Per Item:  0.875\n",
    "\n",
    "Fight Club\n",
    "Precision:  0.7107177395618719\n",
    "Recall:  0.8310226155358899\n",
    "Precision Per Item:  0.6666666666666666\n",
    "Recall Per Item:  0.9285714285714286\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Face Embeddings\n",
    "\n",
    "Strategy: cluster embeddings by shot (number of clusters is max number of people in the shot), then compare cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T01:31:04.516266Z",
     "start_time": "2019-01-14T01:31:04.241962Z"
    }
   },
   "outputs": [],
   "source": [
    "def conversationsq_face_embeddings(video_name):\n",
    "    from query.models import FaceCharacterActor, Shot\n",
    "    from rekall.video_interval_collection import VideoIntervalCollection\n",
    "    from rekall.parsers import in_array, bbox_payload_parser, merge_dict_parsers, dict_payload_parser\n",
    "    from rekall.merge_ops import payload_plus\n",
    "    from rekall.payload_predicates import payload_satisfies\n",
    "    from rekall.spatial_predicates import scene_graph\n",
    "    from esper.rekall import intrvllists_to_result_bbox\n",
    "    from query.models import Face\n",
    "    from rekall.video_interval_collection import VideoIntervalCollection\n",
    "    from rekall.parsers import in_array, bbox_payload_parser\n",
    "    from rekall.merge_ops import payload_plus, merge_named_payload, payload_second\n",
    "    from esper.rekall import intrvllists_to_result_bbox\n",
    "    from rekall.payload_predicates import payload_satisfies\n",
    "    from rekall.list_predicates import length_at_most\n",
    "    from rekall.logical_predicates import and_pred, or_pred, true_pred\n",
    "    from rekall.spatial_predicates import scene_graph, make_region\n",
    "    from rekall.temporal_predicates import before, after, overlaps, equal\n",
    "    from rekall.bbox_predicates import height_at_least\n",
    "    from esper.rekall import intrvllists_to_result, intrvllists_to_result_with_objects, add_intrvllists_to_result\n",
    "    from esper.prelude import esper_widget\n",
    "    from rekall.interval_list import Interval, IntervalList\n",
    "    import esper.face_embeddings as face_embeddings\n",
    "    \n",
    "    EMBEDDING_EQUALITY_THRESHOLD = 1.\n",
    "    ONE_FRAME = 1\n",
    "    \n",
    "    faces_qs = Face.objects.annotate(\n",
    "        min_frame=F('frame__number'),\n",
    "        max_frame=F('frame__number'),\n",
    "        video_id=F('frame__video_id')\n",
    "    ).filter(frame__video__name__contains=video_name, frame__regularly_sampled=True)\n",
    "    \n",
    "    faces_per_frame = VideoIntervalCollection.from_django_qs(\n",
    "        faces_qs,\n",
    "        with_payload=in_array(merge_dict_parsers([\n",
    "            bbox_payload_parser(VideoIntervalCollection.django_accessor),\n",
    "            dict_payload_parser(VideoIntervalCollection.django_accessor, { 'face_id': 'id' }),\n",
    "        ]))\n",
    "    ).coalesce(payload_merge_op=payload_plus)\n",
    "    \n",
    "    shots_qs = Shot.objects.filter(cinematic=True)\n",
    "    shots = VideoIntervalCollection.from_django_qs(shots_qs)\n",
    "    \n",
    "    shots_with_faces = shots.merge(\n",
    "        faces_per_frame, \n",
    "        predicate=overlaps(), \n",
    "        payload_merge_op=lambda shot_id, faces_in_frame: [faces_in_frame]\n",
    "    ).coalesce(payload_merge_op=payload_plus)\n",
    "   \n",
    "    def cluster_center(face_ids):\n",
    "        mean_embedding = face_embeddings.mean(face_ids)\n",
    "        dists = face_embeddings.dist(face_ids, [mean_embedding])\n",
    "        return min(zip(dists, face_ids))[1]\n",
    "\n",
    "    def cluster_and_compute_centers(faces_in_frame_list):\n",
    "        num_people = max(len(faces_in_frame) for faces_in_frame in faces_in_frame_list)\n",
    "        face_ids = [face['face_id'] for faces_in_frame in faces_in_frame_list for face in faces_in_frame]\n",
    "        if num_people == 1:\n",
    "            clusters = [(fid, 0) for fid in face_ids]\n",
    "        else:\n",
    "            clusters = face_embeddings.kmeans(face_ids, num_people)\n",
    "        centers = [\n",
    "            (\n",
    "                cluster_center([\n",
    "                    face_id\n",
    "                    for face_id, cluster_id in clusters\n",
    "                    if cluster_id == i\n",
    "                ]), [\n",
    "                    face_id\n",
    "                    for face_id, cluster_id in clusters\n",
    "                    if cluster_id == i\n",
    "                ]\n",
    "            )\n",
    "            for i in range(num_people)\n",
    "        ]\n",
    "        return centers\n",
    "\n",
    "    print(\"Clusters computed\")\n",
    "    \n",
    "    shots_with_centers = shots_with_faces.map(\n",
    "        lambda intrvl: (intrvl.start, intrvl.end, cluster_and_compute_centers(intrvl.payload))\n",
    "    )\n",
    "    \n",
    "    def same_face(center1, center2):\n",
    "        return face_embeddings.dist([center1], target_ids=[center2])[0] < EMBEDDING_EQUALITY_THRESHOLD\n",
    "\n",
    "    def cross_product_faces(intrvl1, intrvl2):\n",
    "        payload1 = intrvl1.get_payload()\n",
    "        payload2 = intrvl2.get_payload()\n",
    "        payload = []\n",
    "        for cluster1 in payload1:\n",
    "            for cluster2 in payload2:\n",
    "                if not same_face(cluster1[0], cluster2[0]):\n",
    "                    new_payload = {'A': cluster1, 'B': cluster2}\n",
    "                    payload.append(new_payload)\n",
    "\n",
    "        return [(min(intrvl1.get_start(), intrvl2.get_start()),\n",
    "                 max(intrvl1.get_end(), intrvl2.get_end()), {\n",
    "            'chrs': payload,\n",
    "            'shots': 1\n",
    "        })]\n",
    "    \n",
    "    two_shots = shots_with_centers.join(\n",
    "        shots_with_centers,\n",
    "        predicate=after(max_dist=ONE_FRAME, min_dist=ONE_FRAME), \n",
    "        merge_op=cross_product_faces\n",
    "    )\n",
    " \n",
    "    print(\"Cross product done\")\n",
    "\n",
    "    def faces_equal(payload1, payload2):\n",
    "        for face_pair1 in payload1['chrs']:\n",
    "            for face_pair2 in payload2['chrs']:\n",
    "                if (same_face(face_pair1['A'][0], face_pair2['A'][0]) and\n",
    "                    same_face(face_pair1['B'][0], face_pair2['B'][0])):\n",
    "                    return True\n",
    "                if (same_face(face_pair1['A'][0], face_pair2['B'][0]) and\n",
    "                    same_face(face_pair1['B'][0], face_pair2['A'][0])):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    convs = two_shots.coalesce(\n",
    "        predicate=payload_satisfies(faces_equal, arity=2),\n",
    "        payload_merge_op = lambda payload1, payload2: {\n",
    "            'chrs': payload1['chrs'] + payload2['chrs'],\n",
    "            'shots': payload1['shots'] + payload2['shots']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Coalesce done\")    \n",
    "        \n",
    "    adjacent_seq = convs.merge(\n",
    "        convs,\n",
    "        predicate=and_pred(\n",
    "            after(max_dist=ONE_FRAME, min_dist=ONE_FRAME),\n",
    "            payload_satisfies(faces_equal, arity=2),\n",
    "            arity=2),\n",
    "        payload_merge_op = lambda payload1, payload2: {\n",
    "            'chrs': payload1['chrs'] + payload2['chrs'],\n",
    "            'shots': payload1['shots'] + payload2['shots']\n",
    "        },\n",
    "        working_window=1\n",
    "    )\n",
    "    convs = convs.set_union(adjacent_seq)\n",
    "    # convs = convs.coalesce(predicate=times_equal, payload_merge_op=shots_equal)\n",
    "    \n",
    "    print(\"Two-shot adjacencies done\")\n",
    "\n",
    "    def filter_fn(intvl):\n",
    "        payload = intvl.get_payload()\n",
    "        if type(payload) is dict and 'shots' in payload:\n",
    "            return payload['shots'] >= 2\n",
    "        return False \n",
    "    \n",
    "    convs = convs.filter(filter_fn)\n",
    "    convs = convs.coalesce()\n",
    "    \n",
    "    print(\"Final filter done\")\n",
    "\n",
    "#     for video_id in convs.intervals.keys():\n",
    "#         print(video_id)\n",
    "#         intvllist = convs.get_intervallist(video_id)\n",
    "#         for intvl in intvllist.get_intervals():\n",
    "#             print(intvl.payload)\n",
    "#             print(str(intvl.start) + ':' + str(intvl.end))\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T00:33:08.531865Z",
     "start_time": "2019-01-14T00:32:50.082095Z"
    }
   },
   "outputs": [],
   "source": [
    "convs = conversationsq_face_embeddings('apollo 13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T00:33:10.901251Z",
     "start_time": "2019-01-14T00:33:10.853784Z"
    }
   },
   "outputs": [],
   "source": [
    "convs.get_intervallist(15).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T01:25:38.481869Z",
     "start_time": "2019-01-14T01:25:38.427476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Returns precision, recall, precision_per_item, recall_per_item\n",
    "def compute_statistics(query_intrvllists, ground_truth_intrvllists):\n",
    "    total_query_time = 0\n",
    "    total_query_segments = 0\n",
    "    total_ground_truth_time = 0\n",
    "    total_ground_truth_segments = 0\n",
    "    \n",
    "    for video in query_intrvllists:\n",
    "        total_query_time += query_intrvllists[video].coalesce().get_total_time()\n",
    "        total_query_segments += query_intrvllists[video].size()\n",
    "    for video in ground_truth_intrvllists:\n",
    "        total_ground_truth_time += ground_truth_intrvllists[video].coalesce().get_total_time()\n",
    "        total_ground_truth_segments += ground_truth_intrvllists[video].size()\n",
    "        \n",
    "    total_overlap_time = 0\n",
    "    overlapping_query_segments = 0\n",
    "    overlapping_ground_truth_segments = 0\n",
    "    \n",
    "    for video in query_intrvllists:\n",
    "        if video in ground_truth_intrvllists:\n",
    "            query_list = query_intrvllists[video]\n",
    "            gt_list = ground_truth_intrvllists[video]\n",
    "            \n",
    "            total_overlap_time += query_list.overlaps(gt_list).coalesce().get_total_time()\n",
    "            overlapping_query_segments += query_list.filter_against(gt_list, predicate=overlaps()).size()\n",
    "            overlapping_ground_truth_segments += gt_list.filter_against(query_list, predicate=overlaps()).size()\n",
    "    \n",
    "    if total_query_time == 0:\n",
    "        precision = 1.0\n",
    "        precision_per_item = 1.0\n",
    "    else:\n",
    "        precision = total_overlap_time / total_query_time\n",
    "        precision_per_item = overlapping_query_segments / total_query_segments\n",
    "    \n",
    "    if total_ground_truth_time == 0:\n",
    "        recall = 1.0\n",
    "        recall_per_item = 1.0\n",
    "    else:\n",
    "        recall = total_overlap_time / total_ground_truth_time\n",
    "        recall_per_item = overlapping_ground_truth_segments / total_ground_truth_segments\n",
    "    \n",
    "    return precision, recall, precision_per_item, recall_per_item\n",
    "\n",
    "def print_statistics(query_intrvllists, ground_truth_intrvllists):\n",
    "    precision, recall, precision_per_item, recall_per_item = compute_statistics(\n",
    "        query_intrvllists, ground_truth_intrvllists)\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Precision Per Item: \", precision_per_item)\n",
    "    print(\"Recall Per Item: \", recall_per_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T01:31:51.540601Z",
     "start_time": "2019-01-14T01:31:33.449593Z"
    }
   },
   "outputs": [],
   "source": [
    "apollo_convs = conversationsq_face_embeddings(\"apollo 13\")\n",
    "apollo_data = [\n",
    "    (2578, 4100), (4244, 4826), (5098, 5828), (7757, 9546),\n",
    "    (9602, 10300), (12393, 12943), (13088, 13884), (14146, 15212),\n",
    "    (15427, 16116), (18040, 19198), (20801, 23368), (24572, 26185),\n",
    "    (26735, 28753), (29462, 30873), (31768, 34618)]\n",
    "apollo_gt = {15: IntervalList([Interval(start, end, payload=None) for (start,end) in apollo_data])}\n",
    "print_statistics({15: apollo_convs.filter(lambda intrvl: intrvl.start < 34618).get_intervallist(15)}, apollo_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T01:31:27.133583Z",
     "start_time": "2019-01-14T01:31:08.808338Z"
    }
   },
   "outputs": [],
   "source": [
    "godfather_convs = conversationsq_face_embeddings(\"the godfather part iii\")\n",
    "godfather_data = [(12481, 13454), (13673, 14729), (16888, 17299), (21101, 27196),\n",
    "    (27602, 29032), (29033, 33204), (34071, 41293), (41512, 43103)]\n",
    "godfather_gt = {216: IntervalList([Interval(start, end, payload=None) for (start,end) in godfather_data])}\n",
    "print_statistics({216: godfather_convs.filter(lambda intrvl: intrvl.start < 43103).get_intervallist(216)}, \n",
    "                 godfather_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T01:32:16.058456Z",
     "start_time": "2019-01-14T01:31:56.424718Z"
    }
   },
   "outputs": [],
   "source": [
    "hp2_convs = conversationsq_face_embeddings('harry potter and the chamber of secrets')\n",
    "hp2_query = hp2_convs.filter(lambda inv: inv.start < 20308)\n",
    "hp2_query = {'374': hp2_query.get_intervallist(374)}\n",
    "hp2_data = [(2155, 4338), (4687, 6188), (6440, 10134), (12921, 13151), (16795, 17370),\n",
    "            (17766, 18021), (18102, 19495), (19622, 20308)]\n",
    "hp2_gt = {'374': IntervalList([Interval(start, end, payload=None) for (start,end) in hp2_data])}\n",
    "print_statistics(hp2_query, hp2_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T01:32:41.609487Z",
     "start_time": "2019-01-14T01:32:23.275148Z"
    }
   },
   "outputs": [],
   "source": [
    "fc_query = conversationsq_face_embeddings('fight club')\n",
    "fc_query = fc_query.filter(lambda inv: inv.start < 58258)\n",
    "fc_query = {'61': fc_query.get_intervallist(61)}\n",
    "fc_data = [(4698, 5602), (6493, 6865), (8670, 9156), (9517, 10908), (11087, 13538), (22039, 24188),\n",
    "           (25603, 27656), (31844, 32812), (32918, 33451), (33698, 35363), (42072, 45143),\n",
    "           (45272, 46685), (49162, 50618), (56830, 58258)]\n",
    "fc_gt = {'61': IntervalList([Interval(start, end, payload=None) for (start,end) in fc_data])}\n",
    "print_statistics(fc_query, fc_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with threshold of 0.9:\n",
    "```\n",
    "Precision:  0.9390051766824218\n",
    "Recall:  0.8327760866310694\n",
    "Precision Per Item:  0.9411764705882353\n",
    "Recall Per Item:  1.0\n",
    "Precision:  0.7085401799565622\n",
    "Recall:  0.7960695455139658\n",
    "Precision Per Item:  0.6\n",
    "Recall Per Item:  0.875\n",
    "Precision:  0.7528127623845507\n",
    "Recall:  0.8525244841684891\n",
    "Precision Per Item:  0.5625\n",
    "Recall Per Item:  1.0\n",
    "Precision:  0.6229706390328152\n",
    "Recall:  0.7093411996066863\n",
    "Precision Per Item:  0.6451612903225806\n",
    "Recall Per Item:  0.9285714285714286\n",
    "\n",
    "Average precision: 75.6\n",
    "Average recall: 79.8\n",
    "```\n",
    "\n",
    "Results with a threshold of 1.0:\n",
    "```\n",
    "Precision:  0.9040439021791251\n",
    "Recall:  0.8467488397624632\n",
    "Precision Per Item:  0.8888888888888888\n",
    "Recall Per Item:  1.0\n",
    "Precision:  0.6720145787179304\n",
    "Recall:  0.8195128328031722\n",
    "Precision Per Item:  0.5238095238095238\n",
    "Recall Per Item:  1.0\n",
    "Precision:  0.7255309325946445\n",
    "Recall:  0.8965484453741561\n",
    "Precision Per Item:  0.5625\n",
    "Recall Per Item:  1.0\n",
    "Precision:  0.5912671438282219\n",
    "Recall:  0.7269911504424779\n",
    "Precision Per Item:  0.5555555555555556\n",
    "Recall Per Item:  0.9285714285714286\n",
    "\n",
    "Average precision: 72.3\n",
    "Average recall: 82.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Embeddings Algorithm on Identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T23:19:43.307922Z",
     "start_time": "2019-01-13T23:19:43.078352Z"
    }
   },
   "outputs": [],
   "source": [
    "def conversationsq_face_embeddings_with_identities(video_name):\n",
    "    from query.models import FaceCharacterActor, Shot\n",
    "    from rekall.video_interval_collection import VideoIntervalCollection\n",
    "    from rekall.parsers import in_array, bbox_payload_parser, merge_dict_parsers, dict_payload_parser\n",
    "    from rekall.merge_ops import payload_plus\n",
    "    from rekall.payload_predicates import payload_satisfies\n",
    "    from rekall.spatial_predicates import scene_graph\n",
    "    from esper.rekall import intrvllists_to_result_bbox\n",
    "    from query.models import Face\n",
    "    from rekall.video_interval_collection import VideoIntervalCollection\n",
    "    from rekall.parsers import in_array, bbox_payload_parser\n",
    "    from rekall.merge_ops import payload_plus, merge_named_payload, payload_second\n",
    "    from esper.rekall import intrvllists_to_result_bbox\n",
    "    from rekall.payload_predicates import payload_satisfies\n",
    "    from rekall.list_predicates import length_at_most\n",
    "    from rekall.logical_predicates import and_pred, or_pred, true_pred\n",
    "    from rekall.spatial_predicates import scene_graph, make_region\n",
    "    from rekall.temporal_predicates import before, after, overlaps, equal\n",
    "    from rekall.bbox_predicates import height_at_least\n",
    "    from esper.rekall import intrvllists_to_result, intrvllists_to_result_with_objects, add_intrvllists_to_result\n",
    "    from esper.prelude import esper_widget\n",
    "    from rekall.interval_list import Interval, IntervalList\n",
    "    import esper.face_embeddings as face_embeddings\n",
    "    \n",
    "    EMBEDDING_EQUALITY_THRESHOLD = 10\n",
    "    ONE_FRAME = 1\n",
    "    \n",
    "    faces_with_character_actor_qs = FaceCharacterActor.objects.annotate(\n",
    "        min_frame=F('face__frame__number'),\n",
    "        max_frame=F('face__frame__number'),\n",
    "        video_id=F('face__frame__video_id'),\n",
    "        bbox_x1=F('face__bbox_x1'),\n",
    "        bbox_y1=F('face__bbox_y1'),\n",
    "        bbox_x2=F('face__bbox_x2'),\n",
    "        bbox_y2=F('face__bbox_y2'),\n",
    "        character_name=F('characteractor__character__name')\n",
    "    ).filter(face__frame__video__name__contains=video_name)\n",
    "    \n",
    "    faces_per_frame = VideoIntervalCollection.from_django_qs(\n",
    "        faces_with_character_actor_qs,\n",
    "        with_payload=in_array(merge_dict_parsers([\n",
    "            bbox_payload_parser(VideoIntervalCollection.django_accessor),\n",
    "            dict_payload_parser(VideoIntervalCollection.django_accessor, { 'character': 'character_name' }),\n",
    "        ]))\n",
    "    ).coalesce(payload_merge_op=payload_plus)\n",
    "    \n",
    "    shots_qs = Shot.objects.filter(cinematic=True)\n",
    "    shots = VideoIntervalCollection.from_django_qs(shots_qs)\n",
    "    \n",
    "    shots_with_faces = shots.merge(\n",
    "        faces_per_frame, \n",
    "        predicate=overlaps(), \n",
    "        payload_merge_op=lambda shot_id, faces_in_frame: [faces_in_frame]\n",
    "    ).coalesce(payload_merge_op=payload_plus)\n",
    "   \n",
    "    def cluster_center(face_ids):\n",
    "        mean_embedding = face_embeddings.mean(face_ids)\n",
    "        dists = face_embeddings.dist(face_ids, [mean_embedding])\n",
    "        return min(zip(dists, face_ids))[1]\n",
    "\n",
    "    def cluster_and_compute_centers(faces_in_frame_list):\n",
    "#         num_people = max(len(faces_in_frame) for faces_in_frame in faces_in_frame_list)\n",
    "#         face_ids = [face['face_id'] for faces_in_frame in faces_in_frame_list for face in faces_in_frame]\n",
    "#         if num_people == 1:\n",
    "#             clusters = [(fid, 0) for fid in face_ids]\n",
    "#         else:\n",
    "#             clusters = face_embeddings.kmeans(face_ids, num_people)\n",
    "#         centers = [\n",
    "#             (\n",
    "#                 cluster_center([\n",
    "#                     face_id\n",
    "#                     for face_id, cluster_id in clusters\n",
    "#                     if cluster_id == i\n",
    "#                 ]), [\n",
    "#                     face_id\n",
    "#                     for face_id, cluster_id in clusters\n",
    "#                     if cluster_id == i\n",
    "#                 ]\n",
    "#             )\n",
    "#             for i in range(num_people)\n",
    "#         ]\n",
    "#         return centers\n",
    "        return set([face['character'] for faces_in_frame in faces_in_frame_list for face in faces_in_frame])\n",
    "\n",
    "    print(\"Clusters computed\")\n",
    "    \n",
    "    shots_with_centers = shots_with_faces.map(\n",
    "        lambda intrvl: (intrvl.start, intrvl.end, cluster_and_compute_centers(intrvl.payload))\n",
    "    )\n",
    "    \n",
    "    def same_face(center1, center2):\n",
    "        return center1 == center2\n",
    "\n",
    "    def cross_product_faces(intrvl1, intrvl2):\n",
    "        payload1 = intrvl1.get_payload()\n",
    "        payload2 = intrvl2.get_payload()\n",
    "        payload = []\n",
    "        for cluster1 in list(payload1):\n",
    "            for cluster2 in list(payload2):\n",
    "                if not same_face(cluster1, cluster2):\n",
    "                    new_payload = {'A': cluster1, 'B': cluster2}\n",
    "                    payload.append(new_payload)\n",
    "\n",
    "        return [Interval(min(intrvl1.get_start(), intrvl2.get_start()),\n",
    "                 max(intrvl1.get_end(), intrvl2.get_end()), {\n",
    "            'chrs': payload,\n",
    "            'shots': 1\n",
    "        })]\n",
    "    \n",
    "    two_shots = shots_with_centers.join(\n",
    "        shots_with_centers,\n",
    "        predicate=after(max_dist=ONE_FRAME, min_dist=ONE_FRAME), \n",
    "        merge_op=cross_product_faces,\n",
    "        working_window=ONE_FRAME\n",
    "    )\n",
    " \n",
    "    print(\"Cross product done\")\n",
    "\n",
    "    def faces_equal(payload1, payload2):\n",
    "        for face_pair1 in payload1['chrs']:\n",
    "            for face_pair2 in payload2['chrs']:\n",
    "                if (same_face(face_pair1['A'][0], face_pair2['A'][0]) and\n",
    "                    same_face(face_pair1['B'][0], face_pair2['B'][0])):\n",
    "                    return True\n",
    "                if (same_face(face_pair1['A'][0], face_pair2['B'][0]) and\n",
    "                    same_face(face_pair1['B'][0], face_pair2['A'][0])):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    convs = two_shots.coalesce(\n",
    "        predicate=payload_satisfies(faces_equal, arity=2),\n",
    "        payload_merge_op = lambda payload1, payload2: {\n",
    "            'chrs': payload1['chrs'] + + payload2['chrs'],\n",
    "            'shots': payload1['shots'] + payload2['shots']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Coalesce done\")    \n",
    "        \n",
    "    adjacent_seq = convs.merge(\n",
    "        convs,\n",
    "        predicate=and_pred(\n",
    "            after(max_dist=ONE_FRAME, min_dist=ONE_FRAME),\n",
    "            payload_satisfies(faces_equal, arity=2),\n",
    "            arity=2),\n",
    "        payload_merge_op = lambda payload1, payload2: {\n",
    "            'chrs': payload1['chrs'] + payload2['chrs'],\n",
    "            'shots': payload1['shots'] + payload2['shots']\n",
    "        },\n",
    "        working_window=1\n",
    "    )\n",
    "    convs = convs.set_union(adjacent_seq)\n",
    "    # convs = convs.coalesce(predicate=times_equal, payload_merge_op=shots_equal)\n",
    "    \n",
    "    print(\"Two-shot adjacencies done\")\n",
    "\n",
    "    def filter_fn(intvl):\n",
    "        payload = intvl.get_payload()\n",
    "        if type(payload) is dict and 'shots' in payload:\n",
    "            return payload['shots'] >= 2\n",
    "        return False \n",
    "    \n",
    "    convs = convs.filter(filter_fn)\n",
    "    convs = convs.coalesce()\n",
    "    \n",
    "    print(\"Final filter done\")\n",
    "\n",
    "    for video_id in convs.intervals.keys():\n",
    "        print(video_id)\n",
    "        intvllist = convs.get_intervallist(video_id)\n",
    "        for intvl in intvllist.get_intervals():\n",
    "            print(intvl.payload)\n",
    "            print(str(intvl.start) + ':' + str(intvl.end))\n",
    "    \n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T23:19:58.302076Z",
     "start_time": "2019-01-13T23:19:43.783839Z"
    }
   },
   "outputs": [],
   "source": [
    "convs2 = conversationsq_face_embeddings_with_identities('apollo 13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T23:11:45.862602Z",
     "start_time": "2019-01-13T23:11:45.828948Z"
    }
   },
   "outputs": [],
   "source": [
    "apollo_data = [\n",
    "    (2578, 4100), (4244, 4826), (5098, 5828), (7757, 9546),\n",
    "    (9602, 10300), (12393, 12943), (13088, 13884), (14146, 15212),\n",
    "    (15427, 16116), (18040, 19198), (20801, 23368), (24572, 26185),\n",
    "    (26735, 28753), (29462, 30873), (31768, 34618)]\n",
    "apollo_gt = {15: IntervalList([Interval(start, end, payload=None) for (start,end) in apollo_data])}\n",
    "print_statistics({15: convs2.filter(lambda intrvl: intrvl.start < 34618).get_intervallist(15)}, apollo_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T21:15:53.596865Z",
     "start_time": "2019-01-13T21:15:53.555205Z"
    }
   },
   "source": [
    "Results:\n",
    "```\n",
    "Precision:  0.9756586483390607\n",
    "Recall:  0.510055391985628\n",
    "Precision Per Item:  1.0\n",
    "Recall Per Item:  0.9333333333333333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dan's Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T20:16:05.600115Z",
     "start_time": "2019-01-13T20:15:48.601081Z"
    }
   },
   "outputs": [],
   "source": [
    "from query.models import FaceCharacterActor, Shot\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "from rekall.parsers import in_array, bbox_payload_parser, merge_dict_parsers, dict_payload_parser\n",
    "from rekall.merge_ops import payload_plus\n",
    "from rekall.payload_predicates import payload_satisfies\n",
    "from rekall.spatial_predicates import scene_graph\n",
    "from esper.rekall import intrvllists_to_result_bbox\n",
    "from query.models import Face\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "from rekall.parsers import in_array, bbox_payload_parser\n",
    "from rekall.merge_ops import payload_plus, merge_named_payload, payload_second\n",
    "from esper.rekall import intrvllists_to_result_bbox\n",
    "from rekall.payload_predicates import payload_satisfies\n",
    "from rekall.list_predicates import length_at_most\n",
    "from rekall.logical_predicates import and_pred, or_pred, true_pred\n",
    "from rekall.spatial_predicates import scene_graph, make_region\n",
    "from rekall.temporal_predicates import before, after, overlaps\n",
    "from rekall.bbox_predicates import height_at_least\n",
    "from esper.rekall import intrvllists_to_result, intrvllists_to_result_with_objects, add_intrvllists_to_result\n",
    "from esper.prelude import esper_widget\n",
    "from rekall.interval_list import Interval, IntervalList\n",
    "import esper.face_embeddings as face_embeddings\n",
    "\n",
    "# faces are sampled every 12 frames\n",
    "SAMPLING_RATE = 12\n",
    "ONE_FRAME = 1\n",
    "\n",
    "video_name='apollo 13'\n",
    "\n",
    "faces_qs = Face.objects.annotate(\n",
    "    min_frame=F('frame__number'),\n",
    "    max_frame=F('frame__number'),\n",
    "    video_id=F('frame__video_id')\n",
    ").filter(\n",
    "    frame__video__name__contains=video_name,\n",
    "    frame__regularly_sampled=True,\n",
    "    probability__gte=0.9\n",
    ")\n",
    "\n",
    "faces_per_frame = VideoIntervalCollection.from_django_qs(\n",
    "    faces_qs,\n",
    "    with_payload=in_array(merge_dict_parsers([\n",
    "        bbox_payload_parser(VideoIntervalCollection.django_accessor),\n",
    "        dict_payload_parser(VideoIntervalCollection.django_accessor, { 'face_id': 'id' }),\n",
    "    ]))\n",
    ").coalesce(payload_merge_op=payload_plus)\n",
    "\n",
    "shots_qs = Shot.objects.filter(cinematic=True)\n",
    "shots = VideoIntervalCollection.from_django_qs(shots_qs)\n",
    "\n",
    "shots_with_faces = shots.merge(\n",
    "    faces_per_frame, \n",
    "    predicate=overlaps(), \n",
    "    payload_merge_op=lambda shot_id, faces_in_frame: [faces_in_frame]\n",
    ").coalesce(payload_merge_op=payload_plus)\n",
    "\n",
    "def cluster_center(face_ids):\n",
    "    mean_embedding = face_embeddings.mean(face_ids)\n",
    "    dists = face_embeddings.dist(face_ids, [mean_embedding])\n",
    "    return min(zip(dists, face_ids))[1]\n",
    "\n",
    "def cluster_and_compute_centers(faces_in_frame_list):\n",
    "    num_people = max(len(faces_in_frame) for faces_in_frame in faces_in_frame_list)\n",
    "    face_ids = [face['face_id'] for faces_in_frame in faces_in_frame_list for face in faces_in_frame]\n",
    "    if num_people == 1:\n",
    "        clusters = [(fid, 0) for fid in face_ids]\n",
    "    else:\n",
    "        clusters = face_embeddings.kmeans(face_ids, num_people)\n",
    "    centers = [\n",
    "        (\n",
    "            cluster_center([\n",
    "                face_id\n",
    "                for face_id, cluster_id in clusters\n",
    "                if cluster_id == i\n",
    "            ]), [\n",
    "                face_id\n",
    "                for face_id, cluster_id in clusters\n",
    "                if cluster_id == i\n",
    "            ]\n",
    "        )\n",
    "        for i in range(num_people)\n",
    "    ]\n",
    "    return centers\n",
    "\n",
    "shots_with_centers = shots_with_faces.map(\n",
    "    lambda intrvl: (intrvl.start, intrvl.end, cluster_and_compute_centers(intrvl.payload))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:35:52.230446Z",
     "start_time": "2019-01-13T19:35:52.178431Z"
    }
   },
   "outputs": [],
   "source": [
    "shots_with_centroids.get_intervallist(15).filter(payload_satisfies(lambda p: len(p) > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:37:30.788177Z",
     "start_time": "2019-01-13T19:37:30.737228Z"
    }
   },
   "outputs": [],
   "source": [
    "a_list = [706034, 706036, 706038, 706040, 706042, 706043, 706046, 706048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:37:34.846070Z",
     "start_time": "2019-01-13T19:37:34.806381Z"
    }
   },
   "outputs": [],
   "source": [
    "b_list = [706033, 706035, 706037, 706039, 706041, 706044, 706045, 706047, 706049, 706050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:38:53.575228Z",
     "start_time": "2019-01-13T19:38:53.531613Z"
    }
   },
   "outputs": [],
   "source": [
    "a_mean = face_embeddings.mean(a_list)\n",
    "b_mean = face_embeddings.mean(b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:38:59.194911Z",
     "start_time": "2019-01-13T19:38:59.150118Z"
    }
   },
   "outputs": [],
   "source": [
    "a_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:39:04.961630Z",
     "start_time": "2019-01-13T19:39:04.918043Z"
    }
   },
   "outputs": [],
   "source": [
    "b_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:40:18.782947Z",
     "start_time": "2019-01-13T19:40:18.741878Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:50:32.415528Z",
     "start_time": "2019-01-13T19:50:32.373860Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(sum((a-b) ** 2 for a, b in zip(a_mean, b_mean))) * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:49:53.518834Z",
     "start_time": "2019-01-13T19:49:53.477136Z"
    }
   },
   "outputs": [],
   "source": [
    "a_mean_small1 = face_embeddings.mean(a_list[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:49:54.258625Z",
     "start_time": "2019-01-13T19:49:54.219999Z"
    }
   },
   "outputs": [],
   "source": [
    "a_mean_small2 = face_embeddings.mean(a_list[4:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:50:26.234601Z",
     "start_time": "2019-01-13T19:50:26.190052Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(sum((a-b) ** 2 for a, b in zip(a_mean_small1, a_mean_small2))) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:59:43.132030Z",
     "start_time": "2019-01-13T19:59:43.092421Z"
    }
   },
   "outputs": [],
   "source": [
    "a_mean = face_embeddings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:59:43.805279Z",
     "start_time": "2019-01-13T19:59:43.764275Z"
    }
   },
   "outputs": [],
   "source": [
    "b_mean = face_embeddings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T19:59:44.466416Z",
     "start_time": "2019-01-13T19:59:44.421676Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(sum((a-b) ** 2 for a, b in zip(a_mean, b_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T20:10:43.280248Z",
     "start_time": "2019-01-13T20:10:43.240313Z"
    }
   },
   "outputs": [],
   "source": [
    "def cluster_center(face_ids):\n",
    "    mean_embedding = face_embeddings.mean(face_ids)\n",
    "    dists = face_embeddings.dist(face_ids, [mean_embedding])\n",
    "    return min(zip(dists, face_ids))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T20:11:12.050725Z",
     "start_time": "2019-01-13T20:11:12.006774Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_center(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T20:11:10.684677Z",
     "start_time": "2019-01-13T20:11:10.638977Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_center(b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T20:11:42.893105Z",
     "start_time": "2019-01-13T20:11:42.850441Z"
    }
   },
   "outputs": [],
   "source": [
    "face_embeddings.dist([cluster_center(a_list)], target_ids=[cluster_center(b_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T20:12:23.743624Z",
     "start_time": "2019-01-13T20:12:23.703044Z"
    }
   },
   "outputs": [],
   "source": [
    "c_list = [706334, 706336, 706338, 706341, 706343, 706345, 706346, 706348, 706350, 706353, 706355, 706357, 706359, 706361, 706362, 706364, 706365, 706366, 706367, 706368, 706369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T20:12:29.625341Z",
     "start_time": "2019-01-13T20:12:29.583684Z"
    }
   },
   "outputs": [],
   "source": [
    "face_embeddings.dist([cluster_center(a_list)], target_ids=[cluster_center(c_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T20:12:38.065846Z",
     "start_time": "2019-01-13T20:12:38.023587Z"
    }
   },
   "outputs": [],
   "source": [
    "face_embeddings.dist([cluster_center(b_list)], target_ids=[cluster_center(c_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T21:50:15.492695Z",
     "start_time": "2019-01-11T21:50:14.756066Z"
    }
   },
   "outputs": [],
   "source": [
    "from query.models import FaceCharacterActor, Shot\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "from rekall.parsers import in_array, bbox_payload_parser, merge_dict_parsers, dict_payload_parser\n",
    "from rekall.merge_ops import payload_plus\n",
    "from rekall.payload_predicates import payload_satisfies\n",
    "from rekall.spatial_predicates import scene_graph\n",
    "from esper.rekall import intrvllists_to_result_bbox\n",
    "from query.models import Face\n",
    "from rekall.video_interval_collection import VideoIntervalCollection\n",
    "from rekall.parsers import in_array, bbox_payload_parser\n",
    "from rekall.merge_ops import payload_plus, merge_named_payload, payload_second\n",
    "from esper.rekall import intrvllists_to_result_bbox\n",
    "from rekall.payload_predicates import payload_satisfies\n",
    "from rekall.list_predicates import length_at_most\n",
    "from rekall.logical_predicates import and_pred, or_pred, true_pred\n",
    "from rekall.spatial_predicates import scene_graph, make_region\n",
    "from rekall.temporal_predicates import before, after, overlaps\n",
    "from rekall.bbox_predicates import height_at_least\n",
    "from esper.rekall import intrvllists_to_result, intrvllists_to_result_with_objects, add_intrvllists_to_result\n",
    "from esper.prelude import esper_widget\n",
    "from rekall.interval_list import Interval, IntervalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:27:12.183045Z",
     "start_time": "2019-01-11T18:27:12.157781Z"
    }
   },
   "outputs": [],
   "source": [
    "RIGHT_HALF_MIN_X = 0.45\n",
    "LEFT_HALF_MAX_X = 0.55\n",
    "MIN_FACE_HEIGHT = 0.4\n",
    "MAX_FACES_ON_SCREEN = 2\n",
    "# faces are sampled every 12 frames\n",
    "SAMPLING_RATE = 12\n",
    "ONE_SECOND = 1\n",
    "FOUR_SECONDS = 96\n",
    "TEN_SECONDS = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:27:13.277953Z",
     "start_time": "2019-01-11T18:27:13.249640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Annotate face rows with start and end frames and the video ID\n",
    "faces_with_character_actor_qs = FaceCharacterActor.objects.annotate(\n",
    "    min_frame=F('face__frame__number'),\n",
    "    max_frame=F('face__frame__number'),\n",
    "    video_id=F('face__frame__video_id'),\n",
    "    bbox_x1=F('face__bbox_x1'),\n",
    "    bbox_y1=F('face__bbox_y1'),\n",
    "    bbox_x2=F('face__bbox_x2'),\n",
    "    bbox_y2=F('face__bbox_y2'),\n",
    "    character_name=F('characteractor__character__name')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:27:41.347654Z",
     "start_time": "2019-01-11T18:27:13.961116Z"
    }
   },
   "outputs": [],
   "source": [
    "faces_with_identity = VideoIntervalCollection.from_django_qs(\n",
    "    faces_with_character_actor_qs,\n",
    "    with_payload=in_array(merge_dict_parsers([\n",
    "        bbox_payload_parser(VideoIntervalCollection.django_accessor),\n",
    "        dict_payload_parser(VideoIntervalCollection.django_accessor, { 'character': 'character_name' }),\n",
    "    ]))\n",
    ").coalesce(payload_merge_op=payload_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:27:59.389089Z",
     "start_time": "2019-01-11T18:27:46.913263Z"
    }
   },
   "outputs": [],
   "source": [
    "shots_qs = Shot.objects.filter(\n",
    "    labeler=Labeler.objects.get(name='shot-hsvhist-face'))\n",
    "shots = VideoIntervalCollection.from_django_qs(shots_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:28:23.070163Z",
     "start_time": "2019-01-11T18:28:23.037474Z"
    }
   },
   "outputs": [],
   "source": [
    "def payload_unique_characters(payload1, payload2):\n",
    "    if 'characters' not in payload1[0]:\n",
    "        unique_characters = set([p['character'] for p in payload1])\n",
    "        for p in payload2:\n",
    "            unique_characters.add(p['character'])\n",
    "        payload1[0]['characters'] = list(unique_characters)\n",
    "    else:\n",
    "        unique_characters = set([p['character'] for p in payload2])\n",
    "        unique_characters.update(payload1[0]['characters'])\n",
    "        payload1[0]['characters'] = list(unique_characters)\n",
    "    return payload1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:29:49.130286Z",
     "start_time": "2019-01-11T18:28:24.768580Z"
    }
   },
   "outputs": [],
   "source": [
    "shots_with_faces = shots.merge(faces_with_identity, \n",
    "                               predicate=overlaps(), \n",
    "                               payload_merge_op=payload_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:30:11.938506Z",
     "start_time": "2019-01-11T18:30:08.612578Z"
    }
   },
   "outputs": [],
   "source": [
    "shots_with_faces = shots_with_faces.coalesce(payload_merge_op=payload_unique_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:30:13.167661Z",
     "start_time": "2019-01-11T18:30:13.134139Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_product_faces(intrvl1, intrvl2):\n",
    "    payload1 = intrvl1.get_payload()\n",
    "    payload2 = intrvl2.get_payload()\n",
    "    chrtrs1 = payload1[0]['characters'] if 'characters' in payload1[0] else list(set([p['character'] for p in payload1]))\n",
    "    chrtrs2 = payload2[0]['characters'] if 'characters' in payload2[0] else list(set([p['character'] for p in payload2]))\n",
    "    new_intervals = []\n",
    "    for i in payload1:\n",
    "        for j in chrtrs2:\n",
    "            if i!=j:\n",
    "                new_payload = {'A': i, 'B': j}\n",
    "#                 new_payload.update()\n",
    "                start = min(intrvl1.start, intrvl2.start)\n",
    "                end = max(intrvl1.end, intrvl2.end)\n",
    "#                 print(intrvl1.keys())\n",
    "#                 print(intrvl1.video_id == intrvl2.video_id )\n",
    "                new_intervals.append(Interval(start, end, {'A': i, 'B': j}))\n",
    "\n",
    "    return new_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:30:14.808353Z",
     "start_time": "2019-01-11T18:30:14.783402Z"
    }
   },
   "outputs": [],
   "source": [
    "def faces_equal(payload1, payload2):\n",
    "    return (payload1['A'] == payload2['A'] and payload1['B'] == payload2['B']) or (payload1['A'] == payload2['B'] and payload1['B'] == payload2['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T02:04:50.078Z"
    }
   },
   "outputs": [],
   "source": [
    "def faces_equal(payload1, payload2):\n",
    "    if type(payload1) is not list and type(payload1) is not list:\n",
    "        return (payload1['A'] == payload2['A'] and payload1['B'] == payload2['B']) or (payload1['A'] == payload2['B'] and payload1['B'] == payload2['A'])\n",
    "    elif type(payload1) is list and type(payload1) is list:\n",
    "        for i in payload1:\n",
    "            for j in payload2:\n",
    "                if i['A'] == j['A'] and i['B'] == j['B']:\n",
    "                    return True\n",
    "    elif type(payload1) is list:\n",
    "        for i in payload1:\n",
    "            if i['A'] == payload2['A'] and i['B'] == payload2['B']:\n",
    "                return True\n",
    "    else:\n",
    "        for i in payload2:\n",
    "            if i['A'] == payload1['A'] and i['B'] == payload1['B']:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def times_equal(intrvl1, intrvl2):\n",
    "    return intrvl.start == intervl2.start and intrvl.end == intervl2.end\n",
    "\n",
    "def merge_to_list(payload1, payload2):\n",
    "    p1 = payload1 if type(payload1) is list else [payload1]\n",
    "    p2 = payload2 if type(payload2) is list else [payload2]\n",
    "    return p1+p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:30:32.951958Z",
     "start_time": "2019-01-11T18:30:21.962094Z"
    }
   },
   "outputs": [],
   "source": [
    "two_shots = shots_with_faces.join(shots_with_faces, predicate=after(max_dist=ONE_SECOND, min_dist=ONE_SECOND), \n",
    "                                merge_op=cross_product_faces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:30:40.320296Z",
     "start_time": "2019-01-11T18:30:40.285986Z"
    }
   },
   "outputs": [],
   "source": [
    "num_intervals = 0\n",
    "for video_id in two_shots.intervals.keys():\n",
    "    intvllist = two_shots.get_intervallist(video_id)\n",
    "    s = intvllist.size()\n",
    "    print(s)\n",
    "    num_intervals += s\n",
    "print(num_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:30:45.233514Z",
     "start_time": "2019-01-11T18:30:44.912012Z"
    }
   },
   "outputs": [],
   "source": [
    "conversations = two_shots.coalesce(predicate=payload_satisfies(faces_equal, arity=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:30:46.757300Z",
     "start_time": "2019-01-11T18:30:46.723026Z"
    }
   },
   "outputs": [],
   "source": [
    "num_intervals = 0\n",
    "for video_id in conversations.intervals.keys():\n",
    "    intvllist = conversations.get_intervallist(video_id)\n",
    "    s = intvllist.size()\n",
    "    print(s)\n",
    "    num_intervals += s\n",
    "print(num_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T01:23:04.148014Z",
     "start_time": "2019-01-11T01:22:48.915188Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T01:34:21.071232Z",
     "start_time": "2019-01-11T01:34:21.041940Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T01:39:39.038176Z",
     "start_time": "2019-01-11T01:39:36.380864Z"
    }
   },
   "outputs": [],
   "source": [
    "scene = three_shot.merge(three_shot, predicate=and_pred(after(max_dist=ONE_SECOND, min_dist=ONE_SECOND), \n",
    "                                              payload_satisfies(check_B_intersects, arity=2), arity=2)).coalesce()#, payload_merge_op=updateA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T18:36:47.466211Z",
     "start_time": "2019-01-11T18:36:36.861354Z"
    }
   },
   "outputs": [],
   "source": [
    "esper_widget(intrvllists_to_result_with_objects(\n",
    "            conversations.get_allintervals(), lambda payload, video: []),\n",
    "            crop_bboxes=False,\n",
    "            disable_playback=False,\n",
    "            jupyter_keybindings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T06:29:53.082Z"
    }
   },
   "outputs": [],
   "source": [
    "conversations.get_allintervals().key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T06:30:19.748Z"
    }
   },
   "outputs": [],
   "source": [
    "conversations.intrvls.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T06:31:05.130Z"
    }
   },
   "outputs": [],
   "source": [
    "conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T22:32:14.677147Z",
     "start_time": "2019-01-13T22:32:14.628475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Returns precision, recall, precision_per_item, recall_per_item\n",
    "def compute_statistics(query_intrvllists, ground_truth_intrvllists):\n",
    "    total_query_time = 0\n",
    "    total_query_segments = 0\n",
    "    total_ground_truth_time = 0\n",
    "    total_ground_truth_segments = 0\n",
    "    \n",
    "    for video in query_intrvllists:\n",
    "        total_query_time += query_intrvllists[video].coalesce().get_total_time()\n",
    "        total_query_segments += query_intrvllists[video].size()\n",
    "    for video in ground_truth_intrvllists:\n",
    "        total_ground_truth_time += ground_truth_intrvllists[video].coalesce().get_total_time()\n",
    "        total_ground_truth_segments += ground_truth_intrvllists[video].size()\n",
    "        \n",
    "    total_overlap_time = 0\n",
    "    overlapping_query_segments = 0\n",
    "    overlapping_ground_truth_segments = 0\n",
    "    \n",
    "    for video in query_intrvllists:\n",
    "        if video in ground_truth_intrvllists:\n",
    "            query_list = query_intrvllists[video]\n",
    "            gt_list = ground_truth_intrvllists[video]\n",
    "            \n",
    "            total_overlap_time += query_list.overlaps(gt_list).coalesce().get_total_time()\n",
    "            overlapping_query_segments += query_list.filter_against(gt_list, predicate=overlaps()).size()\n",
    "            overlapping_ground_truth_segments += gt_list.filter_against(query_list, predicate=overlaps()).size()\n",
    "    \n",
    "    if total_query_time == 0:\n",
    "        precision = 1.0\n",
    "        precision_per_item = 1.0\n",
    "    else:\n",
    "        precision = total_overlap_time / total_query_time\n",
    "        precision_per_item = overlapping_query_segments / total_query_segments\n",
    "    \n",
    "    if total_ground_truth_time == 0:\n",
    "        recall = 1.0\n",
    "        recall_per_item = 1.0\n",
    "    else:\n",
    "        recall = total_overlap_time / total_ground_truth_time\n",
    "        recall_per_item = overlapping_ground_truth_segments / total_ground_truth_segments\n",
    "    \n",
    "    return precision, recall, precision_per_item, recall_per_item\n",
    "\n",
    "def print_statistics(query_intrvllists, ground_truth_intrvllists):\n",
    "    precision, recall, precision_per_item, recall_per_item = compute_statistics(\n",
    "        query_intrvllists, ground_truth_intrvllists)\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Precision Per Item: \", precision_per_item)\n",
    "    print(\"Recall Per Item: \", recall_per_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T18:04:11.021054Z",
     "start_time": "2019-01-13T18:03:53.836477Z"
    }
   },
   "outputs": [],
   "source": [
    "godfather_query = conversationsq('the godfather part iii')\n",
    "for k in godfather_query.intervals.keys():\n",
    "    print(k)\n",
    "godfather_query = godfather_query.filter(lambda inv: inv.start < 43103)\n",
    "godfather_query = {'216': godfather_query.get_intervallist(216)}\n",
    "\n",
    "data = [(12481, 13454), (13673, 14729), (16888, 17299), (21101, 27196), (27602, 29032), (29033, 33204), (34071, 41293), (41512, 43103)]\n",
    "godfather_gt = {'216': IntervalList([Interval(start, end, payload=None) for (start,end) in data])}\n",
    "print_statistics(godfather_query, godfather_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T23:17:55.432489Z",
     "start_time": "2019-01-13T23:17:40.285686Z"
    }
   },
   "outputs": [],
   "source": [
    "apollo_query = conversationsq('apollo 13')\n",
    "apollo_query = apollo_query.filter(lambda inv: inv.start < 34618)\n",
    "apollo_query = {'15': apollo_query.get_intervallist(15)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T23:17:55.486805Z",
     "start_time": "2019-01-13T23:17:55.436234Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [(2578, 4100), (4244, 4826), (5098, 5828), (7757, 9546), (9602, 10300), (12393, 12943), (13088, 13884), (14146, 15212), (15427, 16116), (18040, 19198), (20801, 23368), (24572, 26185), (26735, 28753), (29462, 30873), (31768, 34618)]\n",
    "apollo_gt = {'15': IntervalList([Interval(start, end, payload=None) for (start,end) in data])}\n",
    "print_statistics(apollo_query, apollo_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T02:50:44.749090Z",
     "start_time": "2019-01-13T02:50:44.689610Z"
    }
   },
   "outputs": [],
   "source": [
    "invllist = caption_metadata_for_video(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T18:01:45.248465Z",
     "start_time": "2019-01-13T18:01:29.479521Z"
    }
   },
   "outputs": [],
   "source": [
    "hp2_query = conversationsq('harry potter and the chamber')\n",
    "hp2_query = hp2_query.filter(lambda inv: inv.start < 20308)\n",
    "hp2_query = {'374': hp2_query.get_intervallist(374)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T18:01:45.298673Z",
     "start_time": "2019-01-13T18:01:45.251526Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = [(2155, 4338), (4687, 6188), (6440, 10134), (12921, 13151), (16795, 17370), (17766, 18021), (18102, 19495), (19622, 20308)]\n",
    "hp2_gt = {'374': IntervalList([Interval(start, end, payload=None) for (start,end) in data])}\n",
    "print_statistics(hp2_query, hp2_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T18:05:29.491933Z",
     "start_time": "2019-01-13T18:05:14.209000Z"
    }
   },
   "outputs": [],
   "source": [
    "fc_query = conversationsq('fight club')\n",
    "fc_query = fc_query.filter(lambda inv: inv.start < 58258)\n",
    "fc_query = {'61': fc_query.get_intervallist(61)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T18:05:29.550998Z",
     "start_time": "2019-01-13T18:05:29.495823Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [(4698, 5602), (6493, 6865), (8670, 9156), (9517, 10908), (11087, 13538), (22039, 24188), (25603, 27656), (31844, 32812), (32918, 33451), (33698, 35363), (42072, 45143), (45272, 46685), (49162, 50618), (56830, 58258)]\n",
    "fc_gt = {'61': IntervalList([Interval(start, end, payload=None) for (start,end) in data])}\n",
    "print_statistics(fc_query, fc_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T02:50:47.379283Z",
     "start_time": "2019-01-13T02:50:47.073573Z"
    }
   },
   "outputs": [],
   "source": [
    "for intvl in invllist.get_intervals():\n",
    "    if 'speaker' in intvl.payload:\n",
    "        print(intvl.payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apollo 13\n",
    "1:48 --> 2:50 V\n",
    "2:57 --> 3:20 V\n",
    "5:24 --> 7:44 V (5:24 - 6:24) (6:45 - 7:18) (shot broken up because of shots of the moon)\n",
    "8:26 --> 9:02; 8:36 - 9:02 \n",
    "10:00 --> 10:33; 9:41 - 10:33 - long shot times w multiple people present\n",
    "10:33 --> 11:11; 10:44 - 11:12 - skipped the daughter\n",
    "12:40 --> 13:17; 12:33 - 13:21 (shot is a bit over extended)\n",
    "                [14:28 - 14:51] - reaction sequence; not dialogue\n",
    "17:03 -- 18:02;  18:15 ; V - over extended; catches him in the next scene\n",
    "20:29 --> 21:27 [DID NOT CATCH]\n",
    "22:04 --> 23:56 V\n",
    "27:04 --> 27:30 [caught and \n",
    "27:34 --> 27:47 combined in unexpected ways\n",
    "                 \n",
    "Godfather\n",
    "\n",
    "                 \n",
    "                 \n",
    "                 \n",
    "data = [\n",
    "    (8757,9049),\n",
    "    (12750,13463),\n",
    "    (13683,14227),\n",
    "    (21357,22236),\n",
    "    (22294,22758),\n",
    "    (23147,25854),\n",
    "    (26007,26942),\n",
    "    (27620,28172),\n",
    "    (28382,28623),\n",
    "    (28785,29036),\n",
    "    (29904,31014),\n",
    "    (33936,35339),\n",
    "    (35421,36248),\n",
    "    (39388,40062),\n",
    "    (41675,42689),\n",
    "    (51246,52118),\n",
    "    (53117,54776),\n",
    "    (54895,55762),\n",
    "    (56819,59963),\n",
    "    (60253,61875),\n",
    "    (66533,67846),\n",
    "    (68729,69040),\n",
    "    (69421,70153),\n",
    "    (70285,71102)]\n",
    "intrvllist = IntervalList([Interval(start, end, payload=None) for (start,end) in data])\n",
    "shot_reverse_shot_labelled = {216: intrvllist}\n",
    "esper_widget(intrvllists_to_result_with_objects(shot_reverse_shot_labelled, lambda payload, video: []), disable_captions=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
